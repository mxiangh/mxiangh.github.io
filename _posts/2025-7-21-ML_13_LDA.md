---

title: 机器学习（13）线性判别分析——Linear Discriminant Analysis. LDA
tags: ML Dimensionality_Reduction
---

https://www.cnblogs.com/wj-1314/p/10234256.html

思想背景：线性判别分析是一种有监督的线性降维方法（分类），其目的是将不同类别的n维数据通过重新构建正交基找到数据点在低维空间的投影，要求降维后的数据，相同类别尽可能接近，不同类别尽可能远。

<!--more-->

1. 类内散度矩阵定义

$$  S_w = \sum_{\mathbf{x} \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum_{\mathbf{x} \in X_1}(x-\mu_1)(x-\mu_1)^T $$

2. 类间散度矩阵定义

$$  S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$$

3. 定义损失函数，化多目标为单目标

$$ J = \frac{||w^T\mu_0-w^T\mu_1||^2}{w^T \sum_{\mathbf{x} \in X_0}(x-\mu_0)(x-\mu_0)^T w + w^T \sum_{\mathbf{x} \in X_1}(x-\mu_1)(x-\mu_1)^T w} $$

令$S_w$为之间均值距离，则化简式子

$$ J = \frac{w^T S_b w}{ w^T S_w w} $$

这就是LDA的损失函数需要最大化的目标。

4. 化简式子

观察发现，这个函数与w的长度没有关系，因为是分式，w改变，分式整体没有变化。所以可以对分母进行约束为1，得

$$ min \ -w^T S_b w $$

$$ s.t. \  w^T S_w w = 1 $$

5. 拉格朗日乘子法
$$ min \ -w^T S_b w + \lambda (1- w^T S_w w) $$

令偏导函数为0，可得

$$ S_b w = \lambda S_w w $$

同主成分分析，代入原损失函数发现只与$\lambda$有关，求特征向量即可

$$ S_w^{-1} S_b w = \lambda w $$
