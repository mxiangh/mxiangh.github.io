---

title: 机器学习（13）线性判别分析——Linear Discriminant Analysis. LDA
tags: ML Dimensionality_Reduction
---

#### 线性判别分析——Linear Discriminant Analysis. LDA

思想：线性判别分析是一种有监督的线性降维方法（分类），其目的是将不同两类的n维数据通过重新构建正交基找到数据点在低维空间的投影，要求降维后的数据，相同类别尽可能接近，不同类别尽可能远。

<!--more-->

这个模型与主成分分析密切相关，学会了主成分分析，才能看懂线性判别分析。

以下内容以两个类别为例：

##### 1.类内散度矩阵定义

投影后，类别内部各个点的协方差矩阵

$$  S_w = \sum_{\mathbf{x} \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum_{\mathbf{x} \in X_1}(x-\mu_1)(x-\mu_1)^T $$

##### 2.类间散度矩阵定义

投影后，两个类别均值向量的协方差矩阵

$$  S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$$

##### 3.定义损失函数，化多目标为单目标

分母是降维后类别内部各个点的投影欧式距离之和

分子是降维后两个类别投影后的均值向量的欧式距离

$$ J = \frac{||w^T\mu_0-w^T\mu_1||^2}{w^T \sum_{\mathbf{x} \in X_0}(x-\mu_0)(x-\mu_0)^T w + w^T \sum_{\mathbf{x} \in X_1}(x-\mu_1)(x-\mu_1)^T w} $$

令$S_w$为之间均值距离，则化简式子

$$ J = \frac{w^T S_b w}{ w^T S_w w} $$

这就是LDA的损失函数需要最大化的目标。

##### 4.化简式子

观察发现，这个函数与w的长度没有关系，因为是分式，w改变，分式整体没有变化。所以可以对分母进行约束为1（修改w使得分母为1，目的是方便求解），得

$$ min \ -w^T S_b w $$
$$ s.t. \  w^T S_w w = 1 $$

##### 5.拉格朗日乘子法
$$ min \ -w^T S_b w + \lambda (1- w^T S_w w) $$

令偏导函数为0，可得

$$ S_b w = \lambda S_w w $$

代入原损失函数发现只与$\lambda$有关，求特征向量即可

$$ S_w^{-1} S_b w = \lambda w $$

##### 6.二分类算法流程：

1. 计算每个类别的均值向量；
2. 计算类内散度矩阵($S_w$)：$  S_w = \sum_{\mathbf{x} \in X_0}(x-\mu_0)(x-\mu_0)^T + \sum_{\mathbf{x} \in X_1}(x-\mu_1)(x-\mu_1)^T $；
3. 计算类间散度矩阵($S_b$)：$  S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$；
4. 计算$ S_w^{-1} S_b $的特征值和特征向量；
5. 选择前 k 个最大特征值对应的特征向量组成降维矩阵 W；
6. 将原始数据通过降维矩阵 W 映射到低维空间。

##### 7.多分类算法流程：

1. 计算每个类别的均值向量和整个数据集的总体均值向量；
2. 计算类内散度矩阵($S_w$)：$  S_w = \sum_c\sum_{\mathbf{x} \in c}(x-\mu_c)(x-\mu_c)^T $；($\mu_c$是第C类样本均值)
3. 计算类间散度矩阵($S_b$)：$  S_b = \sum_c n_c (\mu_c - \mu)(\mu_c - \mu)^T $；($\mu$是总体样本均值，$n_c$是第 c 类的样本数)
4. 计算$ S_w^{-1} S_b $的特征值和特征向量；
5. 选择前 k 个最大特征值对应的特征向量组成降维矩阵 W；
6. 将原始数据通过降维矩阵 W 映射到低维空间。

！！！注意，LDA降维维度不能高于类别数，原因如下：

- 二分类：类间散度矩阵$ S_b = (\mu_0 - \mu_1)(\mu_0 - \mu_1)^T$，这是一个“秩 1” 矩阵，因此，$S_w$最多只有 1 个非零特征值，对应最多 1 维有效投影。
- 多分类：$  S_b = \sum_c n_c (\mu_c - \mu)(\mu_c - \mu)^T $的秩最多为C-1，因为均值$\mu$是全局均值，所有类别均值和全局均值线性相关。

维度由类间散度矩阵定义，本质是线性代数中 “矩阵秩的约束” 和 “可分性的极限”。

##### 8.LDA优缺点

优点：

- 在降维过程中考虑了样本的类别标签，使得降维后的数据在分类任务中能够更好地区分不同的类别，提升分类算法的准确率。
- LDA 找到的投影方向是与类别区分相关的，这些方向在一定程度上可以反映出不同类别之间的差异特征，对于理解数据的类别结构和特征之间的关系具有一定的帮助。

缺点：

- 计算复杂度相对较高。
- 对数据分布假设敏感。
- 非线性数据处理能力不够。

~~~
# 手写实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

def lda(data, labels, n_dim):
    """
    线性判别分析(LDA)降维函数
    参数:
        data: 输入数据，形状为[N,D]，N是样本数，D是特征维度
        labels: 类别标签，形状为[N,]
        n_dim: 降维后的维度（最多为类别数-1）
    返回:
        data_ndim: 降维后的数据，形状为[N,n_dim]
        W: 投影矩阵，形状为[D,n_dim]
    """
    # 获取类别数和特征维度
    classes = np.unique(labels)
    n_classes = len(classes)
    n_samples, n_features = data.shape
    
    # 确保降维维度不超过类别数-1（LDA理论限制）
    if n_dim >= n_classes:
        raise ValueError(f"LDA降维维度最多为{n_classes-1}（类别数-1）")
    
    # 1. 计算各类别的均值向量和总体均值
    class_means = []  # 存储每个类别的均值向量
    overall_mean = np.mean(data, axis=0)  # 总体均值
    
    for c in classes:
        class_data = data[labels == c]
        class_mean = np.mean(class_data, axis=0)
        class_means.append(class_mean)
    
    # 2. 计算类内散度矩阵(Sw)
    Sw = np.zeros((n_features, n_features))
    for c, mean in zip(classes, class_means):
        class_data = data[labels == c]
        # 每个样本减去该类均值后的外积求和
        for x in class_data:
            x_minus_mean = (x - mean).reshape(n_features, 1)
            Sw += np.dot(x_minus_mean, x_minus_mean.T)
    
    # 3. 计算类间散度矩阵(Sb)
    Sb = np.zeros((n_features, n_features))
    for c, mean in zip(classes, class_means):
        n_c = len(data[labels == c])  # 该类别的样本数
        mean_minus_overall = (mean - overall_mean).reshape(n_features, 1)
        Sb += n_c * np.dot(mean_minus_overall, mean_minus_overall.T)
    
    # 4. 求解Sw^(-1)*Sb的特征值和特征向量
    # 为避免Sw奇异，添加微小扰动（可选）
    Sw_reg = Sw + 1e-6 * np.eye(n_features)
    eigenvalues, eigenvectors = np.linalg.eig(np.dot(np.linalg.inv(Sw_reg), Sb))
    
    # 5. 选择最大的n_dim个特征值对应的特征向量
    sorted_indices = np.argsort(-eigenvalues)[:n_dim]
    W = eigenvectors[:, sorted_indices]  # 投影矩阵
    
    # 6. 将数据投影到新空间
    data_ndim = np.dot(data, W)
    
    return data_ndim, W

# 加载数据集
iris = load_iris()
X = iris.data  # 特征数据
y = iris.target  # 类别标签
class_names = iris.target_names  # 类别名称

# LDA降维（最多只能降到2维，因为鸢尾花有3个类别，3-1=2）
X_lda, W = lda(X, y, 2)

# 可视化降维结果
plt.figure(figsize=(8, 6))
for i, class_name in enumerate(class_names):
    mask = y == i
    plt.scatter(X_lda[mask, 0], X_lda[mask, 1],
                label=class_name,
                edgecolor='k',
                s=70)

plt.xlabel('LD 1 ')
plt.ylabel('LD 2 ')
plt.title('LDA of IRIS Dataset')
plt.legend()
plt.grid(linestyle='--', alpha=0.6)
plt.show()    
~~~

~~~
# sklearn实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.discriminant_analysis import LinearDiscriminantAnalysis
from sklearn.preprocessing import StandardScaler

# 加载数据集
iris = load_iris()
X = iris.data  # 特征数据
y = iris.target  # 类别标签
class_names = iris.target_names  # 类别名称

# 数据标准化（LDA对特征尺度也敏感，建议标准化）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用sklearn的LDA进行降维
# n_components最大只能是类别数-1（鸢尾花有3类，最大为2）
lda = LinearDiscriminantAnalysis(n_components=2)
X_lda = lda.fit_transform(X_scaled, y)  # LDA是有监督的，需要传入标签y

# 可视化降维结果
plt.figure(figsize=(8, 6))
for i, class_name in enumerate(class_names):
    mask = y == i
    plt.scatter(X_lda[mask, 0], X_lda[mask, 1],
                label=class_name,
                edgecolor='k',
                s=70)

plt.xlabel(f'LD 1 ')
plt.ylabel(f'LD 2 ')
plt.title('LDA of IRIS Dataset (using sklearn)')
plt.legend()
plt.grid(linestyle='--', alpha=0.6)
plt.show()    
~~~

