---

title: 深度学习（1）初识神经网路（MLP）
tag: DL
typora-root-url: ./..
---

从回归和分类过渡到神经网络。

<!--more-->

##### 1.神经元

神经网络起源于神经元的研究。一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。

![](/images/MLP/one.png)

基于此，有人提出了第一个神经元模型，包含3个输入，1个输出，以及2个计算功能

![](/images/MLP/two.png)

图上的箭头被称为“连接”，每个“连接”上有一个**权重**。

将神经元图中的所有变量用符号表示，并写出输出的计算公式

![](/images/MLP/three.png)

$$ z=g(a1 \cdot w1 + a2 \cdot w2 + a3 \cdot w3  )$$

将sum函数和sgn函数放在同一个圈内，代表神经元的内部计算，就得到了一个现在非常常见的**神经元**。

![](/images/MLP/4.png)

当然，输出z可以有多个箭头，但是值是一样的。当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用**“单元”（unit）**来指代。同时由于神经网络的表现形式是一个有向图，有时也会用**“节点”（node）**来表达同样的意思。 

如果还记得感知机模型的话，可以发现上述结构和感知机非常相似，sgn是指示函数。

##### 2.单层神经网络——感知机

在第一节的神经元模型“输入“位置添加神经元节点，标志其为”输入单元“，得到了下图

![](/images/MLP/5.png)

在感知机中，有两个层次，分别为输入层和输出层，输入层只负责传输数据，不做计算；输出层则需要对前面一层的输入进行计算。将需要计算的层次称为“计算层”，并把拥有计算层的网络称之为“单层神经网络”。（本文按照计算层数量对网络命名，一层计算层就是单层神经网络，两层计算层就是两层神经网络）

假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。

![](/images/MLP/6.png)

$z_1$与$z_2$两个输出的计算方式是一样，但是这样看不出区分度，所以用一个二维下标$w_{x,y}$来表示权重，其中x代表后一层神经元的序号，y代表前一层神经元的序号，于是就有了如下图和公式

![](/images/MLP/7.png)

$$ z_1 = g(a_1 \cdot w_{1,1} + a_2 \cdot w_{1,2} + a_3 \cdot w_{1,3}  ) $$

$$ z_2 = g(a_1 \cdot w_{2,1} + a_2 \cdot w_{2,2} + a_3 \cdot w_{2,3}  ) $$

可以发现，这两个公式就是线性代数方程组，因此可以用矩阵乘法来表达这两个公式，令输入为$\mathbf{a}=[a_1,a_2,a_3]^T$，输出为$\mathbf{z}=[z_1,z_2,z_3]^T$，权重为$\mathbf{w}$（两行三列矩阵），于是就有

$$\begin{bmatrix}
 w_{11} & w_{12} & w_{13} \\
 w_{21} & w_{22} & w_{23}
\end{bmatrix}\begin{bmatrix}
 a_{1}\\
 a_{2}\\
 a_{3}
\end{bmatrix}=\begin{bmatrix}
 z_{1}\\
 z_{2}
\end{bmatrix}$$

但是，正如之前讲述感知机模型的缺陷，这个模型只能做简单的线性分类任务，无法解决异或问题。

##### 3.两层神经网络——多层感知机（Multi-Layer Perceptron，MLP）

单层神经网络无法解决异或问题，但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。唯一的问题是，两层神经网络的权重参数非常多，计算非常复杂，难以进行优化。

直到反向传播（Backpropagation，BP）算法的出现，才使得两层神经网络的求解可以实现。

两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。由于节点和层数增多，所以用上标表示层数，下标表示节点序号。

![](/images/MLP/8.png)

$$ a_1^{(2)} = g(a_1^{(1)} \cdot w_{1,1}^{(1)} + a_2^{(1)} \cdot w_{1,2}^{(1)} + a_3 \cdot w_{1,3}^{(1)}  ) $$

$$ a_2^{(2)}  = g(a_1^{(1)} \cdot w_{2,1}^{(1)} + a_2^{(1)} \cdot w_{2,2}^{(1)} + a_3^{(1)} \cdot w_{2,3}^{(1)}  ) $$

$$ z = g(a_1^{(2)} \cdot w_{1,1}^{(2)} + a_2^{(2)} \cdot w_{1,2}^{(2)}) $$

如果预测目标是一个向量，与前面类似的在”输出层“增加节点就可以了，再用向量和矩阵表示网络中的变量

![](/images/MLP/9.png)

$$ g(\mathbf{w}^{(1)} \cdot \mathbf{a}^{(1)}) =\mathbf{a}^{(2)}$$

$$ g(\mathbf{w}^{(2)} \cdot \mathbf{a}^{(2)} ) =z$$

由此可见，使用矩阵运算来表达是很简洁的，因此在神经网络中通常使用矩阵运算来描述。

当然，神经网络还可以添加**偏置节点（bias unit）**，但本质上是一个只含有存储功能，且存储值永远为1的单元。在神经网络的每个层次中，除了输出层以外，都可以含有这样一个偏置单元，就像线性回归模型一样。偏置单元与后一层的所有节点都有连接，设这些参数值为向量$\mathbf{b}$，称之为偏置。（一般不画偏置）

![](/images/MLP/10.png)

$$ g(\mathbf{w}^{(1)} \cdot \mathbf{a}^{(1)} + \mathbf{b}^{(1)}) =\mathbf{a}^{(2)}$$

$$ g(\mathbf{w}^{(2)} \cdot \mathbf{a}^{(2)} + \mathbf{b}^{(2)}) =z$$

需要说明的是，在两层神经网络中，不再使用$sgn$函数作为函数$g$，而是使用平滑非线性函数$sigmoid$作为函数$g$，函数$g$也称作**激活函数（active function）**。

经过上述讨论，可以发现神经网络实际上和之前学的线性回归、Logistic回归没有太大的区别，都是通过参数来拟合特征与目标的真实函数关系，找到一个输入到输出的映射。

并且，和单层神经网络不同，在满足一定条件的前提下，两层的神经网络可以以任意精度逼近定义在紧致（有界闭）定义域上的任意连续函数，这也被称为**“通用近似定理”**。

不过，这里有一个问题，单层网络只能做线性分类任务，两层网络的后一层也是线性分类层，只能做线性分类任务，为什么两个线性分类层结合就可以实现非线性分类任务？

关键在于激活函数：在两层神经网络中，中间层的非线性激活函数将输入空间映射到一个新的非线性特征空间，此时输出层的 “线性分类” 本质上是在新的非线性特征空间中进行的，因此整体实现了非线性分类。

中间层既不直接接收原始输入，也不直接输出最终结果，在神经网络的输入与输出之间起到间接的特征转换作用，所以通常被称为**“隐藏层”**。

##### 4.多层神经网络

截至目前，已经将神经网络演变历史以及各个组件都讲清楚了，后续研究中，通常把两层及以上的神经网络称为多层神经网络，区别在于隐藏层的层数以及每层的节点数，如下图是一个三层神经网络。

![](/images/MLP/11.png)

除了上述所讲的感知机（单层神经网络）、多层感知机（多层神经网络），还有一些其他的名字需要了解。

- **全连接神经网络**：如果对每个相邻的两层，相邻的两层每个神经元之间都存在连接，即下一层神经元的输入是上一层所有神经元的输出的加权和，那么这个神经网络就是全连接网络（这种连接也称为稠密连接）。
- **前馈神经网络**：“前馈”描述的是神经网络结构的单向性，信息按照“输入层$\to $隐藏层$\to $输出层”的单向路径传递，没有反向的信息流，也没有层内或跨层的循环连接（反向传播是计算方向上的反向，不是信息的方向，注意区分）。


##### 5.激活函数

第三节说过，非线性激活函数将输入空间映射到一个新的非线性特征空间，使得神经网络可以逼近任意函数，接下来介绍一些常用的激活函数：

###### 5.1 Sigmoid函数

$$ sigmoid(x) = \frac{1}{1+e^{-x}}$$

特点：

- 输出范围在 (0, 1)，可表示概率，仅推荐用于二分类任务的输出层；
- 缺点是梯度消失严重（输入绝对值较大时导数接近 0），计算开销高于 ReLU。

###### 5.2 Tanh函数

$$ tanh(x) = \frac{e^x - e^{-x}}{e^x + e^{-x}}$$

特点：

- 相比 Sigmoid，输出以 0 为中心（均值更接近 0），梯度特性略好。

###### 5.3 Softmax函数

$$ softmax(x) = \frac{e^{x_i}}{\sum_{j} e^{x_j}}$$

特点：

- 输出值在 (0, 1) 之间，且所有输出之和为 1，可直接表示多分类的概率；
- 通常仅用于输出层，且需配合交叉熵损失函数使用。

###### 5.4 ReLU（Rectified Linear Unit，修正线性单元，2010）

$$ ReLU(x)=\left\{\begin{matrix}
x, x\geq 0 \\
0, x < 0
\end{matrix}\right.=max(0,x)$$

特点：

- 应用最广泛的激活函数，计算快，还能缓解梯度消失问题；
- 但输入为负时，输出恒为0，梯度为0无法更新，导致神经元“死亡”。

###### 5.5 ELU（Exponential Linear Unit，指数线性单元，2015）

$$ ELU(x)=\left\{\begin{matrix}
x \quad \quad \quad \ \ , if \quad x > 0 \\
\gamma (e^x-1), if \quad x \leq 0
\end{matrix}\right.=max(0,x)+min(0,\gamma (e^x-1))$$

其中 $𝛾 \geq 0$是一个超参数，决定$𝑥 \leq 0$时的饱和曲线，并调整输出均值在0附近。

###### 5.6 GELU（Gaussian Error Linear Unit，高斯误差线性单元，2016）

$$GELU(𝑥) = 𝑥𝑃(𝑋 \leq 𝑥),$$

其中$𝑃(𝑋 \leq 𝑥)$是高斯分布$N(\mu , \sigma^2)$的累积分布函数，其中$\mu , \sigma$为超参数，一般设

$\mu = 0, \sigma = 1$即可。

此外还有一些其他的激活函数（Maxout、Swish），但这里不过多赘述了，以后用到再做了解，对于当下来说，只需要知道ReLU、Sigmoid、Softmax这三个激活函数就已经够了。

##### 6.损失函数

既然神经网络要处理回归和分类任务，必然少不了损失函数。在学习回归和分类的时候说过，损失函数是用来衡量预测值和真实值之间的误差大小，通常通过最小化损失函数，建立优化目标，让模型有更好的表达能力。

###### 6.1 均方误差（MSE，Mean Squared Error）

$$L = \frac{1}{N} \sum_{i=1}^N (\hat{y}_i - y_i)^2$$
其中$\hat{y}_i$是预测值，$y_i$ 是真实值。

###### 6.2 平均绝对误差（MAE，Mean Absolute Error）

$$L = \frac{1}{N} \sum_{i=1}^N \vert \hat{y}_i - y_i \vert $$

其中$\hat{y}_i$是预测值，$y_i$ 是真实值。

###### 6.3 二分类交叉熵损失（Binary Cross-Entropy）

$$L = -\frac{1}{N} \sum_{i=1}^N [y_i \log(\hat{y}_i) + (1-y_i) \log(1-\hat{y}_i)]$$
其中$y_i \in \lbrace 0,1 \rbrace$ 是真实标签，$\hat{y}_i \in (0,1)$是模型预测的正类概率（由Sigmoid 输出）。

###### 6.4 多分类交叉熵损失（Categorical Cross-Entropy）

$$L = -\frac{1}{N} \sum_{i=1}^N \sum_{c=1}^C y_{i,c} \log(\hat{y}_{i,c})$$
其中 $y_{i,c}$是真实标签的独热编码（正确类别为 1，其余为 0），$\hat{y}_{i,c}$是模型预测的类别概率（由Softmax 输出）。

##### 7.反向传播

