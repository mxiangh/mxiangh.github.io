---

title: 深度学习（1）初识神经网路
tag: DL
typora-root-url: ./..
---

从回归和分类过渡到神经网络。

<!--more-->

##### 1.神经元

神经网络起源于神经元的研究。一个神经元通常具有多个树突，主要用来接受传入信息；而轴突只有一条，轴突尾端有许多轴突末梢可以给其他多个神经元传递信息。轴突末梢跟其他神经元的树突产生连接，从而传递信号。这个连接的位置在生物学上叫做“突触”。

![](/images/MLP/one.png)

基于此，有人提出了第一个神经元模型，包含3个输入，1个输出，以及2个计算功能

![](/images/MLP/two.png)

图上的箭头被称为“连接”，每个“连接”上有一个权重，和线性回归一样，神经元模型可以通过训练权重提升模型的预测效果。和线性回归不同的是，神经元添加了一个非线性函数，目的是使得函数能够预测非线性的数据。如果只是堆砌线性函数，最后的输出也只是一个线性预测。

将神经元图中的所有变量用符号表示，并写出输出的计算公式

![](/images/MLP/three.png)

$$ z=sgn(a1 \cdot w1 + a2 \cdot w2 + a3 \cdot w3  )$$

可以发现，神经元模型实际上就是把一个线性函数的输出经过激活函数转换成一个非线性输出。将sum函数和sgn函数放在同一个圈内，代表神经元的内部计算，就得到了一个现在非常常见的神经元

![](/images/MLP/4.png)

当然，输出z可以有多个箭头，但是值是一样的。当我们用“神经元”组成网络以后，描述网络中的某个“神经元”时，我们更多地会用“单元”（unit）来指代。同时由于神经网络的表现形式是一个有向图，有时也会用“节点”（node）来表达同样的意思。 

##### 2.单层神经网络——感知机

在第一节的神经元模型“输入“位置添加神经元节点，标志其为”输入单元“，得到了下图

![](/images/MLP/5.png)

在感知机中，有两个层次，分别为输入层和输出层，输入层只负责传输数据，不做计算；输出层则需要对前面一层的输入进行计算。将需要计算的层次称为“计算层”，并把拥有一个计算层的网络称之为“单层神经网络”。（本文按照计算层数量对网络命名，一层计算层就是单层神经网络，两层计算层就是两层神经网络）

假如我们要预测的目标不再是一个值，而是一个向量，例如[2,3]。那么可以在输出层再增加一个“输出单元”。

![](/images/MLP/6.png)

Z1与Z2两个输出的计算方式是一样，但是这样看不出区分度，所有用一个二维下标$w_{x,y}$来表示权重，其中x代表后一层神经元的序号，y代表前一层神经元的序号，于是就有了如下图和公式

![](/images/MLP/7.png)

$$ z_1 = sgn(a_1 \cdot w_{1,1} + a_2 \cdot w_{1,2} + a_3 \cdot w_{1,3}  ) $$

$$ z_2 = sgn(a_1 \cdot w_{2,1} + a_2 \cdot w_{2,2} + a_3 \cdot w_{2,3}  ) $$

可以发现，这两个公式就是线性代数方程组，因此可以用矩阵乘法来表达这两个公式，令输入为$\mathbf{a}=[a_1,a_2,a_3]^T$，输出为$\mathbf{z}=[z_1,z_2,z_3]^T$，权重为$\mathbf{w}$（两行三列矩阵），于是就有

$$\begin{bmatrix}
 w_{11} & w_{12} & w_{13} \\
 w_{21} & w_{22} & w_{23}
\end{bmatrix}\begin{bmatrix}
 a_{1}\\
 a_{2}\\
 a_{3}
\end{bmatrix}=\begin{bmatrix}
 z_{1}\\
 z_{2}
\end{bmatrix}$$

但是，正如之前讲述感知机模型的缺陷，这个模型只能做简单的线性分类任务，无法解决异或问题。

##### 3.两层神经网络——多层感知机MLP

单层神经网络无法解决异或问题，但是当增加一个计算层以后，两层神经网络不仅可以解决异或问题，而且具有非常好的非线性分类效果。唯一的问题是，两层神经网络的权重参数非常多，计算非常复杂，难以进行优化。

直到反向传播（Backpropagation，BP）算法的出现，才使得两层神经网络的研究顺利进行。

两层神经网络除了包含一个输入层，一个输出层以外，还增加了一个中间层。此时，中间层和输出层都是计算层。我们扩展上节的单层神经网络，在右边新加一个层次（只含有一个节点）。由于节点和层数增多，所以用上标表示层数，下标表示节点序号。

![](/images/MLP/8.png)

$$ a_1^{(2)} = sgn(a_1^{(1)} \cdot w_{1,1}^{(1)} + a_2^{(1)} \cdot w_{1,2}^{(1)} + a_3 \cdot w_{1,3}^{(1)}  ) $$

$$ a_2^{(2)}  = sgn(a_1^{(1)} \cdot w_{2,1}^{(1)} + a_2^{(1)} \cdot w_{2,2}^{(1)} + a_3^{(1)} \cdot w_{2,3}^{(1)}  ) $$

$$ z = sgn(a_1^{(2)} \cdot w_{1,1}^{(2)} + a_2^{(2)} \cdot w_{1,2}^{(2)}) $$

如果预测目标是一个向量，与前面类似的在”输出层“增加节点就可以了，再用向量和矩阵表示网络中的变量

![](/images/MLP/9.png)

$$ sgn(\mathbf{w}^{(1)} \cdot \mathbf{a}^{(1)}) =\mathbf{a}^{(2)}$$

$$ sgn(\mathbf{w}^{(2)} \cdot \mathbf{a}^{(2)} ) =z$$

由此可见，使用矩阵运算来表达是很简洁的，因此在神经网络中通常使用矩阵运算来描述。

