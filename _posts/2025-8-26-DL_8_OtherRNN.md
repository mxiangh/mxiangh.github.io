---
title: 深度学习（8）深度RNN和双向RNN
tags: DL RNN
typora-root-url: ./..
---

更深的循环神经网络和双向的循环神经网络。

<!--more-->

 ##### 1.深度循环神经网络

之前学习的RNN、LSTM、GRU及实现的语言模型，都是单个隐藏状态层，本篇介绍多层循环神经网络。

![](/images/RNN/6.png)

上图是一个具有$L$个隐藏层的深度循环神经网络，每个隐状态都连续地传递到当前层的下一个时间步和下一层的当前时间步。

假设在时间步$t$有一个小批量的输入数据$X$（样本数：n，每个样本中的输入数：d）。同时，将当前时间步的$l$隐藏层的隐状态设为$H_t^{(l)} $（隐藏单元数：h），输出层变量设为$O$（输出数：q）。设置$H_t^{(0)}=X_t$，第$l$个隐藏层的隐状态使用激活函数$g$，则：
$$\mathbf{H}_t^{(l)}=g(\mathbf{H}_t^{(l-1)} \mathbf{W}_{xh}^{(l)} + \mathbf{H}_{t-1}^{(l)} \mathbf{W}_{hh}^{(l)} + \mathbf{b}_h^{(l)})$$
其中，权重和参数是第$l$个隐藏层的模型参数。最后，输出层的计算仅基于第$l$个隐藏层最终的隐状态（最后一个隐藏层的最后一个隐状态）：

$$ \mathbf{O}_t = \mathbf{H}_{t}^{(L)} \mathbf{W}_{hq} + \mathbf{b}_q$$

与多层感知机一样，隐藏层数目$L$和隐藏单元数目$h$都是超参数。

##### 2.简洁实现多层LSTM

这里沿用简洁LSTM的大部分代码。

~~~
~~~

