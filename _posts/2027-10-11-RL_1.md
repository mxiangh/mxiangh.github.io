---
title: 强化学习（1）基础概念——基础第一节
tags: RL
typora-root-url: ./..
---

介绍状态、行动、状态转移、状态转移概率、策略、奖励、奖励概率、回报、轨迹、回合（试验）、马尔科夫决策过程。

<!--more-->

注：本文以赵世钰课程为基础，对知识进行总结，这是第一节，一共10节。

##### 1.引入

强化学习讨论的问题是一个智能体（agent）怎么在一个复杂不确定的环境（environment）里面去极大化它能获得的奖励。

![](/images/RL/1.png)

在本文及后文的例子中，agent是一个机器人，环境就是这个九宫格，agent需要从start走到target，其中forbidden是禁止区。

##### 2.state（状态）

定义：环境的描述，表示当前的“处境”或“情境”。

在这个例子中，状态就是agent在九宫格的位置，这里一共有九个位置，也就有九个state：$s_1, s_2, \cdots, s_9$.

所有状态的集合被称为状态空间（state space）：$S=\lbrace s_i \rbrace_{i=1}^9$.

![](/images/RL/2.png)

##### 3.action（行动）

定义：在每一个状态可以采取的行动。

在这个例子中，每个状态共有五个行动，$a_1$为向上(upwards) ，$a_2$为向右(rightwards) ，$a_3$为向下(downwards) ，$a_4$为向左(leftwards) ，$a_5$为原地不动 (unchanged)。

![](/images/RL/3.png)

每个状态所有可采取的行动被称为行动空间（action space）：$A(s_i)= \lbrace a_i \rbrace_{i=1}^5$.

在这个例子中，每个状态的行动空间是一样的，但是在其他案例中，行动空间可能不一样。

##### 4.state transition（状态转移）

 当agent在当前状态下选择某个行动后，会进入下一个状态，这个过程被称为状态转移。

例1，在状态$s_1$选择行动$a_2$：

$$ s_1 \overset{a_2}{\rightarrow}  s_2 $$

例2，在状态$s_1$选择行动$a_1$：

$$ s_1 \overset{a_1}{\rightarrow}  s_1 $$

当然，我们可以使用不同的方式去定义状态转移，例如在$s_5$选择$a_2$可以有不同的结果：

结果1，如果forbidden允许agent进入，但是会受到惩罚：

$$ s_5 \overset{a_2}{\rightarrow}  s_6 $$

结果2，如果forbidden不允许agent进入：

$$ s_5 \overset{a_2}{\rightarrow}  s_5 $$

在本文中，只考虑结果1的情形。此时，我们可以使用一个表格来描述所有的状态转移：

![](/images/RL/4.png)

我们可以用条件概率来表示状态转移的可能性（state transition probability），简称状态转移概率，例如在状态$s_1$，选择行动$a_2$，进入下一个状态$s_2$：

$$\begin{aligned}
p\left(s_{2} \mid s_{1}, a_{2}\right) & =1 \\
p\left(s_{i} \mid s_{1}, a_{2}\right) & =0 \quad \forall i \neq 2
\end{aligned}$$

上述的情况是**确定性**的，即**在同一个状态、同一个行动下，只有一个结果**。但是如果有其他影响，状态转移可能是不确定的。例如，有风的时候，$s_1$选择$a_2$时，可能会进入$s_2$，也可能进入$s_5$。

注：这张图对于理解贝尔曼公式很重要。

![](/images/RL/5.png)

##### 5.policy（策略）

策略用来告诉agent在当前状态选择哪个行动进入下一个状态。例如，可以用箭头表示在某个状态的最佳策略。

![](/images/RL/6.png)

基于这些策略，我们能得到不同起始点到终点的路径。

![](/images/RL/7.png)

使用条件概率来描述在当前状态选择某个行动的概率，如果在任意状态选择某个行动的概率只有0和1两种可能，则为确定性策略（deterministic policy）：

$$\begin{aligned}
\pi (a_{1} \mid s_{1} ) & =0 \\
\pi (a_{2} \mid s_{1} ) & =1 \\
\pi (a_{3} \mid s_{1} ) & =0 \\
\pi (a_{4} \mid s_{1} ) & =0 \\
\pi (a_{5} \mid s_{1} ) & =0
\end{aligned}$$

否则为随机性策略（stochastic policies）：

$$\begin{aligned}
\pi (a_{1} \mid s_{1} ) & =0 \\
\pi (a_{2} \mid s_{1} ) & =0.5 \\
\pi (a_{3} \mid s_{1} ) & =0.5 \\
\pi (a_{4} \mid s_{1} ) & =0 \\
\pi (a_{5} \mid s_{1} ) & =0
\end{aligned}$$

注：这里注意区分前文讲述的确定性和不确定性。

可以用一个表格来描述所有的策略。

![](/images/RL/8.png)

##### 6.reward（奖励）

agent执行一个行动后，可能会进入forbidden，或进入target，又或者进入普通区域，对于不同的结果通常会有不同的收获，这种收获用奖励来描述。

正向的奖励表示鼓励这种行动。

负向的奖励表示惩罚这种行动。

此外，0表示没有惩罚，正向也可能有惩罚的作用。

![](/images/RL/2.png)

在这个例子中，我们定义一下几种情况：

（1）如果agent尝试越过边界，则$r_{bound}=-1$；

（2）如果agent尝试进入forbidden，则$r_{forbidden}=-1$；

（3）如果agent到达taget，则$r_{taget}=-1$；

（4）其他情况下，$r = 0$。

奖励可以看成是一种人机交互，我们能引导agent做出我们所期望的行为。

在某个状态选择某个行动后得到的奖励称为奖励转移（reward transition），即agent在状态转移过程中，奖励的变化过程

同样地，可以用一个表格来描述奖励转移。

![](/images/RL/9.png)

同条件概率来描述奖励转移，例如，在状态$s_1$选择行动$a_1$。得到的reward是$-1$：

$$ p(r=-1 \mid s1,a1) = 1 \  and \  p(r\ne-1 \mid s1,a1) = 0$$

和状态转移一样，奖励转移可能是不确定性的。

##### 7.trajectory and return（轨迹和回报）

![](/images/RL/10.png)

一条轨迹指的是一条“状态-行动-奖励”链：

$$ s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 $$

回报是指一条轨迹上所有奖励之和：

$$return = 0+0+0+1=1$$

![](/images/RL/11.png)

对于不同的策略，它的轨迹通常也不一样：

$$ s_1 \xrightarrow[r=0]{a_3} s_4 \xrightarrow[r=-1]{a_3} s_7 \xrightarrow[r=0]{a_2} s_8 \xrightarrow[r=1]{a_2} s_9 $$

这条轨迹的回报为：

$$return = 0-1+0+1=0$$

从直觉上，第一个策略会比第二个好，因为它没有经过forbidden区域。

从数学上，第一个策略也比第二个好，因为它的回报更大。

所以，回报可以用来评估一个策略的好坏。

##### 8.discounted return（折扣回报）

![](/images/RL/12.png)

一条轨迹可能是无穷的，当它达到target后，如果想要获得奖励，就会一直停在$s_9$：

$$ s_1 \overset{a_2}{\rightarrow}  s_2 \overset{a_3}{\rightarrow}  s_5 \overset{a_3}{\rightarrow}  s_8 \overset{a_2}{\rightarrow}  s_9 \overset{a_5}{\rightarrow}  s_9 \overset{a_5}{\rightarrow}  s_9 \dots $$

此时回报也是无穷的：

$$return = 0+0+0+1+1+1+ \cdots = \infty $$

这种定义没有意义，因为回报发散了。于是，这里引出折扣因子（discount factor）$\gamma = \lbrack0,1)$。

将$\gamma$和回报结合，得到了折扣回报：

$$ \begin{aligned}
\text { discounted return } & =0+\gamma 0+\gamma^{2} 0+\gamma^{3} 1+\gamma^{4} 1+\gamma^{5} 1+\ldots \\
& =\gamma^{3} (1+\gamma+\gamma^{2}+\ldots )=\gamma^{3} \frac{1}{1-\gamma} 
\end{aligned}$$

这里有两个作用：

（1）将总和变为有限；

（2）平衡即时奖励和未来奖励。

如果$\gamma$接近0，那么折扣回报的值由近期获得的奖励为主，即即时奖励占主导；

如果$\gamma$接近1，那么折扣回报的值由远期获得的奖励为主，即未来奖励占主导。

因为$\gamma$越小，平方后越小，靠后的奖励乘上$\gamma$也会越小，远期的奖励占比小；反之，近期的奖励占比小。

##### 9.episode or trial（回合或试验）

使用策略与环境交互时，agent可能在终端state停止，由此产生的轨迹被称为一个回合或一次试验。

![](/images/RL/10.png)

例如：

$$ s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 $$

通常假设一个回合是一条有限的轨迹，具有回合的任务被称为情景任务。

有些任务可能没有终端状态，这意味着与环境的交互永远不会结束，这种任务称为持续性任务。在上述的网格例子中，考虑以下两种情形：

情形1，将target看成一种终端状态，一旦agent进入target，就不再离开，r总是为0，此时是情景任务。

情形2，将target看成一种正常状态，agent可以使用policy离开target，并且再次进入时，r=1，此时是持续任务。

本文考虑第二种情形，故我们不需要区分taget state和其他的state。

##### 10.Markov decision process (MDP) 马尔科夫决策过程

集合：状态集合$S$，每个状态的动作集合$A(s)$，在某个状态选择某个动作的奖励集合$R(s,a)$。

概率分布：状态转移概率$p(s_{i+1} \mid s_i,a)$，奖励概率$p(r \mid s,a)$。

策略：在状态$s$，选择行动$a$的概率$ \pi (a \mid s)$。

马尔科夫过程Markov process：

$$p(s_{t+1} \mid a_t, s_t, \dots , a_0,s_0) = p(s_{t+1} \mid a_t , s_t ) $$

$$p(r_{t+1} \mid a_t, s_t, \dots , a_0,s_0) = p(r_{t+1} \mid a_t , s_t ) $$

马尔科夫过程是没有记忆性的，即下一个状态只取决于当前状态，与过去的状态无关。例如在上面的例子中，agent进入下一个状态时，只取决于在当前的状态选择的某个行动，至于之前怎么到达当前状态，并不需要考虑。

将网格例子抽象为一般化模型：

![](/images/RL/13.png)

右图中，圆圈代表状态，带箭头的链接代表状态转换，这是一个马尔科夫过程（MP）。

当给出奖励时，即一个状态到下一个状态后有一个奖励，就变成马尔科夫奖励过程（MRP）。

如果再提供一个行动，即在某个状态选择了一个行动，就变成马尔科夫决策过程（MDP）。

综上，MDP在MP的基础上，只不过增加了奖励和策略。
