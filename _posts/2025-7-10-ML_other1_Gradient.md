---
title: 机器学习补充（1）梯度下降与随机梯度下降
tags: ML Tips
---

介绍一下，梯度、梯度下降、梯度上升、随机梯度下降、小批量梯度下降。

<!--mare-->

##### 1.梯度

- 在单变量函数中，梯度是函数的微分，代表函数在某个给定点的切线的斜率。
- 在多变量函数中，梯度就是一个向量，向量有方向，梯度的方向指出了函数在给定点上升最快的方向。

既然梯度是函数在给定点上升最快的方向，那么梯度的反方向就是函数在给定点下降最快的方向。

##### 2.梯度上升与梯度下降（Gradient Descent）

首先给出数学公式：

$$ \Theta^1 = \Theta^0 + \alpha \nabla J(\Theta)$$

$$ \Theta^1 = \Theta^0 - \alpha \nabla J(\Theta)$$

其中：

$  \Theta^0 $是当前所处位置，$J$是关于$\Theta$的函数，$\bigtriangledown J(\Theta)$是$\Theta$的梯度，也是上升最快的方向，$\alpha$是距离的步长（也是超参数，需要自己定义），所以$\alpha \bigtriangledown J(\Theta)$是朝着当前梯度方向走的一段步长。在当前位置，走一段步长，就能到达下一个点$ \Theta^1$。

梯度前面的符号很关键，如果是“+”，则走上坡路，此时这个算法代表的是梯度上升法；

如果是“-”，则走下坡路。此时这个算法代表的是梯度下降法。

例如，在求解参数w和b时，通常会使用梯度下降法：

$$ w^1 = w^0 - \alpha \nabla J(w)$$

$$ b^1 = b^0 - \alpha \nabla J(b)$$

##### 3.批量梯度下降（Batch Gradient Descent）

梯度下降法也称为批量梯度下降法，更新每一参数时都使用所有的样本来进行更新。

- 优点：每次更新都会朝着正确的方向进行，最后能够保证收敛于极值点(凸函数收敛于全局极值点，非凸函数可能会收敛于局部极值点)。
- 缺点：每次学习时间过长，如果训练集很大以至于需要消耗大量的内存，并且全量梯度下降不能进行在线模型参数更新。

##### 4.随机梯度下降（Stochastic Gradient Descent，SGD）

批量梯度下降法的更新，每次更新参数时只随机抽取一个样本参与计算。

- 优点：既可以减少迭代次数，节省计算时间，又可以防止内存溢出，降低了计算开销。
- 缺点：每次更新可能并不会按照正确的方向进行，因此会带来优化波动(扰动)，即参数更新频率太快，有可能出现目标函数值在最优值附近的震荡现象，并且高频率的参数更新导致了高方差。

从另一个方面来看，随机梯度下降所带来的波动可能会使得优化的方向从当前的局部极小值点跳到另一个更好的局部极小值点，这样便可能对于非凸函数，最终收敛于一个较好的局部极值点，甚至全局极值点。

随机梯度下降虽然提高了计算效率，但是由于每次迭代只随机选择一个样本，因此随机性比较大，所以下降过程中非常曲折。

##### 5.小批量梯度下降(Mini-batch Gradient Descent)

小批量梯度下降是介于上述两种方法之间的优化方法，即在更新参数时，只使用一部分样本（一般256以下）来更新参数，这样既可以保证训练过程更稳定，又可以利用批量训练方法中的矩阵计算的优势

##### 6.动量梯度下降法（Momentum Gradient Descent）

“动量”（Momentum）这个词来自物理，就像生活里的 “惯性”：推一个箱子，推一下后松手，箱子不会立刻停下，会因为惯性再滑一段距离。

动量梯度下降里的 “动量”，就给梯度下降加了这种 “惯性”：每次迈步的方向，不仅看当前脚下的 “梯度（最陡方向）”，还会带上之前迈步的 “惯性趋势”。

第一步：更新惯性速度

$$ v = \gamma \cdot v_{prev} - \alpha \nabla L(w)$$

- $ v_{prev} $: 上一步的惯性速度，初始设置为0。
- $\gamma \cdot v_{prev}$: 保留之前的惯性（比如 $\gamma$=0.9，就保留 90% 的之前速度），$\gamma$也是超参数，被称为动量。
- $-\alpha \nabla L(w)$: 沿着梯度方向走的距离，$\alpha$是学习率或称步长。

第二步：更新当前位置

$$ w = w_{prev} + v$$

- 相当于 “带着当前的速度（v）向前走一步”，把惯性和当前梯度的方向结合起来。

优点：加快收敛速度，减少震荡，容易跳出局部最优解。

缺点：$\gamma$太大会错过最优解，超参数调优略复杂。
