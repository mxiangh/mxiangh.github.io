---
layout: atricle
title: 机器学习（x）拉普拉斯特征映射LE
tags: ML Dimensionality_Reduction
---

#### 拉普拉斯特征映射——Laplacian Eigenmaps，LE

原论文：Laplacian Eigenmaps for Dimensionality Reduction and Data Representation

参考博客:[LE](https://blog.csdn.net/yixon_oss/article/details/133086789)

思想：一种无监督的基于图的非线性降维方法，其基本思想是从高维空间向低维空间映射过程中保持数据间原有的局部结构特性（相似性）。

<!--more-->

目标：原空间中相近的点在映射到新的低维空间中时仍然比较相近。

拉普拉斯特征映射基本步骤：

1. 构建无向图，将所有的样本以点连接成一个图;
2. 构建图的权值矩阵，通过点之间的关联程度来确定点与点之间的权重大小;
3. 特征映射，通过公式$L_y=入D_y$,计算拉普拉斯矩阵L的特征向量和特征值，用最小的m个非零特征值对应的特征向量作为降维的结果。

步骤1：构建邻接图，如果节点$x_i$和$x_j$是邻近点，则形成一条连接边。

(1) $\varepsilon$邻域法。当$||x_i-x_j||^2<=\varepsilon$时，建立连接

- 优点：几何意义明确，关系具有传递性
- 缺点：易产生不连通子图，$\varepsilon$值难以选择

(2)k近邻法。若$x_i$和$x_j$是k个最近邻则连接

- 优点：参数易设定，通常保证图连通性
- 缺点：几何直观性较弱

Q：为什么需要构建邻接图？

答：捕捉高维数据中潜在的局部几何结构，定义了 “哪些点属于局部范围”。

步骤2：选择权重，在节点$x_i$和$x_j$的连接边上赋予权重$W_{ij}$，得到图的邻接矩阵W。

(1)热核函数，对连接节点的连接边赋予权重

$$
W_{i j}=\left\{\begin{array}{l}
e^{-\frac{\left\|x_{i}-x_{j}\right\|^{2}}{t}}(t \in R), \text { connected } \\
0, \text { else }
\end{array}\right.
$$

(2)简易方法

$$
W_{i j}=\left\{\begin{array}{l}
1, \text { connected } \\
0, \text { else }
\end{array}\right.
$$

Q：为什么需要设置权重？

答：定义点与点之间的关联紧密程度。

步骤3：特征映射

(1)设数据在低维的映射$ y=\{y_1,y_2,...,y_n\}^T $，定义优化目标函数

$$ min \ \sum_{i,j}||y_i-y_j||^2 \ W_{ij} $$

其中，$W_{ij}$是邻接矩阵W的元素，对于热核函数，距离远则权重小，距离近则权重大，惩罚实际近，映射远的两个点。

(2)目标函数优化

度矩阵D：邻接矩阵W每行相加放在对角线所得矩阵，即$D_{ii} = \sum_{j=1}^n W_{ij}$

拉普拉斯矩阵：L = D - W

$$
\begin{array}{l}
\sum_{i=1}^{n} \sum_{j=1}^{n}\left\|y_{i}-y_{j}\right\|^{2} W_{i j} \\
=\sum_{i=1}^{n} \sum_{j=1}^{n}\left(y_{i}^{T} y_{i}-2 y_{i}^{T} y_{j}+y_{j}^{T} y_{j}\right) W_{i j} \\
=\sum_{i=1}^{n}\left(\sum_{j=1}^{n} W_{i j}\right) y_{i}^{T} y_{i}+\sum_{j=1}^{n}\left(\sum_{i=1}^{n} W_{i j}\right) y_{j}^{T} y_{j}-2 \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i}^{T} y_{j} W_{i j} \\
=2 \sum_{i=1}^{n} D_{i i} y_{i}^{T} y_{i}-2 \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i}^{T} y_{j} W_{i j} \\
=2 \operatorname{tr}\left(Y^{T} D Y\right)-2 \operatorname{tr}\left(Y^{T} W Y\right) \\
=2 \operatorname{tr}\left[Y^{T}(D-W) Y\right] \\
=2 \operatorname{tr}\left(Y^{T} L Y\right)
\end{array}
$$

简化后的优化目标函数为

$$ min \ \operatorname{tr}\left(Y^{T} L Y\right) $$

这时，发现目标函数$ min \ \operatorname{tr}\left(Y^{T} L Y\right) $可以通过简单地缩放Y，来无限减小其值，导致优化问题无界，无法得到有意义的解。

例如，当$ Y \to \alpha Y $时

$ tr(Y^{T} L Y) =  tr((\alpha Y)^{T} L (\alpha Y)) = \alpha^2 tr(Y^{T} L Y)$

所以增加约束条件$Y^T D Y = I$，该约束强制Y的列向量在加权内积（由D定义）下是标准正交的，从而避免了因缩放而导致的解不唯一或无界的问题。

这一约束相当于将嵌入Y限制在一个“单位球”上（由D定义的度量空间），从而确保优化问题的解在合理的范围内，且具有明确的几何或物理意义。

(3)拉格朗日乘子法

$$ min \ \operatorname{tr}\left(Y^{T} L Y\right)  + tr(\lambda (Y^T D Y - I))$$

对Y求偏导取0，得

$$ LY = \lambda DY $$ 

最后选择最小的m个非零特征值对应的特征向量作为降维后的结果输出。

优势：
- 通过图的拉普拉斯矩阵L捕获数据点之间的局部邻域关系，能够有效保留高维数据中的局部几何结构。

- 基于图拉普拉斯算子的谱理论，具有良好的数学基础，与热扩散、流形学习等理论相关。

局限性：

- 性能受邻域大小 K 或 ϵ 的影响较大，选择不当可能导致降维效果变差。

- 当数据点数量N很大时，计算非常昂贵，难以扩展到超大规模数据集。

- 只能对训练数据进行降维，无法处理新样本。

- 全局结构可能无法完全保留，特别是当数据具有复杂拓扑时。