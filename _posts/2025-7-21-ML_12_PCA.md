---

title: 机器学习（12）主成分分析——Principal Component Analysis, PCA
tags: ML Dimensionality_Reduction
typora-root-url: ./..
---

#### 主成分分析——Principal Component Analysis, PCA

论文：On lines and planes of closest fit to systems of points in space

思想：主成分分析是一种无监督的线性降维方法，其目的是将n维数据通过重新构建正交基找到数据点在低维空间的投影，通过最大化降维后所有点之间的散度，求得降维后低维点的位置。

<!--more-->

原理解释：

1.为什么构建正交基？

1.1 内积与投影

两个维数相同的向量的内积被定义为：

$$\left(a_{1}, a_{2}, \cdots, a_{n}\right)^{\top} \cdot\left(b_{1}, b_{2}, \cdots, b_{n}\right)^{\top}=a_{1} b_{1}+a_{2} b_{2}+\cdots+a_{n} b_{n}$$

内积运算将两个向量映射为一个实数，其计算方式非常容易理解，但是其意义并不明显，

几何意义：

假设A和B是两个n维向量，n维向量可以等价表示为 n 维空间中的一条从原点发射的有向线段，假设 A 和 B 均为二维向量，则

$$A=\left(x_{1}, y_{1}\right), B=\left(x_{2}, y_{2}\right)$$

那么在二维平面上A和B可以用两条发自原点的有向线段表示，如下图：

![](/assets/images/PCA/one.png)

现在从A点向B所在直线引入一条垂线，垂线与B的交点叫做A在B上的投影（投影是一个点！），再假设  A  与  B  的夹角为  a  ，则投影的矢量长度为

$$|A| |B| \cos (a)$$

其中 $ \vert A \vert =\sqrt{x_{1}^{2}+y_{1}^{2}} $ 是向量A的模，也就是A线段的标量长度。

注意这里区分了矢量长度和标量长度，标量长度总是大于等于 0 ，值就是线段的长度；而矢量长度可能为负，其绝对值是线性长度，而符号取决于其方向与标准方向相同或者相反。

将内积表示为另一种方式：

$$A \cdot B=|A||B| \cos (a)$$

 A 与 B 的内积等于 A 到 B 的投影长度乘以 B 的模，在进一步，如果假设B的模为 1 ，即让$\vert B \vert=1$  ，那么就变成了下面公式（也就是上面说的投影的矢量长度）：

$$A \cdot B=|A| \cos (a)$$

也就是说，设向量  B  的模为 1 ，则  A  与  B  的内积值等于  A  向  B  所在直线投影的矢量长度！这就是内积的一种几何解释，也就是得到的第一个重要结论。

1.2 基

下面继续在二维空间讨论向量，一个二维向量可以对应二维笛卡尔直角坐标系中从原点出发的一条有向线段，例如下面这个向量：

![](/assets/images/PCA/two.png)

在代数表示方面，经常使用线段终点的点的坐标表示向量，例如上面的向量可以表示为（3,2），但只有一个（3,2）本身是不能够精确表示一个向量。

这里的坐标（3,2）实际上表示的是向量在x轴上的投影值3，在y轴上的投影值为2。也就是说其隐式引入一个定义：以x轴和y轴上正方向长度为1的向量为标准，那么一个向量（3,2）实际上是说在x轴投影为3而y轴投影为2。

注意投影是一个矢量，所以可以为负。

更正式的说，向量（x,y）实际上表示线性组合：

$$x(1,0)^T+y(0,1)^T$$

不难证明所有二维向量都可以表示为这样的线性组合，此处（1,0）和（0，1）叫做二维空间的一组基坐标。

![](/assets/images/PCA/three.png)

所以，要准确描述向量，首先要确定一组基，然后给出基所在的各个直线上的投影值。任何两个线性无关的二维向量都可以成为一组基，所谓线性无关在二维平面内可以直观认为是两个不再一条直线上的向量。此外，当基的模是1时，以方便的用向量点乘基而直接获得其在新基上的坐标了！

另外这里要注意的是，上述列举的例子中基是正交的（即内积为0，或者说相互垂直），但是可以成为一组基的唯一要求就是线性无关，非正交的基也是可以的，不过因为正交基有较好的性质，所以一般使用的基都是正交的。

1.3 为什么构建正交基？

回到一开始的问题，首先PCA想要找到数据在低维坐标的向量表示，而向量表示需要确定一组基，给出基所在各个直线上的投影。

此外，当基的模长为1时，原始向量与基的内积就是原始向量在基上的投影。

使用正交基是因为正交基好求解。

所以，正交基并令其模长为1，就可以通过与原始向量内积的方式获得投影。

2.标准化处理

2.1 中心化

在数据点周围构建的正交基向量不一定以原点为中心，所以在一开始需要对数据中心化处理，将每个数据减去所有数据的均值，并围绕原点分布，消除数据在绝对位置上的差异

$$ \hat{x} = x - \frac{1}{x} \sum_{i=1}^{n} x_i $$

2.2 标准化

对于不同特征的单位 / 尺度不同时（例如：身高用 cm，体重用 kg），方差大的特征会在 PCA 中被过度重视，也被称为量纲不一致。所以要去除量纲的影响，保证每个特征权重一样

$$ \hat{x} = \frac{x - \bar{x}}{s} $$

$$ s = \sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}$$

也就是将中心化后的结果再除以标准差，s是样本标准差。

3.定义损失函数

我们希望降维后数据点之间尽量保留原有信息，两点之间距离越大，差异越明显，信息越多，如果两个点挨得很近甚至重叠，那这两个点所表示的信息就几乎一样。所以需要让降维后的数据点之间尽可能远，而降维后的数据点其实就是投影点，投影点可以用原始向量与基的内积获得，两个投影点越远越好可以采用欧式距离，则有

$$ max \  \sum_{i=1}^{n}||w^Tx_i-w^Tx_j||^2 $$

$$ max \ \sum_{i=1}^{n}||w^T(x_i-x_j)||^2 $$

$$ max \ \sum_{i=1}^{n}w^T(x_i-x_j)(x_i-x_j)^Tw $$

$$ max \ w^T\{\sum_{i=1}^{n}(x_i-x_j)(x_i-x_j)^T\}w $$

其中，$\sum_{i=1}^{n}(x_i-x_j)(x_i-x_j)^T$正比于协方差矩阵$\sum_{i=1}^{n}(x_i-\mu)(x_i-\mu)^T$，又因为中心化后$\mu=0$，所以有

$$ max \ w^T \sum_{i=1}^{n}x_i x_i^T w $$

注意，在这里，$\sum_{i=1}^{n}x_i x_i^T$是协方差矩阵，这在后面的很多算法中都有涉及。

4.约束条件

正交基是单位矩阵，即$ww^T=I$

5.拉格朗日乘子法

将约束函数转化为无约束函数

$$ max \ w^T \sum_{i=1}^{n}x_i x_i^T w + \lambda (I- ww^T)$$

求最值，令偏导函数为0，得

$$  2 \sum_{i=1}^{n}x_i x_i^T w + \lambda (- 2w) = 0$$

$$  \sum_{i=1}^{n}x_i x_i^T w = \lambda w $$

代入原损失函数得到

$$ max \ \lambda \ w^T w $$

由于$ww^T=I$，可以发现最后损失函数只于特征值$\lambda$有关系，那么只需要找到最大的$\lambda$即可。

也就是说，只需要求协方差矩阵$\sum_{i=1}^{n}(x_i-\mu)(x_i-\mu)^T$的前k个最大特征值对应的特征向量，特征向量就是数据在低维新的位置。

6.特征值相等怎么办？

考虑到特征值有可能相等，那么就需要对排序后的特征值使用施密特正交变换求新的特征向量就行了。

7.累积贡献率与维度选择

- 方差贡献率

$$\frac{\lambda_k}{\sum_{i=1}^D \lambda_i}$$

其中，$\lambda_k$是第k个特征值，也被称为第k个主成分，分母是所有特征值之和。

- 累计贡献率：前d个主成分的累积贡献率为

$$\frac{\sum_{k=1}^d \lambda_k}{\sum_{i=1}^D \lambda_i}$$

8.维度选择准则：
- 阈值法：选择累积贡献率首次超过阈值(如80%)的最小d
- 碎石图法：选择特征值下降趋势变缓的”肘部”位置
- Kaiser准则：保留特征值大于1的主成分
- 交叉验证：通过重构误差最小化确定最优维度

9.实际应用建议：
- 可视化通常选择d = 2或3
- 特征工程建议保留累积贡献率≥ 85%的维度
- 高维数据可结合领域知识调整阈值

算法流程总结：

1. 数据标准化处理：$ \hat{x} = \frac{x - \frac{1}{x} \sum_{i=1}^{n} x_i}{\sqrt{\frac{1}{n-1}\sum_{i=1}^n (x_i - \bar{x})^2}} $；

2. 计算协方差矩阵：$\sum_{i=1}^{n}x_i x_i^T$；

3. 计算协方差矩阵的特征值、特征向量；
4. 将特征值进行排序选取$n_{dim}$个较大的特征值；
5. 选取相应的特征向量组成降维矩阵，也就是构建正交基；
6. 求投影，获取降维后的数据。

注：降维后的成分需要自己赋予其意义。

优点：

- 计算简单高效。
- 保留原始数据中的信息，使得降维后的数据能够最大程度地代表原始数据的分布情况，对于数据可视化、数据压缩等任务有很好的效果。

缺点：

- 主成分是原始特征的线性组合，随着维度的增加，这些主成分的实际物理意义可能会变得难以解释。
- 非线性数据处理能力弱。

~~~
# 手写实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris

def pca(data, n_dim):
    """
    PCA降维函数
    参数:
        data: 输入数据，形状为[N,D]，N是样本数，D是特征维度
        n_dim: 降维后的维度
    返回:
        data_ndim: 降维后的数据，形状为[N,n_dim]
        picked_eig_vector: 选择的特征向量组成的投影矩阵
    """
    N, D = np.shape(data)
    
    # 1. 中心化（减去均值）
    data_centered = data - np.mean(data, axis=0, keepdims=True)
    
    # 2. 标准化（除以标准差）
    data_scaled = data_centered / np.std(data_centered, axis=0, keepdims=True)
    
    # 计算协方差矩阵（使用标准化后的数据）
    C = np.dot(data_scaled.T, data_scaled) / (N - 1)  # [D,D]
    
    # 计算特征值和特征向量
    eig_values, eig_vector = np.linalg.eig(C)
    
    # 将特征值进行排序选取n_dim个较大的特征值
    indexs_ = np.argsort(-eig_values)[:n_dim]
    
    # 选取相应的特征向量组成降维矩阵
    picked_eig_vector = eig_vector[:, indexs_]  # [D,n_dim]
    
    # 对标准化后的数据进行降维
    data_ndim = np.dot(data_scaled, picked_eig_vector)
    
    # 计算贡献率
    explained_variance_ratio = eig_values[indexs_] / eig_values.sum()
    print(f"贡献率: {explained_variance_ratio}")
    
    return data_ndim, picked_eig_vector, explained_variance_ratio

# 加载鸢尾花数据集
iris = load_iris()
labs = iris.target  # 类别标签（0, 1, 2）
class_names = iris.target_names  # 类别名称（setosa, versicolor, virginica）

# 进行降维
data, picked_eig_vector, explained_variance_ratio = pca(iris.data, 2)

# 绘制降维后的图
plt.figure(figsize=(8, 6))

# 为每个类别单独绘制散点并添加标签
for i in range(3):  # 鸢尾花有3个类别
    mask = labs == i  # 创建掩码筛选出当前类别的数据
    plt.scatter(data[mask, 0], data[mask, 1], 
                label=class_names[i],  # 使用实际类别名称作为标签
                edgecolor='k', 
                s=70)

# 图例
plt.legend()

# 坐标轴标签，包含贡献率信息
plt.xlabel(f'Principal Component 1 ({np.round(explained_variance_ratio[0]*100, 1)}%)')
plt.ylabel(f'Principal Component 2 ({np.round(explained_variance_ratio[1]*100, 1)}%)')

# 添加标题
plt.title('PCA of IRIS Dataset')

# 显示网格
plt.grid(True, linestyle='--', alpha=0.6)

plt.show()
~~~

~~~
# sklearn实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.decomposition import PCA
from sklearn.preprocessing import StandardScaler

# 加载数据集
iris = load_iris()
X = iris.data  # 特征数据
y = iris.target  # 类别标签
class_names = iris.target_names  # 类别名称

# 数据标准化（PCA对特征尺度敏感，通常需要标准化）
scaler = StandardScaler()
X_scaled = scaler.fit_transform(X)

# 使用sklearn的PCA进行降维
pca = PCA(n_components=2)  # 降维到2维
X_pca = pca.fit_transform(X_scaled)

# 打印解释方差比（每个主成分解释的方差比例）
print("主成分解释方差比:", pca.explained_variance_ratio_)
print("累计解释方差比:", np.sum(pca.explained_variance_ratio_))

# 可视化降维结果
plt.figure(figsize=(8, 6))

# 为每个类别绘制散点
for i, class_name in enumerate(class_names):
    # 筛选出当前类别的数据
    mask = y == i
    plt.scatter(X_pca[mask, 0], X_pca[mask, 1],
                label=class_name,
                edgecolor='k',
                s=70)

# 添加图表元素
plt.xlabel(f'Principal Component 1 ({pca.explained_variance_ratio_[0]:.2%})')
plt.ylabel(f'Principal Component 2 ({pca.explained_variance_ratio_[1]:.2%})')
plt.title('PCA of IRIS Dataset')
plt.legend()
plt.grid(linestyle='--', alpha=0.6)

plt.show()
    
~~~

