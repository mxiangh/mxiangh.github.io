---

title: 机器学习（12）主成分分析——Principal Component Analysis, PCA
tags: ML Dimensionality_Reduction
---

算法：https://www.cnblogs.com/wj-1314/p/8032780.html

使用scikit-learn工具进行PCA降维：https://www.cnblogs.com/wj-1314/p/10144700.html

思想背景：主成分分析是一种无监督的线性降维方法，其目的是将n维数据通过重新构建正交基找到数据点在低维空间的投影，通过最大化降维后所有点之间的散度，求得降维后低维点的位置。

<!--more-->

原理解释：

1. 为什么构建正交基？

构建正交基等于重新创建一个坐标系，PCA需要找到一个新的低维坐标系以表示原来的数据。

构建坐标系，涉及到投影的知识，假设原数据A需要投影到新的坐标w，则有$ w=|A|cos(\theta)$

又因为

$$ cos(\theta) = \frac{A·w}{|A||w|}$$

因为是w是基向量，则有|w|=1，最终发现$ |A| · cos(\theta) = A·w$，即A与w的内积是A向w所在直线投影的矢量长度。

2. 中心化处理

在数据点周围构建的正交基向量不一定在坐标系原点周围，所以在一开始需要对数据中心化处理，将每个数据减去所有数据的均值，得到新数据$\hat{x}$

$$ \hat{x} = x - \frac{1}{x} \sum_{i=1}^{n} x_i $$

3. 定义损失函数

我们希望降维后数据点之间尽量保留原有信息，两点之间距离越大，差异越明显，信息越多，所以需要让所有数据点之间尽可能远，则有

$$ max \  \sum_{i=1}^{n}||w^Tx_i-w^Tx_j||^2 $$

$$ max \ \sum_{i=1}^{n}||w^T(x_i-x_j)||^2 $$

$$ max \ \sum_{i=1}^{n}w^T(x_i-x_j)(x_i-x_j)^Tw $$

$$ max \ w^T\{\sum_{i=1}^{n}(x_i-x_j)(x_i-x_j)^T\}w $$

其中，$\sum_{i=1}^{n}(x_i-x_j)(x_i-x_j)^T$正比于协方差矩阵$\sum_{i=1}^{n}(x_i-\mu)(x_i-\mu)^T$，又因为中心化后$\mu=0$，所以有

$$ max \ w^T \sum_{i=1}^{n}x_i x_i^T w $$

4. 约束条件

基向量正交是单位矩阵，即$ww^T=I$

5. 拉格朗日乘子法

将约束函数转化为无约束函数

$$ max \ w^T \sum_{i=1}^{n}x_i x_i^T w + \lambda (I- ww^T)$$

求最值，令偏导函数为0，得

$$  2 \sum_{i=1}^{n}x_i x_i^T w + \lambda (- 2w) = 0$$

$$  \sum_{i=1}^{n}x_i x_i^T w = \lambda w $$

代入原损失函数得到

$$ max \ \lambda \ w^T w $$

由于$ww^T=I$，可以发现最后损失函数只于特征值$\lambda$有关系，那么只需要找到最大的$\lambda$即可。

也就是说，只需要求协方差矩阵$\sum_{i=1}^{n}(x_i-\mu)(x_i-\mu)^T$的前k个最大特征值对应的特征向量，特征向量就是数据在低维新的位置。

6. 特征值相等怎么办？

考虑到特征值有可能相等，那么就需要对排序后的特征值使用施密特正交变换求新的特征向量就行了。
