---

title: 机器学习（17）拉普拉斯特征映射——Laplacian Eigenmaps，LE
tags: ML Dimensionality_Reduction
---

#### 拉普拉斯特征映射——Laplacian Eigenmaps，LE

原论文：Laplacian Eigenmaps for Dimensionality Reduction and Data Representation

思想：一种无监督的基于图的非线性降维方法，其基本思想是从高维空间向低维空间映射过程中保持数据间原有的局部结构特性（相似性）。

<!--more-->

目标：原空间中相近的点在映射到新的低维空间中时仍然比较相近（不是保持距离，而是希望更近，但是又不希望所有样本汇聚成一个点，而是保持一定距离）。

这个目标可以解释代码绘制出来的为什么是一个曲线，希望所有点很接近，那最近的就是重叠，所有样本压缩成一个点，但是这样没有意义，所以给两点固定一个距离，这样就形成了一条曲线。

拉普拉斯特征映射基本步骤：

1. 构建无向图，将所有的样本以点连接成一个图;
2. 构建图的权值矩阵，通过点之间的关联程度来确定点与点之间的权重大小;
3. 特征映射，通过公式$L_y=\lambda D_y$,计算拉普拉斯矩阵L的特征向量和特征值，用最小的m个非零特征值对应的特征向量作为降维的结果。

步骤1：构建邻接图，如果节点$x_i$和$x_j$是邻近点，则形成一条连接边。

(1) $\varepsilon$邻域法。当$\left\|x_i-x_j\right\|^2<=\varepsilon$时，建立连接

- 优点：几何意义明确，关系具有传递性
- 缺点：易产生不连通子图，$\varepsilon$值难以选择

(2)k近邻法。若$x_i$和$x_j$是k个最近邻则连接

- 优点：参数易设定，通常保证图连通性
- 缺点：几何直观性较弱

Q：为什么需要构建邻接图？

答：捕捉高维数据中潜在的局部几何结构，定义了 “哪些点属于局部范围”。

步骤2：选择权重，在节点$x_i$和$x_j$的连接边上赋予权重$W_{ij}$，得到图的邻接矩阵W。

(1)热核函数，对连接节点的连接边赋予权重

$$
W_{i j}=\left\{\begin{array}{l}
e^{-\frac{\left\|x_{i}-x_{j}\right\|^{2}}{t}}(t \in R), \text { connected } \\
0, \text { else }
\end{array}\right.
$$

(2)简易方法

$$
W_{i j}=\left\{\begin{array}{l}
1, \text { connected } \\
0, \text { else }
\end{array}\right.
$$

Q：为什么需要设置权重？

答：定义点与点之间的关联紧密程度。

步骤3：特征映射

(1)设数据在低维的映射$ y=\lbrace y_1,y_2,...,y_n \rbrace^T $，定义优化目标函数

$$ min \ \sum_{i,j}||y_i-y_j||^2 \ W_{ij} $$

其中，$W_{ij}$是邻接矩阵W的元素，对于热核函数，距离远则权重小，距离近则权重大，惩罚实际近，映射远的两个点。

(2)目标函数优化（需要较好的线性代数基础）

邻接矩阵W：由节点构成的权重矩阵，是一个对称矩阵，即$w_{ij} = w_{ji}$

度矩阵D：邻接矩阵W每行相加放在对角线所得矩阵，也就是一个对角矩阵，即$D_{ii} = \sum_{j=1}^n W_{ij}$

拉普拉斯矩阵：L = D - W

$$
\begin{array}{l}
\sum_{i=1}^{n} \sum_{j=1}^{n}\left\|y_{i}-y_{j}\right\|^{2} W_{i j} \\
=\sum_{i=1}^{n} \sum_{j=1}^{n}\left(y_{i}^{T} y_{i}-2 y_{i}^{T} y_{j}+y_{j}^{T} y_{j}\right) W_{i j} \\
=\sum_{i=1}^{n}\left(\sum_{j=1}^{n} W_{i j}\right) y_{i}^{T} y_{i}+\sum_{j=1}^{n}\left(\sum_{i=1}^{n} W_{i j}\right) y_{j}^{T} y_{j}-2 \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i}^{T} y_{j} W_{i j} \\
=2 \sum_{i=1}^{n} D_{i i} y_{i}^{T} y_{i}-2 \sum_{i=1}^{n} \sum_{j=1}^{n} y_{i}^{T} y_{j} W_{i j} \\
=2 \operatorname{tr}\left(Y^{T} D Y\right)-2 \operatorname{tr}\left(Y^{T} W Y\right) \\
=2 \operatorname{tr}\left[Y^{T}(D-W) Y\right] \\
=2 \operatorname{tr}\left(Y^{T} L Y\right)
\end{array}
$$

简化后的优化目标函数为

$$ min \ \operatorname{tr}\left(Y^{T} L Y\right) $$

这时，发现目标函数$ min \ \operatorname{tr}\left(Y^{T} L Y\right) $可以通过简单地缩放Y，来无限减小其值，导致优化问题无界，无法得到有意义的解。

例如，当$ Y \to \alpha Y $时

$ tr(Y^{T} L Y) =  tr((\alpha Y)^{T} L (\alpha Y)) = \alpha^2 tr(Y^{T} L Y)$

所以增加约束条件$Y^T D Y = I$，该约束强制Y的列向量在加权内积（由D定义）下是标准正交的，从而避免了因缩放而导致的解不唯一或无界的问题。

这一约束相当于将嵌入Y限制在一个“单位球”上（由D定义的度量空间），从而确保优化问题的解在合理的范围内，且具有明确的几何或物理意义。

(3)拉格朗日乘子法

$$ min \ \operatorname{tr}\left(Y^{T} L Y\right)  + tr(\lambda (Y^T D Y - I))$$

对Y求偏导取0，得

$$ LY = \lambda DY $$ 

最后选择最小的m个非零特征值对应的特征向量作为降维后的结果输出。

优点：
- 通过图的拉普拉斯矩阵L捕获数据点之间的局部邻域关系，能够有效保留高维数据中的局部几何结构。

- 基于图拉普拉斯算子的谱理论，具有良好的数学基础，与热扩散、流形学习等理论相关。

缺点：

- 性能受邻域大小 K 或 ϵ 的影响较大，选择不当可能导致降维效果变差。

- 当数据点数量N很大时，计算非常昂贵，难以扩展到超大规模数据集。

- 只能对训练数据进行降维，无法处理新样本。

- 全局结构可能无法完全保留，特别是当数据具有复杂拓扑时。



```

# 手写实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets

def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):
    # 生成瑞士卷数据集
    t = 1.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))
    x = t * np.cos(t)
    y = 83 * np.random.rand(1, n_samples)
    z = t * np.sin(t)
    X = np.concatenate((x, y, z))
    X += noise * np.random.randn(3, n_samples)
    X = X.T
    t = np.squeeze(t)
    return X, t

def rbf(dist, t = 1.0):
    # 热核函数，t类似高斯分布的方差
    return np.exp(-(dist/t))

def cal_pairwise_dist(x):

    '''
    计算欧式距离, x是样本矩阵
    (a-b)^2 = a^2 + b^2 - 2*a*b
    '''
    sum_x = np.sum(np.square(x), 1)
    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)
    #返回任意两个点之间距离的平方
    return dist

def cal_rbf_dist(data, n_neighbors = 10, t = 1):
    dist = cal_pairwise_dist(data)
    dist[dist < 0] = 0
    n = dist.shape[0]
    rbf_dist = rbf(dist, t)

    W = np.zeros((n, n))
    for i in range(n):
        index_ = np.argsort(dist[i])[1:1+n_neighbors]
        W[i, index_] = rbf_dist[i, index_]
        W[index_, i] = rbf_dist[index_, i]

    return W

def le(data, n_dims = 2, n_neighbors = 5, t = 1.0):
    '''
    参数：
        data: (n_samples, n_features)
        n_dims: 目标维度
        n_neighbors: k nearest neighbors
        t: 热核函数的超参数
    返回:
        降维后数据
    '''
    N = data.shape[0]
    W = cal_rbf_dist(data, n_neighbors, t)
    D = np.zeros_like(W)
    for i in range(N):
        D[i,i] = np.sum(W[i])

    D_inv = np.linalg.inv(D)
    L = D - W
    eig_val, eig_vec = np.linalg.eig(np.dot(D_inv, L))

    sort_index_ = np.argsort(eig_val)

    eig_val = eig_val[sort_index_]

    j = 0
    while eig_val[j] < 1e-6:
        j+=1

    sort_index_ = sort_index_[j:j+n_dims]
    eig_val_picked = eig_val[j:j+n_dims]

    eig_vec_picked = eig_vec[:, sort_index_]

    X_ndim = eig_vec_picked
    return X_ndim



# 手写数据集
# X, color = make_swiss_roll(n_samples = 1500, noise=0.01, random_state=42)
# datasets数据集
X, color = datasets.make_swiss_roll(n_samples=1500, noise=0.01, random_state=42)

data = le(X, n_neighbors = 5, t = 20)

fig = plt.figure(figsize=(12,6))
ax1 = fig.add_subplot(121, projection='3d')
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c = color, cmap=plt.cm.Spectral)
ax1.set_title("make_swiss_roll")

ax2 = fig.add_subplot(122)
ax2.scatter(data[:, 0], data[:, 1], c = color, cmap=plt.cm.Spectral)
ax2.set_title("LE")
plt.show()
```

```

# sklearn学习库
from sklearn import manifold, datasets
import numpy as np
import matplotlib.pyplot as plt

# 生成瑞士卷数据
X, color = datasets.make_swiss_roll(n_samples=1500, noise=0.01, random_state=42)

# 创建一个画布
fig = plt.figure(figsize=(14, 7))  # 设置画布大小

# 三维数据
ax1 = fig.add_subplot(1, 2, 1, projection='3d')  # 1行2列的第1个
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax1.set_title("原始瑞士卷数据")
ax1.view_init(4, -75)  # 调整视角

# 使用 SpectralEmbedding 进行降维
se = manifold.SpectralEmbedding(n_neighbors=10, n_components=2, affinity='nearest_neighbors') # 10个邻居，降维到2维
Y = se.fit_transform(X)

# 降维后的二维数据
ax2 = fig.add_subplot(1, 2, 2)  # 1行2列的第2个
ax2.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
ax2.set_title("LE降维后")

plt.show()
```
