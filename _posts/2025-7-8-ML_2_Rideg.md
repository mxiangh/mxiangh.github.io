---
title: 机器学习（2）岭回归——Ridge Regression
tags: ML Regression
---

思想：为了解决多元线性回归$ \mathbf{X}^T \mathbf{X} $不满秩的情况，引入正则化项，使用$ L_2 $ 正则化项。一方面，解决多重共线问题，另一方面，解决过拟合问题。

<!--more-->

##### 1.模型对比

多元线性回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w})$$

岭回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w}) + \lambda \left\| \hat{w} \right\|^2 $$ 

其中，$ \lambda $是一个自定义的超参数，也称为岭系数、惩罚系数等，$ \left\| \hat{w} \right\|^2 $ 是$ L_2 $范数。

##### 2.范数

强化过后的距离概念，可以把范数理解为距离。设向量$ x=(x_1,x_2,\dots,x_n)^T $，则$ L_2 $范数为

$$ ||x||_2^2 = (|x_1|^2+|x_2|^2+\dots+|x_n|^2)^{\frac{1}{2}} $$

##### 3.Ridge求解

同样对$ \hat{w} $求导

$$ \frac{\partial E_{\hat{w}} }{\partial \hat{w}} = 2 \mathbf{X}^T \mathbf{X} \hat{w} - 2 \mathbf{X}^T \mathbf{y} + 2\lambda \hat{w} = 0 $$

$$ (\mathbf{X}^T \mathbf{X} + 2\lambda I )\hat{w} = \mathbf{X}^T \mathbf{y} $$

- 补充：

- $ \frac{\partial (\theta^T A \theta) }{\partial \theta} = (A+A^T)\theta $

- $ \frac{\partial (\theta^T A) }{\partial \theta} = A $

- $ \frac{\partial (\theta^T \theta) }{\partial \theta} = 2\theta $

其中，I是单位矩阵，$2\lambda I$也是单位矩阵，$ \mathbf{X}^T \mathbf{X} $ 加上单位矩阵必定可逆。

- 证明：

因为对于任意非零向量$ \mathbf{v} $，有 $ \mathbf{v}^T \mathbf{X}^T \mathbf{X}  \mathbf{v} = \left\| \mathbf{X}  \mathbf{v} \right\|^2  \ge 0 $

所以矩阵$ \mathbf{X}^T \mathbf{X} $是一个半正定矩阵

因为对于任意非零向量$ \mathbf{v} $，有 $ \mathbf{v}^T \lambda I  \mathbf{v} = \lambda \left\| \mathbf{v} \right\|^2  > 0 $

所以矩阵$ \lambda I $是一个正定矩阵

由于半正定+正定=正定，所以$ \mathbf{X}^T \mathbf{X} + \lambda I $是正定矩阵

又因为正定矩阵必可逆，所以$ \mathbf{X}^T \mathbf{X} $ 加上单位矩阵必定可逆。

解得

$$ \hat{w} = (\mathbf{X}^T \mathbf{X} + 2\lambda I )^{-1} \mathbf{X}^T \mathbf{y} $$

~~~
# 手写实现
import numpy as np

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b

def Ridge(n_features, n_samples=100, noise_scale=0.1, alpha=1.0):
    X, y, true_w, true_b = generatr_function(n_features, n_samples, noise_scale)
    
    X_with_bias = np.c_[X, np.ones(X.shape[0])]
    
    I = np.eye(X_with_bias.shape[1])  # 单位矩阵
    I[-1, -1] = 0  # 不对偏置项进行正则化
    theta = np.linalg.inv(X_with_bias.T @ X_with_bias + alpha * I) @ X_with_bias.T @ y
    
    return X, y, true_w, true_b, theta[:-1], theta[-1]

X, y, true_w, true_b, estimated_w, estimated_b = Ridge(n_features=1, alpha=1.0)

# 评估
print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", estimated_w)
print("估计偏置 (b̂):", estimated_b)
~~~

~~~
# sklearn实现
from sklearn.linear_model import Ridge
import numpy as np

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b
    
X, y, true_w, true_b = generatr_function(3)

# 创建模型并拟合
model = Ridge(alpha=0.5)
model.fit(X, y)

# 预测
y_pred = model.predict(X)

# 评估
print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", model.coef_)
print("估计偏置 (b̂):", model.intercept_)
~~~

