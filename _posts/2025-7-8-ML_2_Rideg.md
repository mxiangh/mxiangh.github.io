---
title: 机器学习（2）岭回归——Ridge Regression
tags: ML Regression
---

#### 岭回归——Ridge Regression

思想：为了解决多元线性回归$ \mathbf{X}^T \mathbf{X} $不满秩的情况，引入正则化项，使用$ L_2 $ 正则化项。一方面，解决多重共线问题，另一方面，解决过拟合问题。

<!--more-->

多元线性回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w})$$

岭回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w}) + \lambda \left\| \hat{w} \right\|^2 $$ 

其中，$ \lambda $是一个自定义的超参数，也称为岭系数、惩罚系数等，$ \left\| \hat{w} \right\|^2 $ 是$ L_2 $范数。

范数：强化过后的距离概念，可以把范数理解为距离。设向量$ x=(x_1,x_2,\dots,x_n)^T $，则$ L_2 $范数为

$$ ||x||_2^2 = (|x_1|^2+|x_2|^2+\dots+|x_n|^2)^{\frac{1}{2}} $$

同样对$ \hat{w} $求导

$$ \frac{\partial E_{\hat{w}} }{\partial \hat{w}} = 2 \mathbf{X}^T \mathbf{X} \hat{w} - 2 \mathbf{X}^T \mathbf{y} + 2\lambda \hat{w} = 0 $$

$$ (\mathbf{X}^T \mathbf{X} + 2\lambda I )\hat{w} = \mathbf{X}^T \mathbf{y} $$

补充：

- $ \frac{\partial (\theta^T A \theta) }{\partial \theta} = (A+A^T)\theta $

- $ \frac{\partial (\theta^T A) }{\partial \theta} = A $

- $ \frac{\partial (\theta^T \theta) }{\partial \theta} = 2\theta $

其中，I是单位矩阵，$2\lambda I$也是单位矩阵，$ \mathbf{X}^T \mathbf{X} $ 加上单位矩阵必定可逆。

证明：

因为对于任意非零向量$ \mathbf{v} $，有 $ \mathbf{v}^T \mathbf{X}^T \mathbf{X}  \mathbf{v} = \left\| \mathbf{X}  \mathbf{v} \right\|^2  \ge 0 $

所以矩阵$ \mathbf{X}^T \mathbf{X} $是一个半正定矩阵

因为对于任意非零向量$ \mathbf{v} $，有 $ \mathbf{v}^T \lambda I  \mathbf{v} = \lambda \left\| \mathbf{v} \right\|^2  > 0 $

所以矩阵$ \lambda I $是一个正定矩阵

由于半正定+正定=正定，所以$ \mathbf{X}^T \mathbf{X} + \lambda I $是正定矩阵

又因为正定矩阵必可逆，所以$ \mathbf{X}^T \mathbf{X} $ 加上单位矩阵必定可逆。

解得

$$ \hat{w} = (\mathbf{X}^T \mathbf{X} + 2\lambda I )^{-1} \mathbf{X}^T \mathbf{y} $$