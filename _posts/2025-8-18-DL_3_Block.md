---

title: 深度学习（3）如何搭建一个神经网络模型
tag: DL DNN
typora-root-url: ./..
---

用class搭建一个神经网络。

<!--more-->

##### 1.简介

在上一节构造神经网络代码的时候，从设置激活函数、优化器、前向传播、损失函数，可以自定义函数，但是每次执行任务都需要重新定义函数，重复的工作显然太多了，所以这篇来学习如何更高效地搭建神经网络。

##### 2.块级神经网络

之前已经了解了神经网络的构造，由神经元节点和权重组成的模型，还引入了输出层、隐藏层、输出层的概念。

单层的神经网络只有输入和输出层，多层神经网络通过增加隐藏层的层数来构造更复杂的模型。隐藏层和输出层的功能其实大差不多，都是接受上一层输入，生成下一层输出，并且有一组需要优化的参数。

对于整个神经网咯模型，接受原始输⼊（特征），⽣成输出（预测），并且有一组需要优化的参数。

也就是说整个模型和单个层的作用都一样！那能不能研究一种比整个模型小但是又比单个层大的组件呢？这里引入神经网络块的概念。

块（block）可以描述单个层、由多个层组成的组件或整个模型本⾝。使⽤块进⾏抽象的⼀个好处是可以将⼀些块组合成更⼤的组件，这⼀过程通常是递归的，通过定义代码来按需⽣成任意复杂度的块，我们可以通过简洁的代码实现复杂的神经⽹络。

![](/images/block/1.png)

在构建神经网络时，通常实例化nn.Sequential来构建我们的模型，层的执行顺序是作为参数传递的。

~~~
import torch
from torch import nn

net = nn.Sequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
X = torch.rand(2, 20)
print(net(X))
~~~

在$nn.Sequential$中，首先定义了一个线性函数，20是输入样本的个数，256是隐藏层节点数，$nn.Linear$根据这两个数据自动进行矩阵运算并输出向量。$nn.ReLU$是激活函数，接受上一层的$Linear$的输出。最后再构建一个线性函数。接受256维输入，输出10维向量。

换言之，这是⼀个两层神经网络，全连接隐藏层具有256个单元和ReLU激活函数，全连接输出层具有10个隐藏单元且不带激活函数。

事实上，$net$通过$nn.Sequential$定义了一个前向传播的函数，当调用了$net(X)$时，$net$执行一次前向传播，相比于之前的代码，简洁了非常非常多。

##### 3.class定义块

这节学习怎么用class定义一个块网络。

~~~
# 定义一个MLP类，继承自nn.Module（神经网络的妈，管理参数、实现前向传播、设备迁移）
class MLP(nn.Module):
    # 初始化
    def __init__(self):
        # 调⽤MLP的⽗类Module的构造函数来执⾏必要的初始化。
        super().__init__()
        self.hidden = nn.Linear(20, 256) # 隐藏层
        self.relu = nn.ReLU() # 激活函数
        self.out = nn.Linear(256, 10) # 输出层
        
    # 定义模型的前向传播，即如何根据输⼊X返回所需的模型输出    
    def forward(self, X):
        return self.out(self.relu(self.hidden(X)))
~~~

这个class定义了一个隐藏层和一个输出层，前向传播中，函数经过隐藏层$\to$激活函数$\to$输出层，完成一次运算。可以发现，我们并没有初始化参数$w$和$b$的步骤，这是因为在定义$nn.Linear$时，参数$w$和$b$自动被初始化。

之后可以调用这个函数。

~~~
X = torch.rand(2, 20)
net = MLP()
net(X)
~~~

和第二节$nn.Sequential$实现的效果是一样的。

##### 4.用class构造一个Sequential

第二节中的$Sequential$可以实例化构建模型，这节学会如何用$class$搭建一个自己的$Sequential$。

~~~
class MySequential(nn.Module):
    # 初始化，*args用于接受任意数量的参数
    def __init__(self, *args):
        super().__init__()
        # enumerate给列表每个元素分配唯一id索引
        for index, module in enumerate(args):
        	# 添加层网络
            self._modules[str(index)] = module

    def forward(self, X):
    	# 按顺序运行每层网络
        for block in self._modules.values():
            X = block(X)
            
        return X

X = torch.rand(2, 20)
net = MySequential(nn.Linear(20, 256), nn.ReLU(), nn.Linear(256, 10))
print(net(X))
~~~

##### 5.为什么要定义自己的Sequential类

使用torch自带的Sequential类相对而言更简单，但如果想要更灵活的函数，就需要自己的类。例如，我们不再单纯考虑线性函数$w^T \cdot x$，而是要求$a \cdot w^T \cdot x$，其中$a$是一个常数参数。

~~~
class FixedHiddenMLP(nn.Module):
    def __init__(self):
        super().__init__()
        # requires_grad=False，梯度不更新，作为常数a
        self.rand_weight = torch.rand((20, 20), requires_grad=False)
        self.linear = nn.Linear(20, 20)
        self.relu = nn.ReLU()
        
    def forward(self, X):
        X = self.linear(X)
        # 使⽤创建的常量参数以及relu和mm函数
        X = self.relu(torch.mm(X, self.rand_weight) + 1)
        # 复⽤全连接层。相当于两个全连接层共享参数
        X = self.linear(X)
        # 额外操作，如果L1范数大于1，则÷2
        while X.abs().sum() > 1:
            X /= 2
        return X.sum()

X = torch.rand(2, 20)
net = FixedHiddenMLP()
print(net(X))
~~~

除了函数改变，还增加了一个自定义的L1范数判断。当然，怎么做的目的是为了展示自定义Sequential类的作用——添加任意自己想添加的操作，方便以后发明自己的网络。

我们也可以在class里面调用Sequential类构建网络。

~~~
class NestMLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.net = nn.Sequential(nn.Linear(20, 64), nn.ReLU(),
        						 nn.Linear(64, 32), nn.ReLU())
        self.linear = nn.Linear(32, 16)
        
    def forward(self, X):
        return self.linear(self.net(X))
~~~

只要你想，你可以用class搭建任意网络。

##### 6.参数访问

这节学习如何访问神经网络里面的参数，可以把模型理解为一个列表，通过列表访问模型的任意层。

~~~
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)

# 访问第二层的所有参数
print(net[2].state_dict())

# 访问第二层的偏置
print(net[2].bias)

# 访问第二层的偏置的值
print(net[2].bias.data)

# 访问第二层的偏置的梯度
print(net[2].bias.grad)

# 访问第二层的权重
print(net[2].weight)

# 访问第二层的权重的值
print(net[2].weight.data)

# 访问第二层的权重的梯度
print(net[2].weight.grad)
~~~

##### 7.参数初始化

~~~
import torch
from torch import nn

net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(), nn.Linear(8, 1))
X = torch.rand(size=(2, 4))
net(X)

# 定义初始化方法
def init_normal(m):
	# 只对线性函数应用
    if type(m) == nn.Linear:
    	# 将所有权重参数初始化为标准差为0.01的⾼斯随机变量
        nn.init.normal_(m.weight, mean=0, std=0.01)
        # 偏置参数设置为0
        nn.init.zeros_(m.bias)
        
# 应用初始化方法
net.apply(init_normal)

print(net[0].state_dict())
~~~

如果想将所有参数初始化为给定的常数，比如1，可以用下面这行代码：

~~~
nn.init.constant_(m.weight, 1)
~~~

如果想用⽤Xavier初始化参数，可以用下面这行代码：

~~~
nn.init.xavier_uniform_(m.weight)
~~~

##### 8.共享参数（CNN会用到）

第二个隐藏层和第三个隐藏层共享参数，反向传播时，二者梯度会相加。

~~~
shared = nn.Linear(8, 8)
net = nn.Sequential(nn.Linear(4, 8), nn.ReLU(),
                    shared, nn.ReLU(),
                    shared, nn.ReLU(),
                    nn.Linear(8, 1))
net(X)
~~~

##### 9.自定义层

使用class自定义一个layer。

~~~
class MyLinear(nn.Module):
    def __init__(self, in_units, units):
        super().__init__()
        # nn.Parameter将参数视为可训练的参数，会自动进行梯度计算和参数更新
        self.weight = nn.Parameter(torch.randn(in_units, units))
        self.bias = nn.Parameter(torch.randn(units,))
        self.relu = nn.ReLU()
        
    def forward(self, X):
    	# 定义一个线性函数wx+b
        linear = torch.matmul(X, self.weight.data) + self.bias.data
        return self.relu(linear)
        
linear = MyLinear(5, 3)
print(linear.state_dict())
~~~

使用自定义的layer构建网络。

~~~
net = nn.Sequential(MyLinear(64, 8), MyLinear(8, 1))
print(net(torch.rand(2, 64)))
~~~

##### 10.读写文件

这节学习如何保存和加载数据。

###### 10.1 加载和保存张量

对于单个张量，可以直接调⽤load和save函数分别读写。

~~~
import torch
from torch import nn

x = torch.arange(4)

# 保存
torch.save(x, 'x-file')

# 读取
x2 = torch.load('x-file')
~~~

存储和读取张量列表。

~~~
y = torch.zeros(4)

# 保存
torch.save([x, y],'x-files')
# 读取
x2, y2 = torch.load('x-files')
~~~

存储和读取张量的字典。

~~~
mydict = {'x': x, 'y': y}
torch.save(mydict, 'mydict')
mydict2 = torch.load('mydict')
~~~

###### 10.2 加载和保存模型参数

将参数存储在⼀个叫做“mlp.params”的⽂件中。

~~~
class MLP(nn.Module):
    def __init__(self):
        super().__init__()
        self.hidden = nn.Linear(20, 256)
        self.output = nn.Linear(256, 10)
        self.relu = nn.ReLU()
        
    def forward(self, x):
        return self.output(self.relu(self.hidden(x)))
        
net = MLP()
X = torch.randn(size=(2, 20))
Y = net(X)

torch.save(net.state_dict(), 'mlp.params')
~~~

将预保存的模型参数（权重、偏置等）加载到模型中。

~~~
clone = MLP()
# 将预保存的模型参数（权重、偏置等）加载到模型中
clone.load_state_dict(torch.load('mlp.params'))
clone.eval()
~~~

##### 11.使用GPU

判断有无GPU是否可以使用，torch.cuda.is_available()为True则可以。

~~~
# 判断是否可以使用GPU
print("CUDA可用" if torch.cuda.is_available() else "CUDA不可用")
~~~

张量默认在cpu上运行，运行时要确保所有张量在同一个设备上。

~~~
# 直接在创建时指定device
x = torch.tensor([1, 2, 3], device='cpu')  
# 或创建后移动到CPU
x = torch.tensor([1, 2, 3]).to('cpu')
~~~

在GPU创建张量。

~~~
# 直接在创建时指定device
x = torch.tensor([1, 2, 3], device='cuda:0')  
# 或创建后移动到GPU
x = torch.tensor([1, 2, 3]).to('cuda:0')
~~~

