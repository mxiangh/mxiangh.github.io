---
title: 机器学习（1）线性回归——Linear Regression
tags: ML Regression
---

#### 线性回归——Linear Regression

思想背景：线性回归想要寻找一组参数以此反映出样本$\mathbf{x}$与$\mathbf{y}$的关系，即找到一组w和b，使得$ \hat{y} = w_1 x_1 + w_2 x_2 + ... + w_m x_m + b $ 。

<!--more-->

##### 线性回归

对于一元线性回归 $\hat{y} =wx_i+b$，通常使用均方误差构建损失函数，目的是让预测值$ \hat{y} $和实际值y之间的误差尽可能小

$$ E(w, b)= min \sum_{i=1}^{m}(\hat{y_i}-y_i)^2 $$

$$ E(w, b)= min \sum_{i=1}^{m}(y_i-wx_i-b)^2 $$

注：在统计学中，$ \sum_{i=1}^{m}(\hat{y_i}-y_i)^2 $ 严格意义上是离差平方和或残差平方和，并且预测函数需要添加噪声点$ \varepsilon $。由于这里求最小值，均值误差前面的系数对总体没有影响，故省略；噪声点是一个随机值，通常在编写代码时添加，求解过程也可以略去。

求解w和b使得$ E(w, b)= min \sum_{i=1}^{m}(y_i-wx_i-b)^2 $最小化的过程，称为线性回归模型的最小二乘“参数估计”，函数求最值，令其导数为0即可。

对w和b分别求导并为0

$$ \begin{aligned}
\frac{\partial E(w, b) }{\partial b} & = 2 \sum_{i=1}^{m}(y_i-wx_i-b)·(-1) \\
& = -2 \sum_{i=1}^{m}y_i + 2 \sum_{i=1}^{m}w x_i + 2 \sum_{i=1}^{m}b \\
& = 2mb - 2 \sum_{i=1}^{m}(y_i-w x_i) = 0 \\
\end{aligned}$$

得到$ b = \frac{\sum_{i=1}^{m}(y_i-w x_i)}{m} $，注意到均值$ \bar{x} = \sum_{i=1}^{m}x_i $

$$ \begin{aligned}
\frac{\partial E(w, b) }{\partial w} & = 2 \sum_{i=1}^{m}(y_i-wx_i-b)·(-x_i) \\
& = -2 \sum_{i=1}^{m}y_i x_i + 2 w \sum_{i=1}^{m}x_i^2 + 2b \sum_{i=1}^{m}x_i \\
& = -2 \sum_{i=1}^{m}y_i x_i + 2 w \sum_{i=1}^{m}x_i^2 + 2 \sum_{i=1}^{m}(y_i-w x_i) \bar{x} \\
& = -2 \sum_{i=1}^{m}y_i x_i + 2 w \sum_{i=1}^{m}x_i^2 + 2 \sum_{i=1}^{m}y_i \bar{x} - 2 w \sum_{i=1}^{m}x_i \bar{x} = 0 \\
\end{aligned}$$

最终得到w和b的最优解的闭式解

$$ w = \frac{\sum_{i=1}^{m}y_i(x_i-\bar{x})}{\sum_{i=1}^{m}x_i^2 - \frac{1}{m} (\sum_{i=1}^{m}x_i)^2} $$

$$ b = \frac{1}{m}  \sum_{i=1}^{m}(y_i-w x_i) $$

##### 多元线性回归

此时我们想要学习的函数是 $ \hat{y} =w^T x_i+b$，类似的使用最小二乘“参数估计”进行求解。为了方便讨论，通常将w和b合为一个向量形式$ \hat{w} = (w;b)$，同时把数据表示为一个m*（d+1）大小的矩阵$ \mathbf{X} $。m是样本x的个数，前d个元素是d个属性，最后一个元素是1，即

$$
\mathbf{X} = \begin{Bmatrix}
 x_{11} & x_{12} & \cdots & x_{1d} & 1 \\
 x_{21} & x_{22} & \cdots & x_{2d} & 1 \\
 \vdots & \vdots & \ddots  & \vdots & \vdots \\
 x_{m1} & x_{m2} & \cdots  & x_{md} & 1
\end{Bmatrix}=\begin{Bmatrix}
 x_{1}^T & 1  \\
 x_{2}^T & 1  \\
 \vdots & \vdots  \\
 x_{m}^T & 1 
\end{Bmatrix}
$$

将标记写成向量形式$ \mathbf{y} = (y_i;y_2;\dots;y_m) $，学习函数变为$ \mathbf{\hat{y}} =w^T \mathbf{x_i} $，得到损失函数

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w})$$

对$ \hat{w} $ 求导，并令其为0

$$ \begin{aligned}
E_{\hat{w}} & =  ((\mathbf{X} \hat{w})^T-\mathbf{y}^T) (\mathbf{y}-\mathbf{X} \hat{w}) \\
& = \hat{w}^T \mathbf{X}^T \mathbf{y} - \hat{w}^T \mathbf{X}^T \mathbf{X} \hat{w} - \mathbf{y}^T \mathbf{y} + \mathbf{y}^T \mathbf{X} \hat{w} \\
& = 2 \hat{w}^T \mathbf{X}^T \mathbf{y} - \hat{w}^T \mathbf{X}^T \mathbf{X} \hat{w} - \mathbf{y}^T \mathbf{y} \\
\frac{\partial E_{\hat{w}} }{\partial \hat{w}} & = 2 \mathbf{X}^T \mathbf{y} - 2 \mathbf{X}^T \mathbf{X} \hat{w} = 0 \\
\end{aligned}
$$

补充：

- $ \frac{\partial (\theta^T A \theta) }{\partial \theta} = (A+A^T)\theta $

- $ \frac{\partial (\theta^T A) }{\partial \theta} = A $

- $ \frac{\partial (\theta^T \theta) }{\partial \theta} = 2\theta $

这里假设$ \mathbf{X}^T \mathbf{X} $是满秩矩阵或正定矩阵，否则解不唯一，得到$ \hat{w} $

$$ \hat{w}^* = (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$

令$ \mathbf{\hat{x}_i} = (x_i,1)$，最终得到多元线性回归模型

$$ \mathbf{\hat{y}} = \mathbf{\hat{x}_i}^T (\mathbf{X}^T \mathbf{X})^{-1} \mathbf{X}^T \mathbf{y} $$

~~~
# 手写实现
import numpy as np
import matplotlib.pyplot as plt

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b

def MLR(n_features, n_samples=100, noise_scale=0.1):
    X, y, true_w, true_b = generatr_function(n_features, n_samples, noise_scale)
    
    # 添加偏置项（在 X 最右侧加一列1）
    X_with_bias = np.c_[X, np.ones(n_samples)]
    
    # 计算解析解：(X^T X)^{-1} X^T y
    theta = np.linalg.inv(X_with_bias.T @ X_with_bias) @ X_with_bias.T @ y
    
    return X, y, true_w, true_b, theta[:-1], theta[-1]

def mse(y_true, y_pred):
    return np.mean((y_true - y_pred) ** 2)

def r2_score(y_true, y_pred):
    y_mean = np.mean(y_true)
    tss = np.sum((y_true - y_mean) ** 2)  # 总平方和
    rss = np.sum((y_true - y_pred) ** 2)  # 残差平方和
    return 1 - (rss / tss)
    
# 参数拟合
X, y, true_w, true_b, estimated_w, estimated_b = MLR(3)

print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", estimated_w)
print("估计偏置 (b̂):", estimated_b)
print("MSE:", mse(y, X @ estimated_w + estimated_b))
print("R²:", r2_score(y, X @ estimated_w + estimated_b))
~~~

~~~
# 函数库实现
from sklearn.linear_model import LinearRegression
from sklearn.metrics import mean_squared_error, r2_score
import numpy as np

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b
    
# 生成数据
X, y, true_w, true_b = generatr_function(3)

# 创建模型并拟合
model = LinearRegression()
model.fit(X, y)

# 预测
y_pred = model.predict(X)

# 评估
print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", model.coef_)
print("估计偏置 (b̂):", model.intercept_)
print("MSE:", mean_squared_error(y, y_pred))
print("R²:", r2_score(y, y_pred))
~~~

