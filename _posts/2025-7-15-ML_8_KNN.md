---
title: 机器学习（8）K-近邻分类器——K-nearest neighbor, KNN
tags: ML Regression Classification
---

#### K-近邻分类器——K-nearest neighbor，KNN

思想；一种无监督的分类方式，也可以用作回归，但是大多是用来分类。给定一个数据集$T={(x_1,y_1),(x_2,y_2),···,(x_N,y_N)}$，其中$x_i$是特征向量，y是样本类别，$\mathbf{x}$是一个待分类的样本特征向量。根据给定的度量空间，在训练集T中找出与$\mathbf{x}$最近的k个点，再根据分类决策规则决定$\mathbf{x}$的类别y。当k=1时，称为最近邻算法。

<!--more-->

##### KNN分类

KNN没有显式的学习过程，取决于距离度量、K值的选择和分类决策规则三个要素。

##### 1.距离度量

距离反映的是相似程度，一般使用欧式距离，也可以是$L_p$或者Minkowski距离。

设特征空间  $\mathcal{X}$  是  n  维实数向量空间  $\mathbf{R}^{n}, x_{i}, x_{j} \in \mathcal{X}, x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}  ，  x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}, x_{i}, x_{j}$  的  $L_{p}$  距离定义为

$$L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n} \vert \vert x_{i}^{(l)}-x_{j}^{(l)} \vert \vert ^{p}\right)^{\frac{1}{p}}$$


这里 $ p \geqslant 1 $ ．当  p=2  时，称为欧氏距离（Euclidean distance），即

$$L_{2}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n} \vert \vert x_{i}^{(l)}-x_{j}^{(l)} \vert \vert ^{2}\right)^{\frac{1}{2}}$$


当  p=1  时，称为曼哈顿距离（Manhattan distance），即

$$L_{1}\left(x_{i}, x_{j}\right)=\sum_{l=1}^{n} \vert \vert x_{i}^{(l)}-x_{j}^{(l)} \vert \vert $$


当  $p=\infty$  时，它是各个坐标距离的最大值，即

$$L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l} \vert \vert x_{i}^{(l)}-x_{j}^{(l)} \vert \vert $$

不同的距离度量所确定的最近邻点不同。

##### 2.K值的选择

K值较小，近似误差减小，估计误差增大，模型更复杂，容易过拟合。

K值较大，近似误差增大，估计误差减小，模型更简单，容易欠拟合。

K值通常选择奇数，避免无法多数表决。

##### 3.分类决策规则

分类：多数表决，类别多的邻居作为其类别。

回归：平均法，即最近的K个样本的样本输出的平均值作为回归预测值。

##### 4.求解算法

（1）暴力求解

- 计算测试数据与各个训练数据之间的距离；

- 按照距离的递增关系进行排序；

- 选取距离最小的前K个点；

- 确定前K个点所在类别的出现频率；

- 返回前K个点中出现频率最高的类别作为测试数据的预测分类。

（2）KD树（数据结构，二叉树的思想）

- 构造树

a.找到方差最大的维度；

b.排序选择该维度的中位数作为根节点，如果是偶数，则选择大的一方作为中位数；

c.大于中位数的构成右子空间，小于中位数的构成左子空间；

d.重复步骤1、2、3；

e.根据维度=中位数划分超矩形空间，每次选择维度与上一次不一样。

- 搜索最近邻

a.找到包含目标点的叶子节点；

b.以目标点为圆心，目标点到叶子节点的距离为半径画圆，得到一个超球体，最近邻的点一定在这个超球体内部；

c.返回叶子节点的父节点，检查另一个子节点的超矩形体是否和超球体相交，如果相交则寻找是否有更近的邻居；

d.如果不相交，返回父节点的父节点，重复3，直至搜索完所有节点。

（3）球树

与KD树类似，但是超平面选的球形。

##### 5.KD树举例

略

##### 6.KNN优缺点

优点：

1.简单，易于理解和实现，也无需估计参数；

2.没有显示的训练过程；

3.适用多分类问题，且比SVM效果好。

缺点：

1.对于多特征的数据集，计算量大；

2.当样本不平衡时，预测的准确性较低；

3.对训练数据的容错性差；

~~~
~~~



~~~
# sklearn实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn.metrics import accuracy_score

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data[:, :2]  # 只取前两列特征（方便可视化）
y = iris.target

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 创建KNN分类器，使用KD树算法
# n_neighbors: 选择的近邻数量
# algorithm: 指定算法,brute(暴力搜索),kd_tree(kd树),ball_tree(球树),auto(自动)
knn = KNeighborsClassifier(n_neighbors=5, algorithm='auto')

# 训练模型
knn.fit(X_train, y_train)

# 预测并评估
y_pred = knn.predict(X_test)
print(f"测试集准确率: {accuracy_score(y_test, y_pred):.2f}")
y_train_pred = knn.predict(X_train)
print("训练集准确率:", accuracy_score(y_train, y_train_pred))

# 可视化决策边界
def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    plt.xlabel('Sepal Length')
    plt.ylabel('Sepal Width')
    plt.title('Perceptron Decision Boundary')
    
plot_decision_boundary(X_train, y_train, knn)
plt.show()
~~~

