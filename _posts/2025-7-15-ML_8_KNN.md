---
title: 机器学习（8）K-近邻分类器——K-nearest neighbor, KNN
tags: ML Regression Classification
---

https://www.cnblogs.com/wj-1314/p/10291284.html

定义：给定一个数据集$T={(x_1,y_1),(x_2,y_2),···,(x_N,y_N)}$，其中$x_i$是特征向量，y是样本类别，$\mathbf{x}$是一个待分类的样本特征向量。根据给定的度量空间，在训练集T中找出与$\mathbf{x}$最近的k个点，在根据分类决策规则决定$\mathbf{x}$的类别y。当k=1时，称为最近邻算法。

<!--more-->

KNN没有显式的学习过程，取决于距离度量、K值的选择和分类决策规则三个要素。

- 距离度量

距离反映的是相似程度，一般使用欧式距离，也可以是$L_p$或者Minkowski距离。

设特征空间  $\mathcal{X}$  是  n  维实数向量空间  $\mathbf{R}^{n}, x_{i}, x_{j} \in \mathcal{X}, x_{i}=\left(x_{i}^{(1)}, x_{i}^{(2)}, \cdots, x_{i}^{(n)}\right)^{\mathrm{T}}  ，  x_{j}=\left(x_{j}^{(1)}, x_{j}^{(2)}, \cdots, x_{j}^{(n)}\right)^{\mathrm{T}}, x_{i}, x_{j}$  的  $L_{p}$  距离定义为

$$L_{p}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{p}\right)^{\frac{1}{p}}$$


这里 $ p \geqslant 1 $ ．当  p=2  时，称为欧氏距离（Euclidean distance），即

$$L_{2}\left(x_{i}, x_{j}\right)=\left(\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|^{2}\right)^{\frac{1}{2}}$$


当  p=1  时，称为曼哈顿距离（Manhattan distance），即

$$L_{1}\left(x_{i}, x_{j}\right)=\sum_{l=1}^{n}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|$$


当  $p=\infty$  时，它是各个坐标距离的最大值，即

$$L_{\infty}\left(x_{i}, x_{j}\right)=\max _{l}\left|x_{i}^{(l)}-x_{j}^{(l)}\right|$$

不同的距离度量所确定的最近邻点不同。

- K值的选择

K值较小，近似误差减小，估计误差增大，模型更复杂，容易过拟合。

K值较大，近似误差增大，估计误差减小，模型更简单，容易欠拟合。

- 分类决策规则

分类：多数表决，类别多的邻居作为其类别。

回归：平均法，即最近的K个样本的样本输出的平均值作为回归预测值。

- 求解算法

（1）暴力求解

计算预测样本和所有训练集中的样本的距离，然后计算出最小的k个距离，接着多数表决。

（2）KD树

主要使用了二叉树的思想

算法流程：

1） 根据方差选择维度

2） 根据维度选择中位数，大于中位数构成左子空间，小于中位数构成右子空间，基于维度和中位数在超平面绘制水平或垂直线

3） 左右子空间根据另一个维度继续划分

kd树搜索最近邻：当我们生成KD树以后，就可以去预测测试集里面的样本目标点了。对于一个目标点，我们首先在KD树里面找到包含目标点的叶子结点。以目标点为圆心，以目标点到叶子节点样本实例的距离为半径，得到一个超球体，最近邻的点一定在这个超球体内部。然后返回叶子节点的父节点，检查另一个子节点包含的超矩形体是否和超球体相交，如果相交就到这个子节点寻找是否有更加近的近邻，有的话就更新最近邻。如果不相交的话那就简单了，我们直接返回父节点的父节点，在另一个子树继续搜索最近邻。当回溯到根节点时，算法结束，此时保存的最近邻节点就是最终的最近邻。

（3）球树