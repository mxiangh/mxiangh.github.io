---
title: 机器学习（7）支持向量机——support vector machines, SVM
tags: ML Regression Classification
typora-root-url: ./..
---

思想：支持向量机（SVM）是一类按监督学习方式对数据进行二元分类的广义线性分类器，其决策边界是对学习样本求解的最大边距超平面。SVM能够处理线性和非线性分类问题（SVC），回归（SVR）以及异常值检测。

<!--more-->

##### 1.SVM使用准则

$n$为特征数，$m$为训练样本数:

- 如果相较于$m$而言， 要大许多，即训练集数据量不够支持我们训练一个复杂的非线性模型，我们选用逻辑回归模型或者不带核函数的支持向量机。
- 如果$n$较小，而$m$大小中等，例如  在 1-1000 之间，而$m$在10-10000之间，使用高斯核函数的支持向量机。
- 如果$n$较小，而$m$较大，例如  在1-1000之间，而$m$大于50000，则使用支持向量机会非常慢，解决方案是创造、增加更多的特征，然后使用逻辑回归或不带核函数的支持向量机。

##### 2.硬间隔

2.1 回顾感知机模型

只考虑二分类问题，假设有 $n$个训练点$ x_{i}$，每个训练点有一个指标 $y_{i}$。训练集即为：$T=\lbrace (x_{1},y_{1}),(x_{2},y_{2}),...,(x_{n},y_{n}) \rbrace$ 其中$ x_{i}\in R_{N} $为输入变量，其分量称为特征或属性。$y_{i}\in Y = \lbrace 1, -1 \rbrace$是输出指标。

问：给定新的输入 $x$，如何推断它的输出$ y=1 $还是$ y=-1$。

答：找到一个函数 $g:R_{N} \rightarrow R$，然后定义下面的决策函数实现输出。

$$f(x)=sign(g(x))$$

其中$ sign(z)$是激活函数，也就是当$ z\geqslant 0 $时取值+1，否则取值-1。确定$ g() $的算法称为分类机。如果$ g(x)=w^{T}x+b$，则确定 w 和 b 的算法称为感知机。

假如数据是完全的线性可分的，那么学习到的模型可以称为硬间隔支持向量机（感知机）。换个说法，硬间隔指的就是完全分类准确，不能存在分类错误的情况。软间隔，就是允许一定量的样本分类错误。

2.2 求解目标

假设两类数据可以被 $ H = \lbrace x:w^{T}x+b = 0 \rbrace $分离，$H$被称为划分超平面。移动$H$直到碰到某个训练点，可以得到两个超平面 $H_{1} $和$ H_{2}$ ，新的两个平面称为支撑超平面，它们分别支撑两类数据。而位于$ H_{1}$ 和 $H_{2}$正中间的超平面$H$是分离这两类数据最好的选择。

超平面$ H_{1}$ 和$ H_{2} $之间的距离称为间隔，也是超平面$H$的法向量$w$。法向量$w$有很多选择，现在的目的是寻找一个$w$使得间隔达到最大。

![](/assets/images/SVM/one.png)

简单点说，就是$ H_{1}$ 和$ H_{2} $之间的距离代表了两个类别之间的距离，这个距离越大，两类样本分的越开，距离在此处被称为间隔，而这个间隔是两类样本中离超平面$H$最近的两个异类样本的距离之和。

2.3 损失函数推导

假设划分超平面的线性方程为：$ \mathbf{w}^T x + b = 0$ ，其中 $ \mathbf{w} = (w_1, w_2, \dots, w_n)$ 为法向量，决定超平面方向；$b $为位移项，决定超平面与原点距离。

根据点到直线的距离公式，样本到超平面的距离：$r = \frac{\mid \mathbf{w}^T x + b\mid }{\mid \mid \mathbf{w}\mid \mid }$

假设超平面能正确分类训练样本，则满足：

$$\begin{cases} 
\mathbf{w}^T x_i + b \geq +1, & y = +1 \\
\mathbf{w}^T x_i + b \leq -1, & y = -1 
\end{cases}$$

距离超平面最近的这几个训练样本点使得上述不等式中等号可以成立，被称为“支持向量”，两个异类支持向量到超平面距离之和为

$$ \gamma = \frac{\mid \mathbf{w}^T x_1 + b\mid }{\mid \mid \mathbf{w}\mid \mid } + \frac{\mid \mathbf{w}^T x_2 + b\mid }{\mid \mid \mathbf{w}\mid \mid } = \frac{2}{\mid \mid \mathbf{w}\mid \mid }$$

也是两个异类支持向量的间隔。

![](/assets/images/SVM/two.png)

注：根据2.2所述，超平面$H$碰到训练点（即支持向量），得到了支撑超平面$H_1$和$H_2$，所以支持向量在支撑超平面上，则 $ \mid \mathbf{w}^T x_i + b \mid = 1$。

需要找到具有最大间隔的“划分超平面”，也就是要找到满足上述不等式约束的参数w和b，使得间隔$ \gamma$最大。

$$ \underset{w,b}{max} \frac{2}{\mid \mid \mathbf{w}\mid \mid }$$

$$s.t. \begin{cases} 
\mathbf{w}^T x_i + b \geq +1, & y = +1, i=1, 2, \cdots ,m\\
\mathbf{w}^T x_i + b \leq -1, & y = -1, i=1, 2, \cdots ,m
\end{cases}$$

最大化$\mid \mid \mathbf{w}\mid \mid ^ {-1}$等价于最小化$\mid \mid \mathbf{w}\mid \mid ^ {2}$，同时化简约束条件（与感知机类似），得

$$ \underset{w,b}{max} \frac{1}{2} \mid \mid \mathbf{w}\mid \mid ^ 2 $$

$$ s.t. \  y_i(\mathbf{w}^T x_i + b) \ge 1 , i=1, 2, \cdots ,m $$

这就是支持向量机。

2.4 求解损失函数

2.4.1 KKT（Karush-Kuhn-Tucker）条件

2.4.1.1 约束优化问题的一般形式

考虑一个约束优化问题：

$$\begin{align*}
\min_{x} &\ f(x) \\
\text{s.t.} &\ g_i(x) \leq(\geq ) 0, \ i = 1, 2, \ldots, m \\
&\ h_j(x) = 0, \ j = 1, 2, \ldots, n
\end{align*}$$

其中 $f(x)$ 是目标函数， $g_i(x)$ 是不等式约束函数， $h_j(x)$是等式约束函数。

2.4.1.2 拉格朗日函数构造

构造拉格朗日函数 $L(x,\alpha,\beta)$ ：

$$L(x,\alpha,\beta) = f(x) + \sum_{i = 1}^{m} \alpha_i g_i(x) + \sum_{j = 1}^{n} \beta_j h_j(x)$$

其中$\alpha_i$ 是对应不等式约束$g_i(x)$的拉格朗日乘子，$\beta_j$是对应等式约束$h_j(x)$ 的拉格朗日乘子。

2.4.1.3 KKT 条件内容

- 梯度为零条件：

拉格朗日函数关于$x$的梯度为零，即$ \nabla_x L(x,\alpha,\beta) = \nabla f(x) + \sum_{i = 1}^{m} \alpha_i \nabla g_i(x) + \sum_{j = 1}^{n} \beta_j \nabla h_j(x) = 0$。这意味着在最优解处，目标函数的梯度与约束函数梯度的线性组合为零。

- 原问题约束条件：

不等式约束 $g_i(x) \leq(\geq ) 0$ （ $i = 1, 2, \ldots, m$ ）和等式约束 $ h_j(x) = 0$ （ $j = 1, 2, \ldots, n$ ）需要满足，这保证了解在可行域内。

- 对偶互补条件（也称松弛互补条件）：

对于不等式约束，$\alpha_i g_i(x) = 0$ （ $i = 1, 2, \ldots, m$ ），且 $\alpha_i \geq 0$ 。这表明要么$\alpha_i = 0$ ，此时对应的不等式约束$g_i(x)$不起作用；要么$g_i(x) = 0$，此时对应的不等式约束是紧约束（在最优解处约束取等号 ），该约束对最优解有影响。

2.4.2 拉格朗日乘子法

我们希望求解2.3构造的损失函数，以此得到划分超平面对应的模型

$$ f(x) = w^T x + b $$

对损失函数约束条件添加拉格朗日乘子，构造拉格朗日函数：

$$L(w,b,\alpha) = \frac{1}{2} \mid \mid \mathbf{w}\mid \mid ^ 2  + \sum_{i=1}{m} \alpha_i (1-y_i(\mathbf{w}^T x_i + b))$$

其中，$\mathbf{\alpha} = (\alpha_1;\alpha_2;\cdots;\alpha_m)$。令$L(w,b,\alpha)$对w和b的偏导为零得

$$ \frac{\partial L}{\partial w} = \mid \mid w \mid \mid-\sum_{i=1}^m\alpha_iy_i\mathbf{x}_i = 0$$

$$ \frac{\partial L}{\partial b} = \sum_{i=1}^m\alpha_iy_i = 0 $$

代入拉格朗日函数，消去w和b，得到

$$\underset{\alpha}{max} \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j + \sum_{i=1}^{m} \alpha_i - \sum_{i=1}^{m} \alpha_i y_i (\sum_{j=1}^{m} \alpha_j y_j x_i^T x_j - b )$$

$$=\underset{\alpha}{max} \sum_{i=1}^{m} \alpha_i - \frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_i \alpha_j y_i y_j x_i^T x_j$$

$$ \begin{aligned}
s.t. & \sum_{i=1}^m\alpha_iy_i = 0 \\
 & \alpha_i \geq 0, i=1,2,...,m
\end{aligned}$$

只要求出拉格朗日乘子$\alpha$，就能求出w和b。将得到的w和b的值代入划分超平面可得模型：

$$ f(x) = w^T x + b = \sum_{i=1}^m\alpha_iy_i\mathbf{x}_i^T x + b$$

上述过程还需要满足KKT条件：

$$\begin{cases} 
\alpha_i \geq 0 ;\\
y_i f(x_i) - 1 \geq 0;\\
\alpha_i(y_i f(x_i) - 1) = 0
\end{cases}$$

于是，对任意训练样本，总有 $\alpha_i=0 $ 或 $y_i f(x_i)=1$。

如果$\alpha_i=0 $，则该样本将不参与划分超平面的求和，也就不会对$f(x)$有任何影响;

如果$\alpha_i>0 $，则必有$y_i f(x_i)=1$，所对应的样本点位于最大间隔边界上，是一个支持向量。

这显示出支持向量机的一个重要性质：训练完成后，大部分的训练样本都不需保留，最终模型仅与支持向量有关.

为了求解拉格朗日乘子$\alpha$，这里引出SMO算法。

2.4.3 序列最小优化（Sequential Minimal Optimization，SMO）算法

2.4.3.1 思想

SVM 的对偶问题是一个凸二次规划问题，常规的二次规划算法在处理大规模数据时计算效率较低。SMO 算法的核心思想是将大的二次规划问题分解为一系列最小的二次规划子问题，每次迭代只优化两个拉格朗日乘子，其他乘子固定，通过不断迭代直至满足 KKT 条件。

2.4.3.2 算法步骤

（1）初始化

- 拉格朗日乘子$\alpha_i = 0$（$i=1,2,...,N$）；
- 偏置 $b = 0$；
- 设设定最大迭代次数和收敛阈值（KKT 条件满足的误差容忍度）。

（2）变量选择（每次迭代选择 2 个变量 $\alpha_i$, $\alpha_j$）

- 第一个变量$\alpha_i$的选择

优先选择违反$KKT$条件最严重的样本。硬间隔$SVM$的$KKT$条件为：

若$\alpha_i = 0$，则$y_i f(x_i) \geq 1$（样本在分类正确的一侧，且远离超平面）；

若$\alpha_i > 0$，则$y_i f(x_i) = 1$（样本为支持向量，在分类边界上）。

其中$f(x_i) = \sum_{k=1}^N \alpha_k y_k (x_k \cdot x_i) + b$ 是样本$x_i$的预测值。若样本不满足上述条件，则违反$KKT$条件，需优先优化。

- 第二个变量$\alpha_j$的选择

选择使 $ \mid E_i - E_j\mid $ 最大的样本$ j$（$j \neq i$），其中 $E_i = f(x_i) - y_i$ 是样本$i$的预测误差。此选择可最大化优化步长，加速收敛。若未找到，则遍历所有其他样本。

（3）求解 2 变量子问题

固定其他变量后，子问题仅含 $\alpha_i$和 $\alpha_j$，需在约束下求解最优值。

- 约束转化

由等式约束$\sum_{k=1}^N \alpha_k y_k = 0$，固定其他变量后可得：$\alpha_i y_i + \alpha_j y_j = \zeta$ ($\zeta = -\sum_{k \neq i,j} \alpha_k y_k $，为常数)

若$y_i = y_j$，则$\alpha_i + \alpha_j = \zeta'$（$\zeta' = \zeta / y_i$），此时 $\alpha_j$ 的取值范围为$\lbrack \max(0, \zeta' - \alpha_i^{old}) , \zeta' \rbrack$；

若$y_i \neq y_j$，则$\alpha_i - \alpha_j = \zeta'$（y是1或-1），此时$\alpha_j$ 的取值范围为$\lbrack 0, \zeta' + \alpha_i^{old}\rbrack$。

（2）求解最优$\alpha_j$

将$\alpha_i$用 $\alpha_j$ 表示（通过约束），代入目标函数后，子问题转化为关于 $\alpha_j$ 的单变量二次函数。其最优解（未剪辑）为：$\alpha_j^{new, unclipped} = \alpha_j^{old} + \frac{y_j (E_i - E_j)}{K(x_i,x_i) + K(x_j,x_j) - 2K(x_i,x_j)}$

其中 $K(x_i,x_j) = x_i \cdot x_j$ 为线性核函数。

由于 $\alpha_j \geq 0$ 且需满足上述区间约束，需对最优解进行剪辑：

$\alpha_j^{new} = \begin{cases} 
H & \text{若 } \alpha_j^{new, unclipped} > H \\
\alpha_j^{new, unclipped} & \text{若 } L \leq \alpha_j^{new, unclipped} \leq H \\
L & \text{若 } \alpha_j^{new, unclipped} < L 
\end{cases}$

（L 和 H 为（3）中步骤一的约束转化里计算的区间上下界）

（3）更新 $\alpha_i$

根据约束关系，由 $\alpha_j^{new}$ 计算 $\alpha_i^{new}$：

若 $y_i = y_j$，则 $\alpha_i^{new} = \zeta' - \alpha_j^{new}$；

若 $y_i \neq y_j$，则 $\alpha_i^{new} = \zeta' + \alpha_j^{new}$。

（4）更新偏置 b

偏置 b 需满足支持向量的约束 $y_k f(x_k) = 1$（$\alpha_k > 0$）。

若 $\alpha_i^{new} > 0$，则用样本 i 计算：$b^{new} = y_i - \sum_{k=1}^N \alpha_k^{new} y_k (x_k \cdot x_i)$

若 $\alpha_j^{new} > 0$，则用样本 j 计算（结果应与上式一致）；

若两者均为 0，则暂时不更新 b。

（5）收敛判断

重复步骤 2-4，直到满足以下条件之一：

- 所有样本均满足 KKT 条件（误差小于预设阈值）；

- 迭代次数达到最大值。

##### 3.软间隔

3.1 优化函数

硬间隔要求训练样本线性可分，即全部分类正确，但现实中样本不一定线性可分。而核函数技巧需要选择核函数，也很难判断有无过拟合。于是乎，引入“软间隔”概念，允许部分样本分类错误。

![](/assets/images/SVM/three.png)

在满足最大化间隔的同时，不满足约束$y_i(\mathbf{w}^T x_i + b) \geq 1$的样本应尽可能少，优化目标为

$$ \underset{w,b}{min} \frac{1}{2} \mid \mid w \mid \mid^2 + C \sum_{i=1}^{m} l_{0/1}(y_i(w^T x_i + b ) - 1) $$

其中$C>0$是正则化常数，$l_{0/1}$是“0/1损失函数”

$$ l_{0/1}(z)=\left\{\begin{array}{l}
1, \text { if } z < 0 \\
0, otherwise
\end{array}\right.$$

显然，当C为无穷大时，所有样本都被迫满足约束，使得样本分类正确，重新变为“硬间隔支持向量机”；当C取有限值时，允许一些样本不满足约束。

由于$l_{0/1}$非凸、非连续，常用一些其他的“替代损失”函数替代它，这些函数是凸的连续函数

$$ \text{ hinge损失 }：l_{hinge}(z)=max(0,1-z) $$

$$ \text{指数损失}(exponential loss)：l_{exp}(z)=exp(-z) $$

$$ \text{ 对率损失 }(logistic loss)：l_log(z)=log(1+exp(-1)) $$

假设使用hinge函数，则

$$ \underset{w,b}{min} \frac{1}{2} \mid \mid w \mid \mid^2 + C \sum_{i=1}^{m} max(0,1-y_i(w^T x_i + b )) $$

令$\xi_i = max(0,1-y_i(w^T x_i + b )) \geq 0 $，则得到“软间隔支持向量机”

$$ \underset{w,b,\xi_i}{min} \frac{1}{2} \mid \mid w \mid \mid^2 + C \sum_{i=1}^{m} \xi_i $$

$$ \begin{aligned}
s.t. \quad  & y_i(w^T x_i + b) \geq 1-\xi_i \\
& \xi_i \geq 0,i=1,2,\cdots,m 
\end{aligned}$$

3.2 求解

与“硬间隔支持向量机”求解类似，可以使用拉格朗日乘子和SMO算法。

$$ L(\mathbf(w),b,\alpha ,\xi ,\mu ) = \frac{1}{2} \mid \mid w \mid \mid^2 + C \sum_{i=1}^{m} \xi_i + \sum_{i=1}{m} \alpha_i(1-\xi_i-y_i(w^T x_i + b))-\sum_{i=1}{m}\mu_i \xi_i $$

其中，$\alpha_i \geq 0$，$\mu_i \geq 0$是拉格朗日乘子。

令L分别对$\mathbf{w}$，$b$，$\xi_i$的偏导为零

$$\begin{aligned}
\mathbf{w} & =\sum_{i=1}^{m} \alpha_{i} y_{i} \mathbf{x}_{i} \\
0 & =\sum_{i=1}^{m} \alpha_{i} y_{i} \\
C & =\alpha_{i}+\mu_{i}
\end{aligned}$$

代入原损失函数

$$\begin{array}{ll}
\underset{\mathbf{\alpha}}{max} & \sum_{i=1}^{m} \alpha_{i}-\frac{1}{2} \sum_{i=1}^{m} \sum_{j=1}^{m} \alpha_{i} \alpha_{j} y_{i} y_{j} \mathbf{x}_{i}^{T} \mathbf{x}_{j} \\
\text { s.t. } & \sum_{i=1}^{m} \alpha_{i} y_{i}=0 \\
& 0 \leq \alpha_{i} \leq C, \quad i=1,2, \ldots, m
\end{array}$$

同时满足KKT条件，$f(\mathbf{x_i})=w^T \mathbf{x_i} + b$

$$ \left\{\begin{array}{l}
\alpha_i \geq 0,\quad \mu_i \geq 0 ,\\
y_i f(\mathbf{x_i}) - 1 + \xi_i \geq 0 ,\\
\alpha_{i}(y_i f(\mathbf{x_i}) - 1 + \xi_i) = 0, \\
\xi_i \geq 0,\mu_i \xi_i = 0.
\end{array}\right.$$

于是，对任意训练样本，总有 $\alpha_i=0$或$y_i f(\mathbf{x_i}) = 1 - \xi_i$。

若$\alpha_i=0$，则该样本不会对$f(\mathbf{x})$有任何影响；若$\alpha_i > 0$，则必有$y_i f(\mathbf{x_i})=1 - \xi_i$，即该样本
是支持向量。

若$\alpha_i<C$，则$\mu_i>0$，进而有$\xi_i=0$，即该样本恰在最大间隔边界上;若$\alpha_i=C$，则有$\mu_i=0$，此时若$\xi_i \geq 1$则该样本落在最大间隔内部，若$\xi_i>1$则该样本被错误分类，所以“软间隔支持向量机”的最终仅与支持向量有关。

##### 4.支持向量回归

传统的回归模型基于模型输出$f(\mathbf{x}))$与真实输出之间y的差别来计算损失，当二者完全相同，损失函数才为0。支持向量回归假设我们能容忍$f(\mathbf{x}))$与y之间最多有$\epsilon$的偏差，只有二者差别绝对值大于$\epsilon$才计算损失。

![](/assets/images/SVM/four.png)



~~~
# sklearn分类
import numpy as np
import matplotlib.pyplot as plt
from sklearn.datasets import load_iris
from sklearn.model_selection import train_test_split
from sklearn.metrics import accuracy_score
from sklearn.preprocessing import StandardScaler
from sklearn.svm import SVC  # 导入支持向量机分类器

# 加载鸢尾花数据集
iris = load_iris()
X = iris.data[:100, :2]  # 只取前两列特征（方便可视化）
y = iris.target[:100]    # 只取前两类（0和1）

# 划分训练集和测试集
X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42)

# 特征标准化
scaler = StandardScaler()
X_train_scaled = scaler.fit_transform(X_train)
X_test_scaled = scaler.transform(X_test)

# 初始化并训练支持向量机模型
# kernel：核函数，'linear'（线性核），'rbf'（径向基核，默认），'poly'（多项式核）等
# C：正则化参数，控制软间隔的严格程度，值越小正则化越强
# gamma：核系数，用于'rbf'、'poly'和'sigmoid'，值越大模型越复杂
svm_model = SVC(kernel='linear', C=1.0, random_state=42)
svm_model.fit(X_train_scaled, y_train)

# 预测并评估
y_pred = svm_model.predict(X_test_scaled)
print(f"测试集准确率: {accuracy_score(y_test, y_pred):.2f}")
y_train_pred = svm_model.predict(X_train_scaled)
print(f"训练集准确率: {accuracy_score(y_train, y_train_pred):.2f}")

# 可视化决策边界
def plot_decision_boundary(X, y, model):
    x_min, x_max = X[:, 0].min() - 1, X[:, 0].max() + 1
    y_min, y_max = X[:, 1].min() - 1, X[:, 1].max() + 1
    xx, yy = np.meshgrid(np.arange(x_min, x_max, 0.02),
                         np.arange(y_min, y_max, 0.02))
    Z = model.predict(np.c_[xx.ravel(), yy.ravel()])
    Z = Z.reshape(xx.shape)
    
    plt.contourf(xx, yy, Z, alpha=0.3)
    plt.scatter(X[:, 0], X[:, 1], c=y, edgecolors='k')
    # 标记支持向量
    plt.scatter(X[model.support_, 0], X[model.support_, 1], 
                s=100, linewidth=1, facecolors='none', edgecolors='red', 
                label='Support Vectors')
    plt.xlabel('Sepal Length (scaled)')
    plt.ylabel('Sepal Width (scaled)')
    plt.title('SVM Decision Boundary (scikit-learn)')
    plt.legend()
    
plot_decision_boundary(X_train_scaled, y_train, svm_model)
plt.show()

# 打印支持向量的数量
print(f"支持向量的数量: {len(svm_model.support_)}")

~~~



~~~
~~~

