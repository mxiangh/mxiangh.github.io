---
title: 机器学习（21）密度聚类——Density-Based Spatial Clustering of Appliactions with Noise，DBSCAN
tags: ML Clustering
---

思想：对于簇中的每个点，其给定半径（Eps）邻域内必须包含至少最小数量（MinPts）的点，即密度需超过阈值。邻域形状由距离函数dist(p,q)决定（如曼哈顿距离对应矩形邻域）。为便于可视化，示例均采用二维欧氏距离。

<!--more-->

原论文：A density-based algorithm for discovering clusters in large spatial databases with noise

定义1：（点的Eps邻域）：

点p的Eps邻域$  N_{E p s}(p)  $定义为：

$$ N_{E p s}(p)=\{q \in D \mid \operatorname{dist}(p, q) \leq E p s\} $$

若简单要求簇中每个点的Eps邻域至少包含MinPts个点，会忽略核心点（core point，邻域满足MinPts）与边界点（border point，邻域不满足MinPts）的差异。为此，我们要求：对于簇C中的任意点p，存在C中的点q使得p位于q的Eps邻域内，且 $ N_{\mathrm{Eps}}(q)  $满足MinPts条件。

定义2（直接密度可达）：

点  p  从点  q  关于Eps、MinPts直接密度可达的条件：

1. $ p \in N_{\mathrm{Eps}}(q)  ；$
2. $ \left|N_{\mathrm{Eps}}(q)\right| \geq \operatorname{MinPts} $（核心点条件）。

该关系对核心点对称，但对核心点与边界点不对称。

定义3（密度可达）：

若存在点链 $ p_{1}, \ldots, p_{n}\left(p_{1}=q, p_{n}=p\right) $ ，且 $ p_{i+1} $ 从 $ p_{i} $ 直接密度可达，则  p  从  q  密度可达。该关系可传递但不对称。

簇与璟言的正式定义

定义5（簇）：

数据库D中关于Eps、MinPts的簇C需满足：

1．最大性：若$p \in C $且q从p密度可达，则$ q \in C $；

2．连通性：$ \forall p, q \in C $, p与q密度相连。

定义6（噪声）：

噪声是D中不属于任何簇$ C_{i} $的点的集合：

$$ \text { noise }=\left\{p \in D \mid \forall i: p \notin C_{i}\right\} $$

流程：任意选择一个没有类别的核心对象作为种子，然后找到所有这个核心对象能够密度可达的样本集合，即为一个聚类簇。接着继续选择另一个没有类别的核心对象去寻找密度可达的样本集合，这样就得到另一个聚类簇。一直运行到所有核心对象都有类别为止。

参数Eps与MinPts的确定

基于以下观察：对于点p到其第k近邻的距离d，其d-邻域通常包含k+1个点。定义k-dist函数表示各点到第k近邻的距离，按k-dist值降序排序后：

阈值点（threshold point）：位于“最稀疏”簇的k-dist最大值处（对应排序图中首个“谷底”）。

参数设定：取Eps=k-dist(阈值点)，MinPts=k。

第一个是一些异常样本点或者说少量游离于簇外的样本点，这些点不在任何一个核心对象在周围，在DBSCAN中，我们一般将这些样本点标记为噪音点。

第二个是距离的度量问题，即如何计算某样本和核心对象样本的距离。在DBSCAN中，一般采用最近邻思想，采用某一种距离度量来衡量样本距离，比如欧式距离。这和KNN分类算法的最近邻思想完全相同。对应少量的样本，寻找最近邻可以直接去计算所有样本的距离，如果样本量较大，则一般采用KD树或者球树来快速的搜索最近邻。

第三种问题比较特殊，某些样本可能到两个核心对象的距离都小于ϵ，但是这两个核心对象由于不是密度直达，又不属于同一个聚类簇，此时DBSCAN采用先来后到，先进行聚类的类别簇会标记这个样本为它的类别。也就是说DBSCAN的算法不是完全稳定的算法。

DBSCAN的主要优点有：

1）可以对任意形状的稠密数据集进行聚类，相对的，K-Means之类的聚类算法一般只适用于凸数据集。

2）可以在聚类的同时发现异常点，对数据集中的异常点不敏感。

3）聚类结果没有偏倚，相对的，K-Means之类的聚类算法初始值对聚类结果有很大影响。

DBSCAN的主要缺点有：

1）如果样本集的密度不均匀、聚类间距差相差很大时，聚类质量较差，这时用DBSCAN聚类一般不适合。

2）如果样本集较大时，聚类收敛时间较长，此时可以对搜索最近邻时建立的KD树或者球树进行规模限制来改进。

3）调参相对于传统的K-Means之类的聚类算法稍复杂，主要需要对距离阈值ϵ，邻域样本数阈值MinPts联合调参，不同的参数组合对最后的聚类效果有较大影响。