---
title: 强化学习（1）基础概念
tags: RL
typora-root-url: ./..
---

介绍状态、行动、状态转移、状态转移概率、策略、奖励、奖励概率、轨迹、回合（试验）、马尔科夫决策过程。

<!--more-->

注：本文以赵世钰第一节课程为基础，对知识进行总结。

##### 1.引入

强化学习讨论的问题是一个智能体（agent）怎么在一个复杂不确定的环境（environment）里面去极大化它能获得的奖励。

![](/images/RL/1.png)

在本文例子中，agent是一个机器人，environment就是这个九宫格，agent需要从start走到target，其中forbidden是禁止区。

##### 2.state（状态）

定义：environment的描述，表示当前的“处境”或“情境”。

在这个例子中，state就是agent在九宫格的位置，这里一共有九个位置，也就有九个state：$s_1, s_2, \cdots, s_9$.

所有状态的集合被称为状态空间（state space）：$S=\lbrace s_i \rbrace_{i=1}^9$.

![](/images/RL/2.png)

##### 3.action（行动）

定义：在每一个state可以采取的行动。

在这里例子中，每个state共有五个action，$a_1$为向上(upwards) ，$a_2$为向右(rightwards) ，$a_3$为向下(downwards) ，$a_4$为向左(leftwards) ，$a_5$为原地不动 (unchanged)。

![](/images/RL/3.png)

每个state的所有可采取的action被称为行动空间（action space）：$A(s_i)= \lbrace a_i \rbrace_{i=1}^5$.

在这个例子中，每个state的action space是一样的，但是在其他任务中，action space可能不一样。

##### 4.state transition（状态转移）

 当agent在当前state下做出action后，会进入下一个state，这个过程被称为state transition。

例1，在状态$s_1$选择行动$a_2$：

$$ s_1 \overset{a_2}{\rightarrow}  s_2 $$

例2，在状态$s_1$选择行动$a_1$：

$$ s_1 \overset{a_1}{\rightarrow}  s_1 $$

当然，我们可以使用不同的方式去定义state transition。在$s_5$选择$a_2$可以有不同的结果：

结果1，如果forbidden允许agent进入，但是会受到惩罚：

$$ s_5 \overset{a_2}{\rightarrow}  s_6 $$

结果2，如果forbidden不允许agent进入：

$$ s_5 \overset{a_2}{\rightarrow}  s_5 $$

在本文中，考虑只结果1的情形。此时，我们可以使用一个表格来描述所有的state transition：

![](/images/RL/4.png)

我们可以用条件概率来表示状态转移的可能性（state transition probability），例如在状态$s_1$，选择行动$a_2$，进入下一个状态$s_2$：

$$\begin{aligned}
p\left(s_{2} \mid s_{1}, a_{2}\right) & =1 \\
p\left(s_{i} \mid s_{1}, a_{2}\right) & =0 \quad \forall i \neq 2
\end{aligned}$$

上述的情况是确定性的，即在同一个state，同一个action下，只有一个结果。但是如果有其他影响，state transition可能是不确定的。例如，有风的时候，$s_1$选择$a_2$时，可能会进入$s_2$，也可能进入$s_5$。

![](/images/RL/5.png)

##### 5.policy（策略）

policy用来告诉agent选择哪个action进入下一个state。例如，可以用箭头表示在每个state的最佳policy。

![](/images/RL/6.png)

基于这些policy，我们能得到不同起始点到终点的路径。

![](/images/RL/7.png)

使用条件概率来描述在当前state选择某个action的概率，如果只有一个可选action，则为确定性策略（deterministic policy）：

$$\begin{aligned}
\pi (a_{1} \mid s_{1} ) & =0 \\
\pi (a_{2} \mid s_{1} ) & =1 \\
\pi (a_{3} \mid s_{1} ) & =0 \\
\pi (a_{4} \mid s_{1} ) & =0 \\
\pi (a_{5} \mid s_{1} ) & =0
\end{aligned}$$

如果可选action不止一个，则为随机性策略（stochastic policies）：

$$\begin{aligned}
\pi (a_{1} \mid s_{1} ) & =0 \\
\pi (a_{2} \mid s_{1} ) & =0.5 \\
\pi (a_{3} \mid s_{1} ) & =0.5 \\
\pi (a_{4} \mid s_{1} ) & =0 \\
\pi (a_{5} \mid s_{1} ) & =0
\end{aligned}$$

可以用一个表格来描述所有的policy。

![](/images/RL/8.png)

##### 6.reward（奖励）

agent执行一个action后，可能会进入forbidden，或进入target，又或者进入普通区域，对于不同的结果通常会有不同的收获，这种收获用reward来描述。

正向的reward表示鼓励这种action。

负向的reward表示惩罚这种action。

此外，0表示没有惩罚，正向也可能有惩罚的作用。

![](/images/RL/2.png)

在这个例子中，我们定义一下几种情况：

（1）如果agent尝试越过边界，则$r_{bound}=-1$；

（2）如果agent尝试进入forbidden，则$r_{forbidden}=-1$；

（3）如果agent到达taget，则$r_{taget}=-1$；

（4）其他情况下，$r = 0$。

reward可以看成是一种人机交互，我们能引导agent做出我们所期望的行为。

同样地，可以用一个表格来描述reward transition，也就是agent在状态转移过程中，奖励的变化过程。

![](/images/RL/9.png)

同条件概率来描述这个过程，例如，在状态$s_1$选择行动$a_1$。得到的reward是$-1$：

$$ p(r=-1 \mid s1,a1) = 1 \  and \  p(r\ne-1 \mid s1,a1) = 0$$

和状态转移一样，reward transition可能是随机的。

##### 7.trajectory and return（轨迹和回报）

![](/images/RL/10.png)

一个trajectory指的是一条“状态-行动-奖励”链：

$$ s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 $$

return是指trajectory上reward的总和：

$$return = 0+0+0+1=1$$

![](/images/RL/11.png)

对于不同的policy，它的trajectory通常也不一样：

$$ s_1 \xrightarrow[r=0]{a_3} s_4 \xrightarrow[r=-1]{a_3} s_7 \xrightarrow[r=0]{a_2} s_8 \xrightarrow[r=1]{a_2} s_9 $$

这条路径的return为：

$$return = 0-1+0+1=0$$

从直觉上，第一个policy会比第二个好，因为它没有经过forbidden区域。

从数学上，第一个policy也比第二个好，因为它的return更大。

所以，return可以用来评估一个policy的好坏。

##### 8.discounted return（折扣回报）

![](/images/RL/12.png)

一个trajectory可能是无穷的，当它达到target后，如果想要获得reward，就会一直停在$s_9$：

$$ s_1 \overset{a_2}{\rightarrow}  s_2 \overset{a_3}{\rightarrow}  s_5 \overset{a_3}{\rightarrow}  s_8 \overset{a_2}{\rightarrow}  s_9 \overset{a_5}{\rightarrow}  s_9 \overset{a_5}{\rightarrow}  s_9 \dots $$

此时return也是无穷的：

$$return = 0+0+0+1+1+1+ \cdots = \infty $$

此时这种定义没有意义，因为return发散了。于是，这里引出折扣因子（discount factor）$\gamma = \lbrack0,1)$。

将$\gamma$和return结合，得到了discounted return：

$$ \begin{aligned}
\text { discounted return } & =0+\gamma 0+\gamma^{2} 0+\gamma^{3} 1+\gamma^{4} 1+\gamma^{5} 1+\ldots \\
& =\gamma^{3} (1+\gamma+\gamma^{2}+\ldots )=\gamma^{3} \frac{1}{1-\gamma} 
\end{aligned}$$

这里有两个作用：

（1）将总和变为有限；

（2）平衡远期和近期的回报。

如果$\gamma$接近0，那么discounted return的值由近期获得的return为主；

如果$\gamma$接近1，那么discounted return的值由远期获得的return为主。

##### 9.episode or trial（回合或试验）

使用policy与environment交互时，agent可能在终端state停止，由此产生的trajectory被称为一个episode或一次trial。

![](/images/RL/10.png)

例如：

$$ s_1 \xrightarrow[r=0]{a_2} s_2 \xrightarrow[r=0]{a_3} s_5 \xrightarrow[r=0]{a_3} s_8 \xrightarrow[r=1]{a_2} s_9 $$

通常假设一个episode是一个有限的trajectory，具有episode的任务被称为episode tasks。

有些任务可能没有终端状态，这意味着与环境的交互永远不会结束，这种任务称为持续性任务。在上述的网格例子中，考虑以下两种情形：

情形1，将target看成一种终端状态，一旦agent进入target，就不再离开，r总是为0，此时是情景任务。

情形2，将target看成一种正常状态，agent可以使用policy离开target，并且再次进入时，r=1，此时是持续任务。

本文考虑第二种情形，故我们不需要区分taget state和其他的state。

##### 10.Markov decision process (MDP) 马尔科夫决策过程

集合：状态集合$S$，每个状态的动作集合$A(s)$，在某个状态选择某个动作的奖励集合$R(s,a)$。

概率分布：状态转移概率$p(s_{i+1} \mid s_i,a)$，奖励概率$p(r \mid s,a)$。

策略：在状态$s$，选择行动$a$的概率$ \pi (a \mid s)$。

马尔科夫过程Markov process：

$$p(s_{t+1} \mid a_t, s_t, \dots , a_0,s_0) = p(s_{t+1} \mid a_t , s_t ) $$

$$p(r_{t+1} \mid a_t, s_t, \dots , a_0,s_0) = p(r_{t+1} \mid a_t , s_t ) $$

马尔科夫过程是没有记忆性的，即下一个状态只取决于当前状态，与过去的状态无关。例如在上面的例子中，agent进入下一个state时，只取决于在当前的state选择的某个action，至于之前怎么到达当前state，并不需要考虑。

将网格例子抽象为一般化模型：

![](/images/RL/13.png)

圆圈代表状态，带箭头的链接代表状态转换。一旦给出了策略，马尔可夫决策过程就变成了马尔可夫过程。
