---
title: 自然语言处理（4)Attention Model
tags: DL RNN NLP
typora-root-url: ./..
---

学习注意力模型。

<!--more-->

论文：2014，Neural Machine Translation by Jointly Learning to Align and Translate，带有注意力的编码解码

##### 1.论文解读

在对于传统的seq2seq模型，有一个潜在的问题就是编码器总是将所有出入信息编码为一个定长的隐向量，这可能会导致网络无法解决长句子，尤其是那些比训练语料还要长的句子，当输入的句子长度上升时，传统的编码器—解码器模型的表现急剧下降。

所以引入了权重，将输入句子编码为向量序列，在解码时根据权重自适应地选择这些向量的一个子集，这样就不用将源语句所有信息压缩到固定长度的向量。

解码器中隐状态计算方式为

$$ s_i = f(s_{i-1},y_{i-1},c_i)$$

在之前的seq2seq中，上下文向量是编码器最终的隐状态，而这篇论文将上下文向量$c_i$计算为标注$h_j$的加权和

$$ c_i = \sum_{j=1}^{T_x} \alpha_{ij} h_j $$

每个标注$h_j$的权重$\alpha_j$由下式计算

$$ \alpha_{ij} = \frac{exp(e_{ij})}{\sum_{k=1}^{T_x}exp(e_{ik})} $$

其中

$$ e_{ij} = a(s_{i-1},h_j)=v_a^T tanh(W_a s_{i-1} + U_a h_j) $$

其中，$s_i$是解码器时间步$i$的隐状态，$h_i$是编码器时间步$i$的隐状态，$e_{ij}$是对齐模型。

可以将取加权和的方法理解为计算期望标注，即对所有标注计算期望，第$i$个上下文向量$c_i$是标注在概率$\alpha_{ij}$下的期望标注。其中，$\alpha_{ij}$的作用相当于目标词$y_i$与源词$x_j$对齐或从源词$x_j$翻译而来的概率。

概率$\alpha_{ij}$或与其相关的能量$e_{ij}$反映了在决定下一个状态$s_i$和生成$y_i$时，标注$h_j$相对于之前的隐藏状态$s_{i - 1}$的重要性。直观地说，这实现了解码器中的注意力机制。解码器决定源语句的哪些部分需要关注。通过让解码器具备注意力机制，我们减轻了编码器将源语句的所有信息编码到固定长度向量中的负担。借助这种新方法，所有信息可以分布在整个标注序列中，解码器可以据此有选择地提取这些信息。



