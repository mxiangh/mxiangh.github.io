---
title: 爬虫（2）案例实践
tags: WebCrawler
typora-root-url: ./..
---

爬取国家统计局数据。

<!--more-->

目标网址：https://data.stats.gov.cn/easyquery.htm?cn=C01

##### 1.网页分析

（1）进入网页

![](/images/WebCrawler/11.png)

（2）按F12或者鼠标右键空白区点击检查，点击网络，找到XHR。

![](/images/WebCrawler/12.png)

（3）点击指标下面的第一个一级指标综合，出现一条新内容，点击内容，选择负载，可以看到综合这个指标对应的信息，id为A01。

![](/images/WebCrawler/13.png)

点击第二个一级指标国民经济核算，出现第二条数据，继续看负载，id为A02，其他的不变。说明每个一级指标都有一个不同的唯一id，想要获取对应数据，需要知道对应指标id。

![](/images/WebCrawler/14.png)

（4）点击综合下的第一个二级指标行政划分，继续看负载。

![](/images/WebCrawler/15.png)

第一个m对应的QueryData，这里表示这是用来获取数据的。点击预览，可以看到行政划分的所有数据都存在这里。

![](/images/WebCrawler/16.png)

如果想获取这部分数据，就需要知道这个网页的链接，点击标头，发现第一个就是。如果知道http协议的话，可以知道，？前面是网址，后面是对应的参数信息，参数不同，网页内容也不同。观察这个url可以发现，？后面的内容基本都在负载里面。其中，“k1”是一个时间戳。

![](/images/WebCrawler/17.png)

获取方法分为get和post，图中第二行请求方法为get，说明这个网页要使用get请求。

##### 2.代码实现

~~~
import requests
import time

# 用来获取 时间戳
def gettime():
    return int(round(time.time() * 1000))

# 头部信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) '
                  'AppleWebKit/605.1.15 (KHTML, like Gecko) '
                  'Version/12.0 Safari/605.1.15'
}

# 请求参数，对应行政划分页面的负载信息
params = {
    'm': 'QueryData',
    'dbcode': 'hgnd',
    'rowcode': 'zb',
    'colcode': 'sj',
    'wds': '[]',
    # A0101对应二级指标唯一代码，这里是行政划分
    'dfwds': '[{"wdcode":"zb","valuecode":"A0101"}]',
    'k1': str(gettime())  # 时间戳参数，防止缓存
}

# 目标网址(问号前面的东西)
url = 'http://data.stats.gov.cn/easyquery.htm'

# 发出请求，使用get方法，这里使用我们自定义的头部和参数
r = requests.get(url, headers=headers, params=params)
# 打印返回过来的状态码
print(r.status_code)
# 打印我们构造的url
print(r.url)
# 打印返回的数据
print(r.text)
~~~

这里就完成了数据获取，获得的是一个JSON数据格式，这里的数据和网页上行政划分的预览信息是一样的。

##### 3.数据解析

先将数据解析为python字典。

~~~
import json

# 解析JSON字符串为Python字典
data = json.loads(r.text)
~~~

![](/images/WebCrawler/18.png)

这里有两个层级，第二层的datanodes存放了所有数值数据，wdnodes内部有两个字典，第一个字典存放了列指标，第二个字典存放了行指标。

先获得行指标和列指标名称，指标名称存放在nodes下的cname里。

~~~
# 列指标
col_name = []
for i in data["returndata"]["wdnodes"][0]["nodes"]:
    col_name.append(i["cname"])
# 行指标
row_name = []
for i in data["returndata"]["wdnodes"][1]["nodes"]:
    row_name.append(i["cname"])
print(col_name)
print(row_name)
~~~

现在根据列指标，获取对应长度的行指标数据。

~~~
all_data = []
row_data = []
index = 0
for i in data["returndata"]["datanodes"]:
    index += 1
    row_data.append(i["data"]["data"])
    if index == len(row_name):
        all_data.append(row_data)
        index = 0
        row_data = []

print(all_data)
~~~

##### 4.数据保存

如果需要对数据进行分析，可以先转成pandas格式。

~~~
import pandas as pd

df = pd.DataFrame(data=all_data, index=col_name, columns=row_name)
~~~

![](/images/WebCrawler/19.png)

可以发现，这个数据和官网数据一模一样。如果列指标需要单位的话，也可以再去爬取。

如果想保存为CSV格式，可以运行以下代码。

~~~
# 保存为 CSV 文件
df.to_csv('data.csv')
~~~

如果想保存为Excel格式，可以运行以下代码。

~~~
# 保存为 Excel 文件
df.to_excel('data.xlsx')
~~~

##### 5.代码汇总

~~~
import requests
import time
import json
import pandas as pd

# 用来获取 时间戳
def gettime():
    return int(round(time.time() * 1000))

# 头部信息
headers = {
    'User-Agent': 'Mozilla/5.0 (Macintosh; Intel Mac OS X 10_14) '
                  'AppleWebKit/605.1.15 (KHTML, like Gecko) '
                  'Version/12.0 Safari/605.1.15'
}

# 请求参数
params = {
    'm': 'QueryData',
    'dbcode': 'hgnd',
    'rowcode': 'zb',
    'colcode': 'sj',
    'wds': '[]',
    # A0101对应二级指标唯一代码
    'dfwds': '[{"wdcode":"zb","valuecode":"A0101"}]',
    'k1': str(gettime())  # 时间戳参数，防止缓存
}

# 目标网址(问号前面的东西)
url = 'http://data.stats.gov.cn/easyquery.htm'

# 发出请求，使用get方法，这里使用我们自定义的头部和参数
r = requests.get(url, headers=headers, params=params)

# 解析JSON字符串为Python字典
data = json.loads(r.text)

col_name = []
for i in data["returndata"]["wdnodes"][0]["nodes"]:
    col_name.append(i["cname"])
row_name = []
for i in data["returndata"]["wdnodes"][1]["nodes"]:
    row_name.append(i["cname"])

all_data = []
row_data = []
index = 0
for i in data["returndata"]["datanodes"]:
    index += 1
    row_data.append(i["data"]["data"])
    if index == len(row_name):
        all_data.append(row_data)
        index = 0
        row_data = []

# 保存为 CSV 文件
df.to_csv('data.csv')

# # 保存为 Excel 文件
# df.to_excel('data.xlsx')
~~~

如果想要爬取其他的数据，更改对应二级指标唯一代码就可以了。
