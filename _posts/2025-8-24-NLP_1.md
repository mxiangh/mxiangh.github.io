---
title: 自然语言处理（1）用RNN训练一个语言模型
tags: DL RNN NLP
typora-root-url: ./..
---

从零开始基于循环神经网络实现字符级语言模型，这个模型在H.G.Well的时光机器数据集上训练。

<!--more-->

##### 1.文本预处理

在训练一个RNN模型前，首先要面对文本数据的处理问题，⼀篇文章可以被简单地看作是⼀串单词序列，甚⾄是⼀串字符序列，解析文本的常见步骤包括：

- 将⽂本作为字符串加载到内存中。
- 将字符串拆分为词元（如单词和字符）。
- 建立⼀个词表，将拆分的词元映射到数字索引。
- 将文本转换为数字索引序列，方便模型操作。

注：其实这里已经属于NLP（自然语言处理）的范畴了，3.4汇总了文本预处理代码，可以直接使用，不必对NLP文本处理过程进行了解。

###### 1.1 读取数据集

这里选择李沐动手学深度学习里相同的数据集，H.G.Well的时光机器，当然，这里不会使用d2l这个函数，而是用一个简单的爬虫代码获得txt文本文件。

~~~
import requests

# 数据存放网址
url = "https://www.gutenberg.org/cache/epub/35/pg35.txt"
response = requests.get(url)
with open('TimeMachine.txt', 'w', encoding='utf-8') as f:
    f.write(response.text)
print("文件已成功保存！")    
~~~

这里对代码不做详解，当运行成功时，文件夹会多出一个TimeMachine.txt文件，接着，导入文件并对文本数据进行简单清洗和处理。

~~~
import re

def read_time_machine():
    with open('TimeMachine.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 对每行进行清洗
    # 1.re.sub：去除非字母字符
    # 2.strip：去除字符串首尾的空白字符
    # 3.lower：所有大写字母转为小写字母
    cleaned_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    # 对处理后的列表，删除里面的空字符串
    return [line for line in cleaned_lines if line]

lines = read_time_machine()
print(f'文本总行数: {len(lines)}')
~~~

###### 1.2 词元化

3.1节中最后的lines是已经处理好的文本列表，列表中的每个元素是一个文本序列，每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单元。现在，将lines处理成一个由词元列表组成的列表，其中每个词元都是一个字符串。

~~~
def tokenize(lines, token='word'):
    # 将⽂本⾏拆分为单词或字符词元
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)
        
tokens = tokenize(lines)
# 打印前11行数据
for i in range(11):
    print(tokens[i])
~~~

对tokenize进行简单解释，token为要采取的词元类型，当选择word（默认）时，词元是一个单词，当选择char时，词元是一个字母或字符。

###### 1.3 词表

词元的类型是字符串，但是模型需要的输入是数字（只有数字能做计算）。所以通常需要构建一个字典，或者称词表（vocabulary），⽤来将字符串类型的词元映射到从0开始的数字索引中，这个索引唯一。

将训练集中的所有数据合并在一起，得到的结果称为语料（corpus）。根据每个唯一词元的出现频率，为其分配一个数字索引，得到词表。

很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到⼀个特定的未知词元“<unk>”。我们可以选择增加⼀个列表，⽤于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）。

注：语料库是大量文本数据组成的集合，词表是语料库中提取出来的唯一词元。

~~~
import collections

# 构建词表
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                
    def __len__(self):
        return len(self.idx_to_token)
        
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]
        
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
        
    @property
    def unk(self): # 未知词元的索引为0
        return 0
        
    @property
    def token_freqs(self):
        return self._token_freqs
            
def count_corpus(tokens):
    # 统计词元的频率
    # 这⾥的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成⼀个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

vocab = Vocab(tokens)

for i in [0, 10]:
    print('⽂本:', tokens[i])
    print('索引:', vocab[tokens[i]])
~~~

上述代码使⽤了时光机器数据集作为语料库来构建词表。

###### 1.4 代码汇总

~~~
# 文本预处理，代码汇总
import requests
import re
import collections

# 获取数据
url = "https://www.gutenberg.org/cache/epub/35/pg35.txt" 
response = requests.get(url)
with open('TimeMachine.txt', 'w', encoding='utf-8') as f:
    f.write(response.text)

# 数据清洗
def read_time_machine():
    with open('TimeMachine.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 对每行进行清洗
    # 1.re.sub：去除非字母字符
    # 2.strip：去除字符串首尾的空白字符
    # 3.lower：所有大写字母转为小写字母
    cleaned_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    # 对处理后的列表，删除里面的空字符串
    return [line for line in cleaned_lines if line]

# 词元化
def tokenize(lines, token='word'):
    # 将⽂本⾏拆分为单词或字符词元
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)
        
# 构建词表
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                
    def __len__(self):
        return len(self.idx_to_token)
        
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]
        
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
        
    @property
    def unk(self): # 未知词元的索引为0
        return 0
        
    @property
    def token_freqs(self):
        return self._token_freqs
            
# 统计词元的频率
def count_corpus(tokens):
    # 这⾥的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成⼀个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

# 返回时光机器数据集的词元索引列表和词表（词元为单词），max_tokens用于截断
def load_corpus_time_machine(max_tokens=-1):
    lines = read_time_machine()
    tokens = tokenize(lines,'char')
    vocab = Vocab(tokens)
    
    # 因为时光机器数据集中的每个⽂本⾏不⼀定是⼀个句⼦或⼀个段落，
    # 所以将所有⽂本⾏展平到⼀个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab
~~~

注：vocab是词表，corpus是数字索引（原文每个单词映射后的数字）。

##### 2.语言模型

###### 2.1 简介

语言模型（Language Model, LM）是人工智能领域中专门处理自然语言理解与生成的核心技术，其核心功能是**对语言的概率分布进行建模**，即判断一段文本（句子、段落甚至篇章）在语言层面是否 “合理”，并能基于这种 “合理性” 生成符合语言规律的文本。

简单来说，语言模型的目标是：给定一段文本序列（如 “我今天想去”），预测下一个最可能出现的词（如 “公园”“吃饭” 等），或计算整个序列出现的概率。

例如，对于句子 “猫在__上睡觉”，语言模型能通过学习大量文本，判断 “沙发”“桌子”“屋顶” 等词的出现概率，并优先选择概率最高的词补全句子。

###### 2.2 分析

假设我们将使用神经网络来训练语言模型，模型中的网络⼀次处理具有预定义长度（例如*n*个时间步）的⼀个小批量序列，现在需要随机生成一个小批量数据的特征和标签以用来读取。

由于文本序列可以任意长，于是任意长的序列可以被我们划分为具有相同时间步数的子序列。当训练神经网络时，这样的小批量子序列将被输⼊到模型中。

假设网络⼀次只处理具有5个时间步的子序列，并且每个时间步的词元对应于⼀个字符，我们可以选择任意偏移量来指示初始位置，所以我们有相当大的自由度。

![](/images/NLP/1.png)

然而，如果只选择一个偏移量，用于训练网络的、所有可能的子序列的覆盖范围将是有限的。

因此，我们可以从随机偏移量开始划分序列，以同时获得覆盖性（coverage）和随机性（randomness)。

###### 2.3 随机采样（random sampling）

在随机采样中，**每个样本都是在原始的长序列上任意捕获的子序列**。在迭代过程中，来自两个相邻的、随机的、小批量中的子序列不一定在原始序列上相邻。对于语言建模，目标是基于到目前为止我们看到的词元来预测下一个词元，因此**标签是移位了一个词元的原始序列**。

~~~
# 随机采样
import random
import torch

# 使⽤随机抽样⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_random(corpus, batch_size, num_steps):
    # 从随机偏移量开始对序列进⾏分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # ⻓度为num_steps的⼦序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来⾃两个相邻的、随机的、⼩批量中的⼦序列不⼀定在原始序列上相邻
    random.shuffle(initial_indices)
    
    def data(pos):
        # 返回从pos位置开始的⻓度为num_steps的序列
        return corpus[pos: pos + num_steps]
        
    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这⾥，initial_indices包含⼦序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
~~~

下面我们生成一个从0到34的序列。时间步数为5，这意味着可以生成[（35一1)/5」=6个“特征一标签”子序列对，假设批量大小为2，最终只能得到3个小批量。

~~~
my_seq = list(range(35))
for X, Y in seq_data_iter_random(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
~~~

![](/images/NLP/3.png)

###### 2.4 顺序分区

在迭代过程中，除了对原始序列可以随机抽样外，我们还可以保证两个相邻的小批量中的子序列在原始序列上也是相邻的。这种策略在基于小批量的迭代过程中保留了拆分的子序列的顺序，因此称为顺序分区。

~~~
# 顺序分区
import random
import torch

# 使⽤顺序分区⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
~~~

基于相同的设置，通过顺序分区读取每个小批量的子序列的特征X和标签Y。通过将它们打印出来可以发现：迭代期间来自两个相邻的小批量中的子序列在原始序列中确实是相邻的。

~~~
my_seq = list(range(35))
for X, Y in seq_data_iter_sequential(my_seq, batch_size=2, num_steps=5):
    print('X: ', X, '\nY:', Y)
~~~

![](/images/NLP/4.png)

###### 2.5 封装及代码汇总

~~~
# 代码汇总
import random
import torch

# 使⽤随机抽样⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_random(corpus, batch_size, num_steps):
    # 从随机偏移量开始对序列进⾏分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # ⻓度为num_steps的⼦序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来⾃两个相邻的、随机的、⼩批量中的⼦序列不⼀定在原始序列上相邻
    random.shuffle(initial_indices)
    
    def data(pos):
        # 返回从pos位置开始的⻓度为num_steps的序列
        return corpus[pos: pos + num_steps]
        
    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这⾥，initial_indices包含⼦序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
        
# 使⽤顺序分区⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
        
# 加载序列数据的迭代器
class SeqDataLoader:
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = seq_data_iter_random
        else:
            self.data_iter_fn = seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps
    
    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
        
# 返回数据迭代器和词表
def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab
~~~

##### 3.搭建RNN网络

根据第一节和第二节共两段代码汇总，读取数据集。

~~~
import math
import torch
from torch import nn
from torch.nn import functional as F

# 批量大小，时间步
batch_size, num_steps = 32, 35
train_iter, vocab = load_data_time_machine(batch_size, num_steps)
~~~

###### 3.1 独热编码

独热编码（one-hot encoding）是一个向量，它的分量和类别一样多，类别对应的分量设置为1，其他所有位置为0。例如对于三个类别的“猫”、“狗”、“鸡”，分别对应(1,0,0)、(0,1,0)、(0,0,1)，则标签为：

$$ y \in \lbrace (1,0,0)，(0,1,0)，(0,0,1) \rbrace $$

在第一节中，每个词元都表示为一个数字索引，我们通常使用one_hot函数将这些词元表示为独热编码，即将每个索引映射为相互不同的单位向量。

我们每次采样的小批量数据形状是二维张量：（批量大小，时间步数），而one_hot函数会在末尾增加词表⼤⼩（len(vocab)）的维度，变为（批量大小，时间步数，词表大小）。

但是输出的维度通常是（时间步数，批量大小，词表大小），因为RNN是跟时间步关联，对时间步索引，内部剩下（批量大小，词表大小），上一时间步批量和当前时间步批量就可以很容易获得，所以需要转换维度。

~~~
# 批量为2，时间步为5
X = torch.arange(10).reshape((2, 5))
~~~

###### 3.2 初始化模型参数

在RNN中，隐藏单元数（单层神经元节点数）num_hiddens是一个可调的超参数，当训练语言模型时，输入和输出来自同一个词表，所以输入和输出的维度与词表大小一样。

~~~
# RNN模型初始化
def get_params(vocab_size, num_hiddens, device):
    # 输入和输出维度一样
    num_inputs = num_outputs = vocab_size
    
    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01
        
    # 隐藏层参数 W_xh * x_t + W_hh * h_t-1 + b_h
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # 输出层参数 W_hq * h_t + b_q
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        # 设置每个参数都需要更新梯度
        param.requires_grad_(True)
        
    return params
~~~

###### 3.3 RNN模型

首先需要一个函数在初始化时返回隐状态，即给初始隐状态一个值。初始隐状态为一个全0张量，形状为（批量大小，隐藏单元数）。

~~~
# 隐状态初始化
def init_rnn_state(batch_size, num_hiddens, device):
	return (torch.zeros((batch_size, num_hiddens), device=device), )
~~~

接着，需要定义如何在一个时间步计算隐状态和输出，相当于单个时间步的前向传播

~~~
# 计算隐状态和输出
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量⼤⼩，词表⼤⼩)
    W_xh, W_hh, b_h, W_hq, b_q = params
    # 初始隐状态
    H,  = state
    outputs = []
    # X的形状：(批量⼤⼩，词表⼤⼩)，对时间步按顺序循环
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)
~~~

现在，定义一个类来包装函数，并存储所有模型参数。

~~~
# 从零开始实现的循环神经⽹络模型
class RNNModelScratch:
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn
        
    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)
        
    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)
~~~

###### 3.4 预测

让我们首先定义预测函数来生成prefix之后的新字符，其中的prefix是一个用户提供的包含多个字符的字符串。在循环遍历prefix中的开始字符时，我们不断地将隐状态传递到下一个时间步，但是不生成任何输出。这被称为预热（warm-up）期，因为在此期间模型会自我更新（例如，更新隐状态），但不会进行预测。预热期结束后，隐状态的值通常比刚开始的初始值更适合预测，从而预测字符并输出它们。

~~~
# 预测新字符
# prefix：输入字符；num_preds：预测字符数；net：使用的网络模型；vocab：词表
def predict(prefix, num_preds, net, vocab, device):
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]: # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds): # 预测num_preds步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])
~~~

###### 3.6 梯度裁剪

梯度裁剪（Gradient Clipping）是一种解决梯度爆炸的手段，由于RNN很容易梯度爆炸，所以该步骤非常必要。一个常用的方法是**范数裁剪**（norm clipping），将梯度投影回给定半径$\theta $的球来裁剪梯度：

$$ g \gets min(1,\frac{\theta }{\vert \vert g \vert \vert })g $$

这种方法确保了裁剪后的梯度向量的范数不会超过预设的阀值$\theta $。如果原始梯度向量的范数小于或等于$\theta $，则梯度不会被裁剪，保持不变；如果原始梯度向量的范数大于$\theta $，则梯度会被按比例缩放，使其范数等于$\theta $。并且更新后的梯度完全与$g$的原始方向对齐。

~~~
# 梯度裁剪
def grad_clipping(net, theta):
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm
~~~

###### 3.7 信息论和熵

这里需要回顾一下熵的概念，以及一些信息论的相关知识。

3.7.1 信息

信息，泛指人类社会传播的一切内容，指音讯、消息、通信系统传输和处理的对象。**信息是不确定性的消除**，消除不确定性是指把不确定的事变成一件确定的事。

3.7.2 自信息

信息论的一个基本想法是一个不太可能发生的事件居然发生了，要比一个非常可能的事件发生，能提供更多的信息。消息说：“太阳从东边升起”，信息量是如此之少，以至于没有必要发送；但一条消息说：“太阳从西边升起”，信息量就很丰富。

我们想要通过这种基本想法来量化信息，特别是：

- 非常可能发生的事件信息量要比较少，并且极端的情况下，确保能够发生的事件应该没有信息量。
- 较不可能发生的事件具有更高的信息量。
- 独立事件应具有增量的信息。例如，投掷的硬币两次正面朝上传递的信息，应该是透支一次硬币正面朝上的信息量的两倍。

为了满足上述3个性质，我们定义单个事件“信息量”为，这也被称为自信息。

$$I(x)=−logP(x)$$

其中，以e为底的对数的单位是奈特(nats)，以2为底的对数的单位是**比特**(bit)或香农(shannons)。

3.7.3 信息熵

信息熵也叫香农熵，或者简称熵。

信息熵衡量的是一个概率分布的不确定性大小。对于遵循分布 P 的事件，每个事件 x 发生时会带来一定的 “信息量”，而 “期望信息总量” 就是所有可能事件的信息量，按其发生概率加权平均。

$$H(P)=-\sum_{x} P(x) \log P(x)$$

信息熵的一个核心意义是：对服从分布 P 的符号进行无损编码时，平均每个符号至少需要 H(P)比特。例如，如果熵是 2 比特，说明无论用什么编码方式，平均下来每个符号至少需要 2 比特才能准确表示，即编码的平均比特数不可能比熵更小。

对信息熵的另一种定义：最短平均编码长度。

###### 3.8 模型评估

任何一个模型都应该有一个评估好坏的度量方式，在语言模型中。一个好的语言模型能够用高度准确的词元来预测我们接下来会看到什么。

考虑一下由不同的语言模型给出的对“It s raining…”（“…下雨了”）的续写：

1. “It is raining outside”（外面下雨了）
2. “Itis raining banana tree”（香蕉树下雨了）
3. “It is raining piouw;kcj pwepoiut”(piouw;kcj pwepoiut下雨了)

肉眼观察可得，第一个语言模型输出的内容最合理、逻辑最连贯。第二个模型会写单词，但是逻辑不顺。最后一个模型连单词都不会写，说明无法正确拟合数据。

在3.6节中说过，比特（bit）是信息的基本单位，用来衡量“一个符号包含的信息量”。

较短的序列比较长的序列更有可能出现，所以需要压缩文本，而压缩文本意味着减少比特数。

语言模型的任务是根据 “已有的词元序列”，预测 “下一个词元” 的概率。如果语言模型 “更好”（预测更准确），意味着：对于序列中每个位置的词元，模型能更确定 “下一个词元应该是什么”。这种准确性决定了 “下一个词元的不确定性大小”。

预测越准，概率越高，不确定性越小，所需比特数越少，对文本也有压缩作用。NLP的科学家用困惑度（perplexity）衡量语言模型对文本序列的预测能力：

$$ exp(-\frac{1}{n}\sum_{t=1}^n logP(x_t \vert x_{t-1},\cdots,x_1)) $$

困惑度的最好的理解是“下一个词元的实际选择数的调和平均数”。我们看看一些案例：

- 在最好的情况下，模型总是完美地估计标签词元的概率为1。在这种情况下，模型的困惑度为1。
- 在最坏的情况下，模型总是预测标签词元的概率为0。在这种情况下，困惑度是正无穷大。
- 在基线上，该模型的预测是词表的所有可用词元上的均匀分布。在这种情况下，困惑度等于词表中唯一
  词元的数量。事实上，如果我们在没有任何压缩的情况下存储序列，这将是我们能做的最好的编码方
  式。因此，这种方式提供了一个重要的上限，而任何实际模型都必须超越这个上限。

###### 3.9 训练

定义优化器。

~~~
# 小批量随机梯度下降
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()
~~~

训练迭代周期。

~~~
import math

# 训练一个迭代周期
def train_epoch(net, train_iter, loss, updater, device, use_random_iter):
    state = None
    total_loss = 0.0  # 累计总损失
    total_tokens = 0  # 累计总词元数
    
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第⼀次迭代或使⽤随机抽样时初始化state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # state对于nn.GRU是个张量
                state.detach_()
            else:
                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # 因为已经调⽤了mean函数
            updater(batch_size=1)
            
        # 累计损失和词元数
        total_loss += l.item() * y.numel()
        total_tokens += y.numel()
    return math.exp(total_loss / total_tokens)
~~~

训练整个模型。

~~~
# 训练模型
def train(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    loss = nn.CrossEntropyLoss()
    perplexities = []  # 存储每次迭代的困惑度
    
    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: sgd(net.params, lr, batch_size)
    
    # 训练和预测
    for epoch in range(num_epochs):
        ppl = train_epoch(net, train_iter, loss, updater, device, use_random_iter)
        perplexities.append(ppl)  # 记录当前epoch的困惑度
        
        # 每50个周期打印一次中间结果
        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch + 1}/{num_epochs}], Perplexity: {ppl:.1f}")

    # 训练结束：输出最终结果
    print(f"Final Perplexity: {ppl:.1f} (on {device})")
    return perplexities
~~~

###### 3.10 代码汇总

~~~
import torch
import math
from torch import nn
from torch.nn import functional as F
import matplotlib.pyplot as plt

# RNN模型初始化
def get_params(vocab_size, num_hiddens, device):
    # 输入和输出维度一样
    num_inputs = num_outputs = vocab_size
    
    def normal(shape):
        return torch.randn(size=shape, device=device) * 0.01
        
    # 隐藏层参数 W_xh * x_t + W_hh * h_t-1 + b_h
    W_xh = normal((num_inputs, num_hiddens))
    W_hh = normal((num_hiddens, num_hiddens))
    b_h = torch.zeros(num_hiddens, device=device)
    # 输出层参数 W_hq * h_t + b_q
    W_hq = normal((num_hiddens, num_outputs))
    b_q = torch.zeros(num_outputs, device=device)
    # 附加梯度
    params = [W_xh, W_hh, b_h, W_hq, b_q]
    for param in params:
        # 设置每个参数都需要更新梯度
        param.requires_grad_(True)
        
    return params

# 隐状态初始化
def init_rnn_state(batch_size, num_hiddens, device):
	return (torch.zeros((batch_size, num_hiddens), device=device), )

# 计算隐状态和输出
def rnn(inputs, state, params):
    # inputs的形状：(时间步数量，批量⼤⼩，词表⼤⼩)
    W_xh, W_hh, b_h, W_hq, b_q = params
    # 初始隐状态
    H,  = state
    outputs = []
    # X的形状：(批量⼤⼩，词表⼤⼩)，对时间步按顺序循环
    for X in inputs:
        H = torch.tanh(torch.mm(X, W_xh) + torch.mm(H, W_hh) + b_h)
        Y = torch.mm(H, W_hq) + b_q
        outputs.append(Y)
    return torch.cat(outputs, dim=0), (H,)

# 从零开始实现的循环神经⽹络模型
class RNNModelScratch:
    def __init__(self, vocab_size, num_hiddens, device,
                 get_params, init_state, forward_fn):
        self.vocab_size, self.num_hiddens = vocab_size, num_hiddens
        self.params = get_params(vocab_size, num_hiddens, device)
        self.init_state, self.forward_fn = init_state, forward_fn
        
    def __call__(self, X, state):
        X = F.one_hot(X.T, self.vocab_size).type(torch.float32)
        return self.forward_fn(X, state, self.params)
        
    def begin_state(self, batch_size, device):
        return self.init_state(batch_size, self.num_hiddens, device)

# 预测新字符
# prefix：输入字符；num_preds：预测字符数；net：使用的网络模型；vocab：词表
def predict(prefix, num_preds, net, vocab, device):
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]: # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds): # 预测num_preds步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])

# 梯度裁剪
def grad_clipping(net, theta):
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm

# 小批量随机梯度下降
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

# 训练一个迭代周期
def train_epoch(net, train_iter, loss, updater, device, use_random_iter):
    state = None
    total_loss = 0.0  # 累计总损失
    total_tokens = 0  # 累计总词元数
    
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第⼀次迭代或使⽤随机抽样时初始化state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # state对于nn.GRU是个张量
                state.detach_()
            else:
                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # 因为已经调⽤了mean函数
            updater(batch_size=1)
            
        # 累计损失和词元数
        total_loss += l.item() * y.numel()
        total_tokens += y.numel()
    return math.exp(total_loss / total_tokens)
    
# 训练模型
def train(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    loss = nn.CrossEntropyLoss()
    perplexities = []  # 存储每次迭代的困惑度
    
    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: sgd(net.params, lr, batch_size)
    
    # 训练和预测
    for epoch in range(num_epochs):
        ppl = train_epoch(net, train_iter, loss, updater, device, use_random_iter)
        perplexities.append(ppl)  # 记录当前epoch的困惑度
        
        # 每50个周期打印一次中间结果
        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch + 1}/{num_epochs}], Perplexity: {ppl:.5f}")

    # 训练结束：输出最终结果
    print(f"Final Perplexity: {ppl:.5f} (on {device})")
    return perplexities
    
# 批量大小，时间步
batch_size, num_steps = 32, 35
# 迭代器和词表
train_iter, vocab = load_data_time_machine(batch_size, num_steps)

# 设置隐藏单元数
num_hiddens = 512
# 指定设备，优先GPU，没有GPU则用CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")

# 定义网络
net = RNNModelScratch(len(vocab), num_hiddens, device, get_params,
                      init_rnn_state, rnn)

# 迭代次数，学习率
num_epochs, lr = 500, 1
# 顺序分区训练，返回每次迭代的困惑度
perplexities = train(net, train_iter, vocab, lr, num_epochs, device)
# # 随机抽样训练
# perplexities = train(net, train_iter, vocab, lr, num_epochs, device, use_random_iter=True)

# 绘制困惑度随迭代次数下降的折线图
plt.plot(range(1, num_epochs + 1), perplexities, label='train')  # 横轴：epoch，纵轴：困惑度
plt.xlabel('Epoch')
plt.ylabel('Perplexity')
plt.legend()
plt.grid(linestyle='--', alpha=0.7)
plt.show()

# 用于预测下一个词元的序列
prefix = "time traveller"
# 预测50个字符，训练参数存放在net中
print(predict(prefix, 50, net, vocab, device))
~~~

第一张图为顺序分区训练结果，第二张图为随机抽样训练结果。

![](/images/NLP/5.png)

![](/images/NLP/6.png)

注：将三节代码汇总按顺序放在一起，即可完成这个案例。提供序列，这个语言模型将预测序列的后50个可能出现的字符。（对于时间数据集里的内容较准确）

##### 4.RNN简洁实现

这节使用内置的高级API完成训练，其实就只是不用自己定义隐藏层计算，也不用自己初始化了。

###### 4.1 定义模型

高级API提供了循环神经网络的实现，我们构造一个具有256个隐藏单元的单隐藏层的循环神经网络层rnn_layer。

~~~
num_hiddens = 256
rnn_layer = nn.RNN(len(vocab), num_hiddens)
~~~

使用张量来初始化隐状态，它的形状是（隐藏层数，批量大小，隐藏单元数）。

~~~
state = torch.zeros((1, batch_size, num_hiddens))
~~~

通过一个隐状态和一个输入，我们就可以用更新后的隐状态计算输出。需要强调的是，rnn_layer的“输出”（Y）不涉及输出层的计算：它是指每个时间步的隐状态，这些隐状态可以用作后续输出层的输入。

~~~
X = torch.rand(size=(num_steps, batch_size, len(vocab)))
Y, state_new = rnn_layer(X, state)
Y.shape, state_new.shape
~~~

为一个完整的RNN模型定义一个RNNModel类。注意，rnn_layer只包含隐藏的循环层，我们还需要创建一个单独的输出层。

~~~
# 循环神经⽹络模型
class RNNModel(nn.Module):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将Y的形状改为(时间步数*批量大⼩,隐藏单元数)
        # 它的输出形状是(时间步数*批量大⼩,词表大⼩)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return torch.zeros((self.num_directions * self.rnn.num_layers,
                                batch_size, self.num_hiddens), device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))
~~~

###### 4.2 完整代码

本小节完整代码不与前面关联，可直接运行。

~~~
# 代码汇总，高级API
import requests
import re
import collections
import random
import torch
import math
from torch import nn
from torch.nn import functional as F
import matplotlib.pyplot as plt

# # 获取数据
# url = "https://www.gutenberg.org/cache/epub/35/pg35.txt" 
# response = requests.get(url)
# with open('TimeMachine.txt', 'w', encoding='utf-8') as f:
#     f.write(response.text)

# 数据清洗
def read_time_machine():
    with open('TimeMachine.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 对每行进行清洗
    # 1.re.sub：去除非字母字符
    # 2.strip：去除字符串首尾的空白字符
    # 3.lower：所有大写字母转为小写字母
    cleaned_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    # 对处理后的列表，删除里面的空字符串
    return [line for line in cleaned_lines if line]

# 词元化
def tokenize(lines, token='word'):
    # 将⽂本⾏拆分为单词或字符词元
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)
        
# 构建词表
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                
    def __len__(self):
        return len(self.idx_to_token)
        
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]
        
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
        
    @property
    def unk(self): # 未知词元的索引为0
        return 0
        
    @property
    def token_freqs(self):
        return self._token_freqs
            
# 统计词元的频率
def count_corpus(tokens):
    # 这⾥的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成⼀个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

# 返回时光机器数据集的词元索引列表和词表（词元为单词），max_tokens用于截断
def load_corpus_time_machine(max_tokens=-1):
    lines = read_time_machine()
    tokens = tokenize(lines,'char')
    vocab = Vocab(tokens)
    
    # 因为时光机器数据集中的每个⽂本⾏不⼀定是⼀个句⼦或⼀个段落，
    # 所以将所有⽂本⾏展平到⼀个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

# 使⽤随机抽样⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_random(corpus, batch_size, num_steps):
    # 从随机偏移量开始对序列进⾏分区，随机范围包括num_steps-1
    corpus = corpus[random.randint(0, num_steps - 1):]
    # 减去1，是因为我们需要考虑标签
    num_subseqs = (len(corpus) - 1) // num_steps
    # ⻓度为num_steps的⼦序列的起始索引
    initial_indices = list(range(0, num_subseqs * num_steps, num_steps))
    # 在随机抽样的迭代过程中，
    # 来⾃两个相邻的、随机的、⼩批量中的⼦序列不⼀定在原始序列上相邻
    random.shuffle(initial_indices)
    
    def data(pos):
        # 返回从pos位置开始的⻓度为num_steps的序列
        return corpus[pos: pos + num_steps]
        
    num_batches = num_subseqs // batch_size
    for i in range(0, batch_size * num_batches, batch_size):
        # 在这⾥，initial_indices包含⼦序列的随机起始索引
        initial_indices_per_batch = initial_indices[i: i + batch_size]
        X = [data(j) for j in initial_indices_per_batch]
        Y = [data(j + 1) for j in initial_indices_per_batch]
        yield torch.tensor(X), torch.tensor(Y)
        
# 使⽤顺序分区⽣成⼀个⼩批量⼦序列
# corpus是序列列表，batch_size是批量大小，num_steps是时间步数
def seq_data_iter_sequential(corpus, batch_size, num_steps):
    # 从随机偏移量开始划分序列
    offset = random.randint(0, num_steps)
    num_tokens = ((len(corpus) - offset - 1) // batch_size) * batch_size
    Xs = torch.tensor(corpus[offset: offset + num_tokens])
    Ys = torch.tensor(corpus[offset + 1: offset + 1 + num_tokens])
    Xs, Ys = Xs.reshape(batch_size, -1), Ys.reshape(batch_size, -1)
    num_batches = Xs.shape[1] // num_steps
    for i in range(0, num_steps * num_batches, num_steps):
        X = Xs[:, i: i + num_steps]
        Y = Ys[:, i: i + num_steps]
        yield X, Y
        
# 加载序列数据的迭代器
class SeqDataLoader:
    def __init__(self, batch_size, num_steps, use_random_iter, max_tokens):
        if use_random_iter:
            self.data_iter_fn = seq_data_iter_random
        else:
            self.data_iter_fn = seq_data_iter_sequential
        self.corpus, self.vocab = load_corpus_time_machine(max_tokens)
        self.batch_size, self.num_steps = batch_size, num_steps
    
    def __iter__(self):
        return self.data_iter_fn(self.corpus, self.batch_size, self.num_steps)
        
# 返回数据迭代器和词表
def load_data_time_machine(batch_size, num_steps, use_random_iter=False, max_tokens=10000):
    data_iter = SeqDataLoader(batch_size, num_steps, use_random_iter, max_tokens)
    return data_iter, data_iter.vocab

# 循环神经⽹络模型
class RNNModel(nn.Module):
    def __init__(self, rnn_layer, vocab_size, **kwargs):
        super(RNNModel, self).__init__(**kwargs)
        self.rnn = rnn_layer
        self.vocab_size = vocab_size
        self.num_hiddens = self.rnn.hidden_size
        # 如果RNN是双向的（之后将介绍），num_directions应该是2，否则应该是1
        if not self.rnn.bidirectional:
            self.num_directions = 1
            self.linear = nn.Linear(self.num_hiddens, self.vocab_size)
        else:
            self.num_directions = 2
            self.linear = nn.Linear(self.num_hiddens * 2, self.vocab_size)

    def forward(self, inputs, state):
        X = F.one_hot(inputs.T.long(), self.vocab_size)
        X = X.to(torch.float32)
        Y, state = self.rnn(X, state)
        # 全连接层首先将Y的形状改为(时间步数*批量大⼩,隐藏单元数)
        # 它的输出形状是(时间步数*批量大⼩,词表大⼩)。
        output = self.linear(Y.reshape((-1, Y.shape[-1])))
        return output, state

    def begin_state(self, device, batch_size=1):
        if not isinstance(self.rnn, nn.LSTM):
            # nn.GRU以张量作为隐状态
            return torch.zeros((self.num_directions * self.rnn.num_layers,
                                batch_size, self.num_hiddens), device=device)
        else:
            # nn.LSTM以元组作为隐状态
            return (torch.zeros((
                self.num_directions * self.rnn.num_layers,
                batch_size, self.num_hiddens), device=device),
                    torch.zeros((
                        self.num_directions * self.rnn.num_layers,
                        batch_size, self.num_hiddens), device=device))

# 预测新字符
# prefix：输入字符；num_preds：预测字符数；net：使用的网络模型；vocab：词表
def predict(prefix, num_preds, net, vocab, device):
    state = net.begin_state(batch_size=1, device=device)
    outputs = [vocab[prefix[0]]]
    get_input = lambda: torch.tensor([outputs[-1]], device=device).reshape((1, 1))
    for y in prefix[1:]: # 预热期
        _, state = net(get_input(), state)
        outputs.append(vocab[y])
    for _ in range(num_preds): # 预测num_preds步
        y, state = net(get_input(), state)
        outputs.append(int(y.argmax(dim=1).reshape(1)))
    return ''.join([vocab.idx_to_token[i] for i in outputs])

# 梯度裁剪
def grad_clipping(net, theta):
    if isinstance(net, nn.Module):
        params = [p for p in net.parameters() if p.requires_grad]
    else:
        params = net.params
    norm = torch.sqrt(sum(torch.sum((p.grad ** 2)) for p in params))
    if norm > theta:
        for param in params:
            param.grad[:] *= theta / norm

# 小批量随机梯度下降
def sgd(params, lr, batch_size):
    with torch.no_grad():
        for param in params:
            param -= lr * param.grad / batch_size
            param.grad.zero_()

# 训练一个迭代周期
def train_epoch(net, train_iter, loss, updater, device, use_random_iter):
    state = None
    total_loss = 0.0  # 累计总损失
    total_tokens = 0  # 累计总词元数
    
    for X, Y in train_iter:
        if state is None or use_random_iter:
            # 在第⼀次迭代或使⽤随机抽样时初始化state
            state = net.begin_state(batch_size=X.shape[0], device=device)
        else:
            if isinstance(net, nn.Module) and not isinstance(state, tuple):
                # state对于nn.GRU是个张量
                state.detach_()
            else:
                # state对于nn.LSTM或对于我们从零开始实现的模型是个张量
                for s in state:
                    s.detach_()
        y = Y.T.reshape(-1)
        X, y = X.to(device), y.to(device)
        y_hat, state = net(X, state)
        l = loss(y_hat, y.long()).mean()
        if isinstance(updater, torch.optim.Optimizer):
            updater.zero_grad()
            l.backward()
            grad_clipping(net, 1)
            updater.step()
        else:
            l.backward()
            grad_clipping(net, 1)
            # 因为已经调⽤了mean函数
            updater(batch_size=1)
            
        # 累计损失和词元数
        total_loss += l.item() * y.numel()
        total_tokens += y.numel()
    return math.exp(total_loss / total_tokens)
    
# 训练模型
def train(net, train_iter, vocab, lr, num_epochs, device,
              use_random_iter=False):
    loss = nn.CrossEntropyLoss()
    perplexities = []  # 存储每次迭代的困惑度
    
    # 初始化
    if isinstance(net, nn.Module):
        updater = torch.optim.SGD(net.parameters(), lr)
    else:
        updater = lambda batch_size: sgd(net.params, lr, batch_size)
    
    # 训练和预测
    for epoch in range(num_epochs):
        ppl = train_epoch(net, train_iter, loss, updater, device, use_random_iter)
        perplexities.append(ppl)  # 记录当前epoch的困惑度
        
        # 每50个周期打印一次中间结果
        if (epoch + 1) % 50 == 0:
            print(f"Epoch [{epoch + 1}/{num_epochs}], Perplexity: {ppl:.5f}")

    # 训练结束：输出最终结果
    print(f"Final Perplexity: {ppl:.5f} (on {device})")
    return perplexities
    
batch_size, num_steps = 32, 35
train_iter, vocab = load_data_time_machine(batch_size, num_steps)

# 隐藏节点数
num_hiddens = 256

# 单隐藏层的循环神经网络层
rnn_layer = nn.RNN(len(vocab), num_hiddens)

# 隐状态初始化
state = torch.zeros((1, batch_size, num_hiddens))

# 指定设备，优先GPU，没有GPU则用CPU
device = torch.device("cuda" if torch.cuda.is_available() else "cpu")
net = RNNModel(rnn_layer, vocab_size=len(vocab))
net = net.to(device)

num_epochs, lr = 500, 1

# 随机抽样训练
perplexities = train(net, train_iter, vocab, lr, num_epochs, device)

# 绘制困惑度随迭代次数下降的折线图
plt.plot(range(1, num_epochs + 1), perplexities, label='train')  # 横轴：epoch，纵轴：困惑度
plt.xlabel('Epoch')
plt.ylabel('Perplexity')
plt.legend()
plt.grid(linestyle='--', alpha=0.7)
plt.show()

# 用于预测下一个词元的序列
prefix = "time traveller"
# 预测50个字符，训练参数存放在net中
print(predict(prefix, 50, net, vocab, device))
~~~

![](/images/NLP/7.png)
