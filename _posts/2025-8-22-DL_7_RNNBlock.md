---
title: 深度学习（7）GRU和LSTM
tags: DL RNN
typora-root-url: ./..
---

循环神经网络的改进。

<!--more-->

##### 1. RNN的局限

循环神经网络在学习过程中的主要问题是由于梯度消失或爆炸问题，很难建模长时间间隔（Long Range）的状态之间的依赖关系，这个问题也被成为长期依赖问题。

###### 1.1 字符处理

首先先了解一下RNN在文本数据处理中是在做什么事，假设我们有一个文本是machine，输入后得到如图的循环神经网络。

每个时间步对应输入序列的一个字符，从输入到隐藏层再到输出层，我们需要预测下一个时间步的字符是什么，所以每个时间步的标签是下一个时间步的输入。在训练过程中，我们对每个时间步的输出层的输出进⾏softmax操作，然后利⽤交叉熵损失计算模型输出和标签之间的误差。

![](/images/RNNBlock/1.png)

在上图中，第3个时间步的输出$O_3$由前面的输入序列"m"、"a"和当前输入"c"共同决定，损失取决于下一个字符的概率分布。所以，下一个实际输出的字符，是基于输入序列"m"、"a"、"c"和这个时间步的标签"h"共同决定。

###### 1.2 单词处理

对于单词的预测是类似的情况，假设我们想预测“the clouds are in the sky”的最后这个词“sky”，这是一件比较容易的事，因为云大概率是在天上。

![](/images/RNNBlock/2.png)

在这样的场景中，相关的信息“clouds”、“in”和预测的词“sky”，位置之间的间隔是非常小的，RNN可以学会使用先前的信息。

但是，如果我们要去预测一句非常长的话的最后一个词，例如，“I grew up in France......I speak fluent French”中的“French”，中间省略一堆无用信息。当前的信息“I speak fluent”建议下一个词可能是一种语言的名字，但是想要知道是什么语言，则需要横跨很远的距离找到“France”的上下文，随着这个距离不断增大，会出现梯度消失和梯度爆炸的问题，RNN无法学习到有效的信息。

也就是说，如果一条序列足够长，那它们将很难将信息从较早的时间步传送到后面的时间步，RNN会忘记它在较长序列中以前看到的内容，因此RNN只具有**短时记忆**。

##### 2.LSTM

论文：Long short-term memory.

长短期记忆网络（long short-term memory，LSTM），是最早解决该问题的方法，可以学习长期依赖信息。⻓短期记忆⽹络引⼊了记忆元（memory cell），或简称为单元（cell），用于记忆重要信息，忘记不重要信息（学习只保留相关信息来进行预测，并忘记不相关的数据）。

举个例子，点外卖或者网购时，通常会看已购买商品的用户评价，当我们浏览这些评论的时候，只会记住重要的关键词，例如，“好吃”、“好用”、“不错”、“垃圾”。而不会去记住“我”、“的”、“提供”这种词，如果向其他人推荐，也只会形容“好”或者“坏”，而不是一字不差地把评论说一遍。

先回忆一下RNN的结构，当前隐状态$h_t$的值由当前输入、上一隐状态$h_{t-1}$和激活函数决定，在标准的RNN中，激活函数使用tanh函数，用于帮助调节流经网络的值，使得数值始终限制在 -1 和 1 之间。

![](/images/RNNBlock/3.png)

而LSTM包含了四个交互层，三个Sigmoid和一个tanh，并以一种特殊的方式连接。

![](/images/RNNBlock/4.png)

上图中，$\sigma$表示的是Sigmoid激活函数，作用是把值压缩到0~1 之间。如果压缩到0，那么这部分信息就会被扔掉，达到了忘记信息的目的；如果压缩到1，那么这部分信息就会被保留，达到记忆信息的作用。所以Sigmoid激活函数的设置，有助于帮助更新或者忘记信息。原则：因为记忆能力有限，记住重要的，忘记无关紧要的。

接下来，详细拆解这个模型。

###### 2.1 遗忘门、输入门、输出门

![](/images/RNNBlock/5.png)

先给出图中三个门的计算公式：

$$ F_t = \sigma( \mathbf{X}_t \cdot \mathbf{W}_{xf} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{hf} + \mathbf{b}_f) $$

$$ I_t = \sigma( \mathbf{X}_t \cdot \mathbf{W}_{xi} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{hi} + \mathbf{b}_i) $$

$$ O_t = \sigma( \mathbf{X}_t \cdot \mathbf{W}_{xo} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{ho} + \mathbf{b}_o) $$

这三个公式除了符号不一样以外，形式上完全一样，6个W都是权重参数，3个b都是偏置参数。

此外，公式与标准RNN的唯一区别，就是激活函数换成了Sigmoid函数，这三个门的值都在(0, 1)的范围内。

注：虽然公式形式一样，由于权重和偏置的参数不一样，三个门的取值不是完全相同的。

###### 2.2 候选记忆元

三个门的具体作用在后续再揭晓，这里先介绍候选记忆（candidate memory cell），它与标准RNN计算方式一致，使用tanh激活函数。

$$ \tilde{C}_t = tanh( \mathbf{X}_t \cdot \mathbf{W}_{xc} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{hc} + \mathbf{b}_c) $$

候选记忆可以理解为标准RNN在当前时间步下的隐状态，这个隐状态保留了过去和现在的所有信息。

![](/images/RNNBlock/6.png)

###### 2.3 记忆元

记忆元是一个携带了信息的向量，初始时为零，在传递的过程中不断汇集重要信息，存储了需要长期保留的信息。

![](/images/RNNBlock/7.png)

记忆元由两部分决定，一部分是过去的记忆元，一部分是当前候选的记忆元。

$$ C_t = \mathbf{F}_t \odot \mathbf{C}_{t-1} + \mathbf{I}_t \odot \tilde{C}_t $$

这里重新回到前面讲述的遗忘门、输入门和候选记忆。遗忘门$F_t$控制保留多少过去的记忆元$C_{t-1}$，输入门$I_t$控制吸收多少新的数据信息$\tilde{C}_t$，

因为三个门的值都在(0, 1)的范围，如果遗忘门接近1，则过去的记忆元需要保留的就多，如果遗忘门接近0，则过去的记忆元需要保留的就少。

同理，如果输入门接近1，那么候选记忆就保留的多，如果输入门接近0，那么候选记忆就保留的少。

将想要保留的过去记忆和候选记忆相结合，就得到了当前时间步的记忆元。

注：$\odot$表示按元素乘积。


###### 2.4 隐状态

最后，我们还需要计算当前时间步的隐状态。这就是输出门发挥的作用了。

$$ \mathbf{H}_t = \mathbf{O}_t \odot tanh(\mathbf{C}_t) $$

tanh激活函数的目的是为了压缩记忆元信息，将值输出在区间(*−*1*,* 1)内。输出门接近1时，则当前隐状态保留所有记忆信息，输出门接近0时，只保留记忆元内的信息，隐状态不做更新。

![](/images/RNNBlock/8.png)

至此，LSTM的所有细节都讲完了。

##### 3.GRU

论文：On the properties of neural machine translation: encoder-decoder approaches. 

LSTM出现后，针对该网络架构的模型层出不穷，其中比较出名有门控循环单元（gated recurrent units，GRU）。这个模型相比于LSTM，更加简洁。

###### 3.1 重置门、更新门

与LSTM一样，GRU也有两个门：重置门（reset gate）和更新门（update gate）。

- 重置门允许我们控制“可能还想记住”的过去状态的数量；
- 更新门将允许我们控制新状态中有多少个是旧状态的副本。

![](/images/RNNBlock/9.png)

重置门和更新门公式如下：

$$ R_t = \sigma( \mathbf{X}_t \cdot \mathbf{W}_{xr} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{hr} + \mathbf{b}_r) $$

$$ Z_t = \sigma( \mathbf{X}_t \cdot \mathbf{W}_{xz} + \mathbf{H}_{t-1} \cdot \mathbf{W}_{hz} + \mathbf{b}_z) $$

与LSTM的三门计算方法一样，不再赘述。

###### 3.2 候选隐状态

将重置门与常规隐状态更新机制集成，得到当前时间步的候选隐状态。

$$ \tilde{\mathbf{H}}_t = tanh( \mathbf{X}_t \cdot \mathbf{W}_{xh} + (  \mathbf{R}_t \odot \mathbf{H}_{t-1} ) \cdot \mathbf{W}_{hh} + \mathbf{b}_h) $$

$R_t$的作用和LSTM中遗忘门的作用类似，用于选择保留多少上一时间步的隐状态的信息。之后再使用一个普通的RNN操作，得到候选隐状态。

也就是说，GRU将LSTM的遗忘门和输入门合并为一个重置门，同时不再设置记忆元，直接对上一时间步的隐状态进行操作。

![](/images/RNNBlock/10.png)

###### 3.3 隐状态

最后，GRU的更新门与LSTM的输出门作用类似，用于计算当前时间步的隐状态。

$$ \mathbf{H}_t = \mathbf{Z}_t \odot \mathbf{H}_{t-1} + (1 - \mathbf{Z}_t ) \odot \tilde{\mathbf{H}}_t $$

当更新门接近1时，模型倾向于保留旧状态；当更新门接近0时，模型倾向于保留候选状态。

![](/images/RNNBlock/11.png)

到此为止，GRU的拆解也就结束了。

