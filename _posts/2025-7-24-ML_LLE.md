---
layout: atricle
title: 机器学习笔记-局部线性嵌入LLE
tags: ML
---



# 局部线性嵌入——Locally Linear Embedding，LLE

原论文：Nonlinear Dimensionality Reduction by Locally Linear Embedding

参考博客:[LLE](https://www.cnblogs.com/pinard/p/6266408.html)

思想背景：LLE是一种能够将高维输入转换为低维邻域保持嵌入的无监督学习方法，LLE要求每一个数据点都可以由其近邻点的线性加权组合构造得到，并且使得降维后的数据也能基本保持原有流形结构。LLE将输入数据映射到单一全局低维坐标系，且其优化过程不涉及局部极小值问题。通过利用线性重构的局部对称性，LLE通过局部线性拟合来恢复全局非线性结构。

推导：
1. 为什么需要确定k个近邻点？

假设数据由N个D维实值向量$x_i$组成，这些向量采样自某个底层流形。若有足够数据（即流形被充分采样），我们期望每个数据点及其邻近点都位于或接近流形的局部线性区块。主要目的是降低计算复杂度。

一般通过欧氏距离或归一化点积选择k个邻居来构建局部关系。

2. 怎么构建局部关系？

我们通过线性系数来刻画这些区块的局部几何特征，这些系数能用邻近点重建原始数据点。

也就是说我们需要找到$x_i$和其k个最近邻之间的线性关系，这显然是一个回归问题。假设我们有m个n维样本$ \{x_1,x_2,...,x_m\} $，我们可以用均方差作为回归问题的损失函数：即：

$$ J(w)=\sum_{i=1}^{m}\left\|x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} $$

其中，$Q(i)$表示$x_i$的k个近邻样本集合，$w_{ij}$是重建权值。

此外，我们还需要两个约束条件：

- 对权重系数$w_{ij}$做归一化的限制，确保每个最近邻对预测的贡献是平衡的，即权重系数需要满足

$$\sum_{j \in Q(i)} w_{i j}=1$$

- 对于不在样本$x_i$邻域内的样本$x_j$，我们令对应的$w_{ij}=0$，这样可以把 w 扩展到整个数据集的维度。

3. 求解重建权值

对于原损失函数，先将其矩阵化，这里注意上面给的约束条件$\sum_{j \in Q(i)} w_{i j}=1$

$$\begin{aligned}
J(W) & =\sum_{i=1}^{m}\left\|x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j} x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j}\left(x_{i}-x_{j}\right)\right\|_{2}^{2} \\
& =\sum_{i=1}^{m} W_{i}^{T}\left(x_{i}-x_{j}\right)\left(x_{i}-x_{j}\right)^{T} W_{i}
\end{aligned}$$

其中 $ W_i = (w_{i1},w_{i2},...,w_{ik})^T$。

令矩阵  $Z_{i}=\left(x_{i}-x_{j}\right)\left(x_{i}-x_{j}\right)^{T}$, $j \in Q(i) $ ，则
$$J(W)=\sum_{i=1}^{m} W_{i}^{T} Z_{i} W_{i} $$ 

对于约束条件$\sum_{j \in Q(i)} w_{i j}=1$，可以矩阵化为：

$$\sum_{j \in Q(i)} w_{i j}=W_{i}^{T} 1_{k}=1$$

其中$  1_{k}  $为 k 维全 1 向量。

将矩阵化的两个式子用拉格朗日子乘法合为一个优化目标：

$$L(W)=\sum_{i=1}^{k} W_{i}^{T} Z_{i} W_{i}+\lambda\left(W_{i}^{T} 1_{k}-1\right)$$


对  W  求导并令其值为 0 ，我们得到

$$2 Z_{i} W_{i}+\lambda 1_{k}=0$$


即我们的

$$W_{i}=\lambda^{\prime} Z_{i}^{-1} 1_{k}$$


其中 $ \lambda^{\prime}=-\frac{1}{2} \lambda $ 为一个常数。

由于$\lambda^{\prime}$是一个超参数，我们可以利用$ W_{i}^{T} 1_{k}=1$ 将其消去

$$ 1_{k}^{T} w_{i}=\lambda^{\prime} 1_{k}^{T} Z_{i}^{-1} 1_{k} = 1$$
$$ \lambda^{\prime} = \frac{1}{1_{k}^{T} Z_{i}^{-1} 1_{k}}$$

最终权重系数 $ W_{i} $ 为：

$$W_{i}=\frac{Z_{i}^{-1} 1_{k}}{1_{k}^{T} Z_{i}^{-1} 1_{k}}$$

4. 将高维映射到低维

假设数据位于或邻近一个低维（d≪D）平滑非线性流形。在此近似条件下，存在由平移、旋转和缩放组成的线性映射，可将每个邻域的高维坐标映射到流形的全局内部坐标系。而重建权重$W_ij$反映了数据的内在几何特性，这些特性对上述变换具有不变性。因此，我们预期这些权重对原始数据空间局部几何的表征，同样适用于流形上的局部区块。即在D维空间中重建第i个数据点的权重$W_ij$，也应当能在d维空间中重建其嵌入流形坐标。

每个高维观测值$x_i$被映射为代表流形全局内部坐标的低维向量$y_i$，这是通过选择d维坐标$y_i$以最小化嵌入代价函数实现

$$J(w)=\sum_{i=1}^{m}\left\|y_{i}-\sum_{j \in Q(i)} w_{i j} y_{j}\right\|_{2}^{2}$$

这个损失函数与高维损失函数相似，高维中已知高维数据求权重，低维中已知权重求低维数据。

此外，我们想得到以原点为中心的有序正交坐标，这里类似PCA和LDA，增加约束条件

- 中心化，均值$\mu=0$，使得数据以原点为中心
$$ \frac{1}{m} \sum_{i=1}^{m} y_i = 0 $$

- 归一化，协方差矩阵为单位矩阵，使得坐标正交

$$ \frac{1}{m} \sum_{i=1}^{m} (y_i - \mu) (y_i - \mu)^T = I $$

化简上述两个式子，可得约束
$$ \sum_{i=1}^{m} y_i = 0 $$
$$ \frac{1}{m} \sum_{i=1}^{m} y_i y_i^T = I $$

5. 求解低维坐标

先给出下列结论：
- $||\mathbf{A}||^2 = tr(A^TA)$
- $ tr(A^T) = tr(A)$
- $ tr(A+B) = tr(A)+tr(B)$
- $ tr(AB) = tr(BA)$
- $ tr(ABC) = tr(BCA) = tr(CAB)$

将第四步的损失函数矩阵化

$$\begin{aligned}
J(w) & =\sum_{i=1}^{m}\left\|y_{i}-\sum_{j \in Q(i)} w_{i j} y_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|Y I_i - Y W_{i}\right\|_{2}^{2} \\
& =tr(Y(I-W)(I-W)^T Y^T)
\end{aligned}$$

令$ M = (I-W)(I-W)^T $ ，将归一化约束矩阵化得$ Y Y^T = mI $

将矩阵化的两个式子用拉格朗日子乘法合为一个优化目标：

$$L(W)=tr(Y M Y^T)+\lambda (Y Y^T-mI )$$

对Y求偏导并让其为0

$$ M Y^T = \lambda^{\prime} Y^T $$

代入原式，只与$\lambda$有关，要求出矩阵M最小的d个特征值所对应的d个特征向量组成的矩阵即可得到低维坐标

6. 为什么最小特征值为0？

由于M的最小特征值为0不能反应数据特征，此时对应的特征向量为全1，我们通常选择M的第2个到第d+1个最小的特征值对应的特征向量。

由$W^T·e = e $，得$ |W^T - I|·e = 0 $

因为e不等于0，所以$ W^T - I = 0 $，即$ (I - W)^T  = 0 $

两边乘$ (I - W)$，则$ (I - W) (I - W)^T e = 0e $，也就是$ M e = 0e $，最小特征值为0。

最后总结，局部线性嵌入基本步骤：
1. 寻找每个样本点的k个近邻点，可以使用欧氏距离；
2. 由每个样本点的近邻点计算出该样本点的局部重建权值矩阵$W_{i}=\frac{Z_{i}^{-1} 1_{k}}{1_{k}^{T} Z_{i}^{-1} 1_{k}}$；
3. 根据权重$W_{i}$组成权重系数矩阵$ M = (I-W)(I-W)^T $；
4. 计算矩阵M的前d+1个特征值，并计算这d+1个特征值对应的特征向量；
5. 由第二个特征向量到第d+1个特征向量所张成的矩阵即为输出低维样本集矩阵。

LLE算法的主要优点有：

1. 可以学习任意维的局部线性的低维流形

2. 算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。

LLE算法的主要缺点有：

1. 算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。

2. 算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。
   

LLE在有些情况下也并不适用，例如数据分布在整个封闭的球面上，LLE则不能将它映射到二维空间，且不能保持原有的数据流形。因此在处理数据的时候，需要确保数据不是分布在闭合的球面或者椭圆面上。