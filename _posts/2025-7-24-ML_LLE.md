---
title: 机器学习（x）局部线性嵌入LLE
tags: ML Dimensionality_Reduction
---

### 局部线性嵌入——Locally Linear Embedding，LLE

原论文：Nonlinear Dimensionality Reduction by Locally Linear Embedding

参考博客:[LLE](https://www.cnblogs.com/pinard/p/6266408.html)

思想背景：LLE是一种能够将高维输入转换为低维邻域保持嵌入的无监督学习方法，LLE要求每一个数据点都可以由其近邻点的线性加权组合构造得到，并且使得降维后的数据也能基本保持原有流形结构。LLE将输入数据映射到单一全局低维坐标系，且其优化过程不涉及局部极小值问题。通过利用线性重构的局部对称性，LLE通过局部线性拟合来恢复全局非线性结构。

<!--more-->

推导：
1. 为什么需要确定k个近邻点？

假设数据由N个D维实值向量$x_i$组成，这些向量采样自某个底层流形。若有足够数据（即流形被充分采样），我们期望每个数据点及其邻近点都位于或接近流形的局部线性区块。主要目的是降低计算复杂度。

一般通过欧氏距离或归一化点积选择k个邻居来构建局部关系。

2. 怎么构建局部关系？

我们通过线性系数来刻画这些区块的局部几何特征，这些系数能用邻近点重建原始数据点。

也就是说我们需要找到$x_i$和其k个最近邻之间的线性关系，这显然是一个回归问题。假设我们有m个n维样本$ \{x_1,x_2,...,x_m\} $，我们可以用均方差作为回归问题的损失函数：即：

$$ J(w)=\sum_{i=1}^{m}\left\|x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} $$

其中，$Q(i)$表示$x_i$的k个近邻样本集合，$w_{ij}$是重建权值。

此外，我们还需要两个约束条件：

- 对权重系数$w_{ij}$做归一化的限制，确保每个最近邻对预测的贡献是平衡的，即权重系数需要满足

$$\sum_{j \in Q(i)} w_{i j}=1$$

- 对于不在样本$x_i$邻域内的样本$x_j$，我们令对应的$w_{ij}=0$，这样可以把 w 扩展到整个数据集的维度。

3. 求解重建权值

对于原损失函数，先将其矩阵化，这里注意上面给的约束条件$\sum_{j \in Q(i)} w_{i j}=1$

$$\begin{aligned}
J(W) & =\sum_{i=1}^{m}\left\|x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j} x_{i}-\sum_{j \in Q(i)} w_{i j} x_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|\sum_{j \in Q(i)} w_{i j}\left(x_{i}-x_{j}\right)\right\|_{2}^{2} \\
& =\sum_{i=1}^{m} W_{i}^{T}\left(x_{i}-x_{j}\right)\left(x_{i}-x_{j}\right)^{T} W_{i}
\end{aligned}$$

其中 $ W_i = (w_{i1},w_{i2},...,w_{ik})^T$。

令矩阵  $Z_{i}=\left(x_{i}-x_{j}\right)\left(x_{i}-x_{j}\right)^{T}$, $j \in Q(i) $ ，则
$$J(W)=\sum_{i=1}^{m} W_{i}^{T} Z_{i} W_{i} $$ 

对于约束条件$\sum_{j \in Q(i)} w_{i j}=1$，可以矩阵化为：

$$\sum_{j \in Q(i)} w_{i j}=W_{i}^{T} 1_{k}=1$$

其中$  1_{k}  $为 k 维全 1 向量。

将矩阵化的两个式子用拉格朗日子乘法合为一个优化目标：

$$L(W)=\sum_{i=1}^{k} W_{i}^{T} Z_{i} W_{i}+\lambda\left(W_{i}^{T} 1_{k}-1\right)$$


对  W  求导并令其值为 0 ，我们得到

$$2 Z_{i} W_{i}+\lambda 1_{k}=0$$


即我们的

$$W_{i}=\lambda^{\prime} Z_{i}^{-1} 1_{k}$$


其中 $ \lambda^{\prime}=-\frac{1}{2} \lambda $ 为一个常数。

由于$\lambda^{\prime}$是一个超参数，我们可以利用$ W_{i}^{T} 1_{k}=1$ 将其消去



$$ 1_{k}^{T} w_{i}=\lambda^{\prime} 1_{k}^{T} Z_{i}^{-1} 1_{k} = 1$$



$$ \lambda^{\prime} = \frac{1}{1_{k}^{T} Z_{i}^{-1} 1_{k}}$$



最终权重系数 $ W_{i} $ 为：

$$W_{i}=\frac{Z_{i}^{-1} 1_{k}}{1_{k}^{T} Z_{i}^{-1} 1_{k}}$$

4. 将高维映射到低维

假设数据位于或邻近一个低维（d≪D）平滑非线性流形。在此近似条件下，存在由平移、旋转和缩放组成的线性映射，可将每个邻域的高维坐标映射到流形的全局内部坐标系。而重建权重$W_ij$反映了数据的内在几何特性，这些特性对上述变换具有不变性。因此，我们预期这些权重对原始数据空间局部几何的表征，同样适用于流形上的局部区块。即在D维空间中重建第i个数据点的权重$W_ij$，也应当能在d维空间中重建其嵌入流形坐标。

每个高维观测值$x_i$被映射为代表流形全局内部坐标的低维向量$y_i$，这是通过选择d维坐标$y_i$以最小化嵌入代价函数实现

$$J(w)=\sum_{i=1}^{m}\left\|y_{i}-\sum_{j \in Q(i)} w_{i j} y_{j}\right\|_{2}^{2}$$

这个损失函数与高维损失函数相似，高维中已知高维数据求权重，低维中已知权重求低维数据。

此外，我们想得到以原点为中心的有序正交坐标，这里类似PCA和LDA，增加约束条件

- 中心化，均值$\mu=0$，使得数据以原点为中心
$$ \frac{1}{m} \sum_{i=1}^{m} y_i = 0 $$

- 归一化，协方差矩阵为单位矩阵，使得坐标正交

$$ \frac{1}{m} \sum_{i=1}^{m} (y_i - \mu) (y_i - \mu)^T = I $$

化简上述两个式子，可得约束
$$ \sum_{i=1}^{m} y_i = 0 $$
$$ \frac{1}{m} \sum_{i=1}^{m} y_i y_i^T = I $$

5. 求解低维坐标

先给出下列结论：
- $\parallel \mathbf{A} \parallel^2 = tr(A^TA)$
- $ tr(A^T) = tr(A)$
- $ tr(A+B) = tr(A)+tr(B)$
- $ tr(AB) = tr(BA)$
- $ tr(ABC) = tr(BCA) = tr(CAB)$

将第四步的损失函数矩阵化

$$\begin{aligned}
J(w) & =\sum_{i=1}^{m}\left\|y_{i}-\sum_{j \in Q(i)} w_{i j} y_{j}\right\|_{2}^{2} \\
& =\sum_{i=1}^{m}\left\|Y I_i - Y W_{i}\right\|_{2}^{2} \\
& =tr(Y(I-W)(I-W)^T Y^T)
\end{aligned}$$

令$ M = (I-W)(I-W)^T $ ，将归一化约束矩阵化得$ Y Y^T = mI $

将矩阵化的两个式子用拉格朗日子乘法合为一个优化目标：

$$L(W)=tr(Y M Y^T)+\lambda (Y Y^T-mI )$$

对Y求偏导并让其为0

$$ M Y^T = \lambda^{\prime} Y^T $$

代入原式，只与$\lambda$有关，要求出矩阵M最小的d个特征值所对应的d个特征向量组成的矩阵即可得到低维坐标

6. 为什么最小特征值为0？

由于M的最小特征值为0不能反应数据特征，此时对应的特征向量为全1，我们通常选择M的第2个到第d+1个最小的特征值对应的特征向量。

由$W^T·e = e $，得$ \vert W^T - I \vert · e = 0 $

因为e不等于0，所以$ W^T - I = 0 $，即$ (I - W)^T  = 0 $

两边乘$ (I - W)$，则$ (I - W) (I - W)^T e = 0e $，也就是$ M e = 0e $，最小特征值为0。

最后总结，局部线性嵌入基本步骤：
1. 寻找每个样本点的k个近邻点，可以使用欧氏距离；
2. 由每个样本点的近邻点计算出该样本点的局部重建权值矩阵$ W_{i} = \frac{ Z_{i}^{-1} 1_{k}}{ 1_{k}^{T} Z_{i}^{-1} 1_{k}} $；
3. 根据权重$W_{i}$组成权重系数矩阵$ M = (I-W)(I-W)^T $；
4. 计算矩阵M的前d+1个特征值，并计算这d+1个特征值对应的特征向量；
5. 由第二个特征向量到第d+1个特征向量所张成的矩阵即为输出低维样本集矩阵。

LLE算法的主要优点有：

1. 可以学习任意维的局部线性的低维流形

2. 算法归结为稀疏矩阵特征分解，计算复杂度相对较小，实现容易。

LLE算法的主要缺点有：

1. 算法所学习的流形只能是不闭合的，且样本集是稠密均匀的。

2. 算法对最近邻样本数的选择敏感，不同的最近邻数对最后的降维结果有很大影响。
   

LLE在有些情况下也并不适用，例如数据分布在整个封闭的球面上，LLE则不能将它映射到二维空间，且不能保持原有的数据流形。因此在处理数据的时候，需要确保数据不是分布在闭合的球面或者椭圆面上。



```
import numpy as np
import matplotlib.pyplot as plt

def make_swiss_roll(n_samples=100, noise=0.0, random_state=None):
    #Generate a swiss roll dataset.
    t = 1.5 * np.pi * (1 + 2 * np.random.rand(1, n_samples))
    x = t * np.cos(t)
    y = 83 * np.random.rand(1, n_samples)
    z = t * np.sin(t)
    X = np.concatenate((x, y, z))
    X += noise * np.random.randn(3, n_samples)
    X = X.T
    t = np.squeeze(t)
    return X, t

#返回任意两个点之间距离的平方
def cal_pairwise_dist(x):
    '''
    计算pairwise 距离, x是matrix
    (a-b)^2 = a^2 + b^2 - 2*a*b
    '''
    sum_x = np.sum(np.square(x), 1) # 求L2范数
    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)
    
    return dist

def get_n_neighbors(data, n_neighbors = 10):
    '''
    :param data: (n_samples, n_features)
    :param n_neighbors: n nearest neighbors
    :return: neighbors indexs
    '''
    dist = cal_pairwise_dist(data) # 计算数据点距离平方
    dist[dist < 0] = 0 # 确保数据没有负数
    dist = dist**0.5 # 获得欧氏距离
    n = dist.shape[0] # 获得样本数
    
    #  argsort按行返回从小到大排序后的索引，跳过自身选择邻居
    Index = np.argsort(dist, axis=1)[:, 1:n_neighbors+1]

    return Index.astype(np.int32)

def lle(data, n_dims = 2, n_neighbors = 10):
    '''
    :param data:(n_samples, n_features)
    :param n_dims: target n_dims
    :param n_neighbors: n nearest neighbors
    :return: (n_samples, n_dims)
    '''
    N = get_n_neighbors(data, n_neighbors) # 邻居索引矩阵
    n, D = np.shape(data)

    # 避免邻居数量多于特征维度时，解不唯一
    if n_neighbors > D:
        tol = 1e-3
    else:
        tol = 0

    # 计算重构权重 w
    W = np.zeros((n_neighbors, n))
    I = np.ones((n_neighbors, 1))
    for i in range(n):
        Xi = np.tile(data[i], (n_neighbors, 1)).T
        Ni = data[N[i]].T

        Si = np.dot((Xi-Ni).T, (Xi-Ni))
        # 防止对角线元素过小
        Si = Si+np.eye(n_neighbors)*tol*np.trace(Si)

        Si_inv = np.linalg.pinv(Si)
        wi = (np.dot(Si_inv, I))/(np.dot(np.dot(I.T, Si_inv), I)[0,0])
        W[:, i] = wi[:,0]

    W_y = np.zeros((n, n))
    for i in range(n):
        index = N[i]
        for j in range(n_neighbors):
            W_y[index[j],i] = W[j,i]

    I_y = np.eye(n)
    M = np.dot((I_y - W_y), (I_y - W_y).T)

    eig_val, eig_vector = np.linalg.eig(M)
    index_ = np.argsort(np.abs(eig_val))[1:n_dims+1]
    
    Y = eig_vector[:, index_]
    return Y
    
X, Y = make_swiss_roll(n_samples = 1500, noise=0.1, random_state=42)

data =lle(X, n_neighbors = 15)

plt.title("my_LLE")
plt.scatter(data[:, 0], data[:, 1], c = Y, cmap=plt.cm.Spectral)
```



```
# sklearn学习库
from sklearn import manifold, datasets
import numpy as np
import matplotlib.pyplot as plt

# 防止matplotlib中文报错
plt.rcParams['font.sans-serif']=['SimHei']
plt.rcParams['axes.unicode_minus'] = False

# 生成瑞士卷数据
X, color = datasets.make_swiss_roll(n_samples=1500)

# 创建一个画布
fig = plt.figure(figsize=(14, 7))  # 设置画布大小

# 第一个子图：原始三维数据
ax1 = fig.add_subplot(1, 2, 1, projection='3d')  # 1行2列的第1个
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax1.set_title("原始瑞士卷数据")  # 设置标题
ax1.view_init(4, -75)  # 调整视角

# 使用 LLE 进行降维
se = manifold.LocallyLinearEmbedding(n_neighbors=10, n_components=2) # 10个邻居，降维到2维

Y = se.fit_transform(X)

# 第二个子图：降维后的二维数据
ax2 = fig.add_subplot(1, 2, 2)  # 1行2列的第2个
ax2.scatter(Y[:, 0], Y[:, 1], c=color, cmap=plt.cm.Spectral)
ax2.set_title("LLE降维后")  # 设置标题

# 显示整个画布
plt.show()
```

