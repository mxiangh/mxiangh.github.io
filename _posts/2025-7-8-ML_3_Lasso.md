---
title: 机器学习（3）拉索回归——Lasso Regression
tags: ML Regression
---

思想：和岭回归非常相似，为了解决多元线性回归$ \mathbf{X}^T \mathbf{X} $不满秩的情况，引入正则化项，使用$ L_1 $ 正则化项。可以将不重要特征的系数压缩至零，也就是生成稀疏解。

<!--more-->

##### 1.模型对比

多元线性回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w})$$

岭回归

$$ E_{\hat{w}} = min (\mathbf{y}-\mathbf{X} \hat{w})^T (\mathbf{y}-\mathbf{X} \hat{w}) + \lambda \left\| \hat{w} \right\| $$ 

其中，$ \lambda $是一个自定义的超参数，$ \left\| \hat{w} \right\| $ 是$ L_1 $范数。

##### 2.范数

强化过后的距离概念，可以把范数理解为距离。设向量$ x=(x_1,x_2,\dots,x_n)^T $，则$ L_1 $范数为

$$ ||x||_1 = |x_1+|x_2|+\dots+|x_n| $$

由于$L_1$正则化项不可导，所以采用坐标下降法求解。

##### 3.坐标下降法（Coordinate descent）

3.1 数学原理

定理：给定一个可微的凸函数 $ f: \mathbb{R}^{n} \rightarrow \mathbb{R} $ ，如果在某一点  x  ，使得  f(x)  在每一个坐标轴上都是最小值，那么  f(x)  是一个全局的最小值。

证明：$\bigtriangledown f(x)=(\frac{\partial f}{\partial x_{1}} (x),\frac{\partial f}{\partial x_{2}} (x),\dots,\frac{\partial f}{\partial x_{n}} (x))=0$

基于此，可以证明给定一个$ f(x)=g(x)+\sum_{i=1}^nh_i(x_i) $，如果在某一点  x  ，使得  f(x)  在每一个坐标轴上都是最小值，那么  f(x)  是一个全局的最小值。

其中g是可微的凸函数，每一个$h_i$都是凸的。

注：这里涉及到凸优化的理论。

证明：对于任意y，有$f(y) \ge f(x) $，则x是全局最小值。

$$ f(y)-f(x) = g(y)-g(x)+\sum_{i=1}^n \lbrack h_i(y_i) - h_i(x_i) \rbrack $$

由于g和$h_i$都是凸的，则有

$$ g(y)-g(x) \ge \bigtriangledown g(x)^T(y-x)$$

$$ h_i(y_i) - h_i(x_i) \ge h_{i}^{'}(x_i)(y_i-x_i)$$

得到

$$ \begin{aligned}
f(y)-f(x) & = g(y)-g(x)+\sum_{i=1}^n \lbrack h_i(y_i) - h_i(x_i) \rbrack \\
& \ge \bigtriangledown g(x)^T(y-x)+\sum_{i=1}^n \lbrack h_i(y_i) - h_i(x_i) \rbrack
\end{aligned}$$

因为x在每个坐标都是最小的，则有

$$ \bigtriangledown_i g(x)+h_{i}^{'}(x_i)=0$$

即对于每一项有$ \bigtriangledown_i g(x)(y_i-x_i) + h_i(y_i) - h_i(x_i) \ge \bigtriangledown_i g(x)+h_{i}^{'}(x_i)=0 $

整体求和则

$$ \begin{aligned}
f(y)-f(x) & = g(y)-g(x)+\sum_{i=1}^n \lbrack h_i(y_i) - h_i(x_i) \rbrack \\
& \ge \bigtriangledown g(x)^T(y-x)+\sum_{i=1}^n \lbrack h_i(y_i) - h_i(x_i) \rbrack \\
& = \sum_{i=1}^n \lbrack \bigtriangledown_i g(x)(y_i-x_i) + h_i(y_i) - h_i(x_i) \rbrack \ge 0
\end{aligned}$$

而$ f(x)=g(x)+\sum_{i=1}^nh_i(x_i) $ 就是Lasso的目标函数。

坐标下降法属于一种非梯度优化的方法，它在每步迭代中沿一个坐标的方向进行线性搜索（线性搜索是不需要求导数的），通过循环使用不同的坐标方法来达到目标函数的局部极小值。但如果目标函数不光滑的话，坐标下降法可能会陷入非驻点。

3.2算法流程：

- 在x向量中随机选择初值，记为$x^{(0)}$，括号里面的数字代表迭代的轮数，初始轮数为0。

- 对于第k轮迭代，从$x_{1}^{k}$开始，到$x_{n}^{k}$为止，依次求$x_{i}^{k}$。

- 如果达到迭代次数或者求解值收敛在阈值之内，则停止迭代，否则重复第二步。

假设目标函数是求解f(x)的极小值，其中$x=(x_1,x_2,\dots,x_n)$是一个n维的向量，从初始点$x^{(0)}$开始对k进行循环：

$$\begin{aligned}
x_1^{(k)}&= \underset{x_1}{arg\ min\ } f(x_1,x_2^{(k-1)},x_3^{(k-1)},\dots,x_n^{(k-1)}) \\
x_2^{(k)}&= \underset{x_2}{arg\ min\ } f(x_1^{(k)},x_2,x_3^{(k-1)},\dots,x_n^{(k-1)}) \\
x_3^{(k)}&= \underset{x_3}{arg\ min\ } f(x_1^{(k)},x_2^{(k)},x_3,\dots,x_n^{(k-1)}) \\
\vdots \\
x_n^{(k)}&= \underset{x_n}{arg\ min\ } f(x_1^{(k)},x_2^{(k)},x_3^{(k)},\dots,x_n) \\
\end{aligned}$$

每次迭代都只是更新x的一个维度，即把该维度当做变量，剩下的n-1个维度当作常量，通过最小化f(x)来找到该维度对应的新的值。坐标下降法就是通过迭代地构造序列$x^0,x^1,x^2,\ldots$来求解问题，即最终点收敛到期望的局部极小值点。通过上述操作，显然有：

$$ f(x^0）\ge f(x^1） \ge f(x^2） \dots $$

软阈值函数：
$$
S(\rho,\lambda)=\left\{\begin{array}{l}
\rho-\lambda, if \ \rho > \lambda \\
\rho+\lambda, if \ \rho < -\lambda \\
0, \text{otherwise} \\
\end{array}\right.
$$

- $\rho$是未正则化的线性回归系数（或梯度方向）。
- $\lambda$是正则化系数。

作用是在保证模型拟合能力的同时，通过 L1 正则化生成稀疏解，自动选择重要特征，并提升计算效率。它是 Lasso 回归能实现特征选择的核心。

3.3 举个例子：

假设输入数据x和目标值y为

$$x=\begin{bmatrix}
 1 & 2\\
 3 & 4
\end{bmatrix}，y=\begin{bmatrix}
 6\\
 8
\end{bmatrix}$$

正则化强度$\lambda=1$

Lasso优化目标函数：$ \underset{w_1,w_2}{min} \lbrace \sum_{i=1}^{n} (y_i-w_1x_{i1}-w_2x_{i2})^2+\lambda(\vert w_1 \vert + \vert w_2 \vert )\rbrace $

（1）初始化权重$w_1=0$，$w_2=0$

（2）对$w_1$进行更新

排除$w_1$的影响，计算残差： $ r=y-X_2 w_2=y-0=y $

计算相关系数：$\rho_1=\frac{X_1^T r}{n}=\frac{1×6+3×8}{2}=15$

注：这里其实和求线性回归的w是一样的，$\frac{X_1^T r}{n}$相当于目标函数对$w_1$求导取0，解得的$w_1$。

应用软阈值函数更新：$ w_1= S(\rho_1,\lambda)=14 $

（3）对$w_2$进行更新

排除$w_2$的影响，计算残差： $ r=y-X_1 w_1=y-14 X_1=\begin{bmatrix} 6-14×1 \\ 8-14×3 \end{bmatrix}=\begin{bmatrix} -8 \\ -46 \end{bmatrix} $

计算相关系数：$\rho_2=\frac{X_2^T r}{n}=\frac{2×(-8)+4×(-46)}{2}=-100$

应用软阈值函数更新：$ w_2= S(\rho_2,\lambda)=-99 $

（4）检查收敛

如果权重w变化很小，例如 $\left\| w_{new} - w_{old} \right\|<0.001$，则停止

否则，继续迭代。

迭代到最后，就能求出$ w_1 $和$w_2$。

~~~
# 手写实现
import numpy as np

# 软阈值函数
# 在保证模型拟合能力的同时，通过 L1 正则化生成稀疏解，自动选择重要特征，并提升计算效率
def soft_threshold(rho, alpha):
    if rho > alpha:
        return rho - alpha
    elif rho < -alpha:
        return rho + alpha
    else:
        return 0

# 坐标下降法
def lasso_coordinate_descent(X, y, alpha=1.0, max_iter=1000, tol=1e-4):
    n_samples, n_features = X.shape
    w = np.zeros(n_features)  # 初始化权重
    
    # 对数据标准化（重要：Lasso 对尺度敏感）
    X = (X - np.mean(X, axis=0)) / np.std(X, axis=0)
    y = y - np.mean(y)
    
    for _ in range(max_iter):
        w_prev = np.copy(w)
        for j in range(n_features):
            # 计算残差（排除当前特征）
            r_j = y - X @ w + X[:, j] * w[j]
            # 更新权重 w[j]
            rho_j = X[:, j] @ r_j / n_samples
            w[j] = soft_threshold(rho_j, alpha)
        
        # 检查收敛
        if np.linalg.norm(w - w_prev) < tol:
            break
    
    return w

def lasso(X, y, alpha=1.0):
    # 中心化 y（偏置项单独计算）
    y_mean = np.mean(y)
    y_centered = y - y_mean
    
    # 拟合 Lasso（不包含偏置项）
    w = lasso_coordinate_descent(X, y_centered, alpha)
    
    # 计算偏置 b = mean(y - Xw)
    b = y_mean - np.mean(X @ w)
    
    return w, b

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b

X, y, true_w, true_b = generatr_function(3)

estimated_w, estimated_b = lasso(X, y)

print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", estimated_w)
print("估计偏置 (b̂):", estimated_b)
~~~

~~~
# sklearn实现
from sklearn.linear_model import Lasso
import numpy as np

# 数据生成
def generatr_function(n_features, n_samples=1000, noise_scale=0.1):
    np.random.seed(42)  # 固定随机种子
    X = np.random.randn(n_samples, n_features)  # 标准正态分布
    
    true_w = np.random.randn(n_features)  # 真实权重
    true_b = np.random.randn()  # 真实偏置
    
    # 计算 y = Xw + b + 噪声
    noise = noise_scale * np.random.randn(n_samples)
    y = X @ true_w + true_b + noise

    return X, y, true_w, true_b
    
X, y, true_w, true_b = generatr_function(3)

# 创建模型并拟合
model = Lasso(alpha=0.1)
model.fit(X, y)

# 预测
y_pred = model.predict(X)

# 评估
print("真实权重 (w):", true_w)
print("真实偏置 (b):", true_b)
print("估计权重 (ŵ):", model.coef_)
print("估计偏置 (b̂):", model.intercept_)
~~~

