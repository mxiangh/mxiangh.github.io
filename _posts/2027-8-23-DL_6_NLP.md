---

title: 自然语言处理（1）初识NLP
tags: DL NLP
typora-root-url: ./..
---

自然语言处理。

<!--more-->

##### 3.文本预处理

在训练一个RNN模型前，首先要面对文本数据的处理问题，⼀篇文章可以被简单地看作是⼀串单词序列，甚⾄是⼀串字符序列，解析文本的常见步骤包括：

- 将⽂本作为字符串加载到内存中。
- 将字符串拆分为词元（如单词和字符）。
- 建⽴⼀个词表，将拆分的词元映射到数字索引。
- 将⽂本转换为数字索引序列，⽅便模型操作。

注：其实这里已经属于NLP（自然语言处理）的范畴了，3.4汇总了文本预处理代码，可以直接使用，不必对NLP文本处理过程进行了解。

###### 3.1 读取数据集

这里选择李沐动手学深度学习里相同的数据集，H.G.Well的时光机器，当然，这里不会使用d2l这个函数，而是用一个简单的爬虫代码获得txt文本文件。

~~~
import requests

# 数据存放网址
url = "https://www.gutenberg.org/cache/epub/35/pg35.txt"
response = requests.get(url)
with open('TimeMachine.txt', 'w', encoding='utf-8') as f:
    f.write(response.text)
print("文件已成功保存！")    
~~~

这里对代码不做详解，当运行成功时，文件夹会多出一个TimeMachine.txt文件，接着，导入文件并对文本数据进行简单清洗和处理。

~~~
import re

def read_time_machine():
    with open('TimeMachine.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 对每行进行清洗
    # 1.re.sub：去除非字母字符
    # 2.strip：去除字符串首尾的空白字符
    # 3.lower：所有大写字母转为小写字母
    cleaned_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    # 对处理后的列表，删除里面的空字符串
    return [line for line in cleaned_lines if line]

lines = read_time_machine()
print(f'文本总行数: {len(lines)}')
~~~

###### 3.2 词元化

3.1节中最后的lines是已经处理好的文本列表，列表中的每个元素是一个文本序列，每个文本序列又被拆分成一个词元列表，词元（token）是文本的基本单元。现在，将lines处理成一个由词元列表组成的列表，其中每个词元都是一个字符串。

~~~
def tokenize(lines, token='word'):
    # 将⽂本⾏拆分为单词或字符词元
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)
        
tokens = tokenize(lines)
# 打印前11行数据
for i in range(11):
    print(tokens[i])
~~~

对tokenize进行简单解释，token为要采取的词元类型，当选择word（默认）时，词元是一个单词，当选择char时，词元是一个字母或字符。

###### 3.3 词表

词元的类型是字符串，但是模型需要的输入是数字（只有数字能做计算）。所以通常需要构建一个字典，或者称词表（vocabulary），⽤来将字符串类型的词元映射到从0开始的数字索引中，这个索引唯一。

将训练集中的所有数据合并在一起，得到的结果称为语料（corpus）。根据每个唯一词元的出现频率，为其分配一个数字索引，得到词表。

很少出现的词元通常被移除，这可以降低复杂性。另外，语料库中不存在或已删除的任何词元都将映射到⼀个特定的未知词元“<unk>”。我们可以选择增加⼀个列表，⽤于保存那些被保留的词元，例如：填充词元（“<pad>”）；序列开始词元（“<bos>”）；序列结束词元（“<eos>”）。

注：语料库是大量文本数据组成的集合，词表是语料库中提取出来的唯一词元。

~~~
import collections

# 构建词表
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                
    def __len__(self):
        return len(self.idx_to_token)
        
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]
        
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
        
    @property
    def unk(self): # 未知词元的索引为0
        return 0
        
    @property
    def token_freqs(self):
        return self._token_freqs
            
def count_corpus(tokens):
    # 统计词元的频率
    # 这⾥的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成⼀个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

vocab = Vocab(tokens)

for i in [0, 10]:
    print('⽂本:', tokens[i])
    print('索引:', vocab[tokens[i]])
~~~

上述代码使⽤了时光机器数据集作为语料库来构建词表。

###### 3.4 代码汇总

~~~
import requests
import re
import collections

# 获取数据
url = "https://www.gutenberg.org/cache/epub/35/pg35.txt"  # 网页链接
response = requests.get(url)
with open('TimeMachine.txt', 'w', encoding='utf-8') as f:
    f.write(response.text)

# 数据清洗
def read_time_machine():
    with open('TimeMachine.txt', 'r', encoding='utf-8') as f:
        lines = f.readlines()
    # 对每行进行清洗
    # 1.re.sub：去除非字母字符
    # 2.strip：去除字符串首尾的空白字符
    # 3.lower：所有大写字母转为小写字母
    cleaned_lines = [re.sub('[^A-Za-z]+', ' ', line).strip().lower() for line in lines]
    # 对处理后的列表，删除里面的空字符串
    return [line for line in cleaned_lines if line]

# 词元化
def tokenize(lines, token='word'):
    # 将⽂本⾏拆分为单词或字符词元
    if token == 'word':
        return [line.split() for line in lines]
    elif token == 'char':
        return [list(line) for line in lines]
    else:
        print('错误：未知词元类型：' + token)
        
# 构建词表
class Vocab:
    def __init__(self, tokens=None, min_freq=0, reserved_tokens=None):
        if tokens is None:
            tokens = []
        if reserved_tokens is None:
            reserved_tokens = []
        # 按出现频率排序
        counter = count_corpus(tokens)
        self._token_freqs = sorted(counter.items(), key=lambda x: x[1], reverse=True)
        # 未知词元的索引为0
        self.idx_to_token = ['<unk>'] + reserved_tokens
        self.token_to_idx = {token: idx for idx, token in enumerate(self.idx_to_token)}
        for token, freq in self._token_freqs:
            if freq < min_freq:
                break
            if token not in self.token_to_idx:
                self.idx_to_token.append(token)
                self.token_to_idx[token] = len(self.idx_to_token) - 1
                
    def __len__(self):
        return len(self.idx_to_token)
        
    def __getitem__(self, tokens):
        if not isinstance(tokens, (list, tuple)):
            return self.token_to_idx.get(tokens, self.unk)
        return [self.__getitem__(token) for token in tokens]
        
    def to_tokens(self, indices):
        if not isinstance(indices, (list, tuple)):
            return self.idx_to_token[indices]
        return [self.idx_to_token[index] for index in indices]
        
    @property
    def unk(self): # 未知词元的索引为0
        return 0
        
    @property
    def token_freqs(self):
        return self._token_freqs
            
def count_corpus(tokens):
    # 统计词元的频率
    # 这⾥的tokens是1D列表或2D列表
    if len(tokens) == 0 or isinstance(tokens[0], list):
        # 将词元列表展平成⼀个列表
        tokens = [token for line in tokens for token in line]
    return collections.Counter(tokens)

# 返回时光机器数据集的词元索引列表和词表（词元为单个字符）
def load_corpus_time_machine(max_tokens=-1): #@save
    lines = read_time_machine()
    tokens = tokenize(lines, 'char')
    vocab = Vocab(tokens)
    
    # 因为时光机器数据集中的每个⽂本⾏不⼀定是⼀个句⼦或⼀个段落，
    # 所以将所有⽂本⾏展平到⼀个列表中
    corpus = [vocab[token] for line in tokens for token in line]
    if max_tokens > 0:
        corpus = corpus[:max_tokens]
    return corpus, vocab

corpus, vocab = load_corpus_time_machine()
print(len(corpus))
print(len(vocab))
~~~

注：vocab是词表，corpus是数字索引（原文每个单词映射后的数字）

