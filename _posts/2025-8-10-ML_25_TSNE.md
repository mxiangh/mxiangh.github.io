---

title: 机器学习（25）T分布-随机邻域嵌入——T-Stochastic Neighbor Embedding, T-SNE
tags: ML Dimensionality_Reduction
---

思想：将高维空间和低维空间数据点之间的距离转为成对相似性的联合概率，高维用高斯分布计算概率，低维用T分布计算概率，之后使用KL散度来衡量这两种概率相同的程度。。

<!--more-->

论文：2008，Visualizing Data using t-SNE

##### 1.随机邻域嵌入（Stochastic Neighbor Embedding，SNE）

思想：将高维空间和低维空间数据点之间的距离转为成对相似性的条件概率，用高斯分布计算概率，之后使用KL散度来衡量这两种概率相同的程度。

1.1 高维欧氏距离转为相似性条件概率

数据点$x_j$ 相对于数据点$x_i$的相似性，是条件概率$p_{j \mid i}$ ，即若邻居是根据以$x_i$为中心的高斯分布的概率密度来选取时，$x_i$选取$x_j$作为邻居的概率。对于邻近的数据点，$p_{j\mid i}$ 相对较高；而对于相距较远的数据点，$p_{j\mid i}$ 几乎会非常小（当高斯分布的方差$\sigma_i$取合理值时）：

$$p_{j\mid i} = \frac{\exp\left(-\lVert x_i - x_j \rVert^2 / 2\sigma_i^2\right)}{\sum_{k\neq i} \exp\left(-\lVert x_i - x_k \rVert^2 / 2\sigma_i^2\right)}$$

其中，$\sigma_i$ 是以数据点$x_i$ 为中心的高斯分布的方差。

注：概率密度函数是一个描述随机变量在某个确定的取值点附近的可能性的函数，也就是$x_j$在$x_i$附近的可能性可以用概率密度函数表示。

对于高维数据点$x_i$ 和$x_j$对应的低维数据点$y_i$和$y_j$ ，可以计算一个类似的条件概率，我们记为$q_{j\mid i}$。我们将计算条件概率$q_{j\mid i}$时所用高斯分布的方差设为$(\frac{1}{\sqrt{2}})^2$ 。因此，我们通过下式对映射点$y_j$相对于映射点$y_i$的相似性进行建模：

$$q_{j\mid i} = \frac{\exp\left(-\lVert y_i - y_j \rVert^2\right)}{\sum_{k\neq i} \exp\left(-\lVert y_i - y_k \rVert^2\right)}$$

由于我们仅关注成对相似性建模，所以设$q_{i\mid i} = 0$。

1.2 损失函数

如果映射点$y_i$和$y_j$正确建模了高维数据点$x_i$和$x_j$之间的相似性，那么条件概率$p_{j\mid i}$和$q_{j\mid i}$ 将会相等。

为了使$p_{j\mid i}$和$q_{j\mid i}$之间的不匹配程度最小，故使用梯度下降法最小化所有数据点的KL散度之和：

$$C = \sum_{i} KL(P_i \lVert Q_i) = \sum_{i} \sum_{j} p_{j\mid i} \log \frac{p_{j\mid i}}{q_{j\mid i}}$$

其中，$P_i$表示给定数据点$x_i$时，相对于所有其他数据点的条件概率分布；$Q_i$表示给定映射点$y_i$时，相对于所有其他映射点的条件概率分布。

注：KL散度用来衡量两种概率分布的差异

通过二分搜索来寻找$\sigma_i$的值，使得产生的概率分布$P_i$具有用户指定的固定困惑度：

$$Perp(P_i) = 2^{H(P_i)}$$

其中，$H(P_i)$ 是$P_i$的香农熵：

$$H(P_i) = -\sum_{j} p_{j\mid i} \log_2 p_{j\mid i}$$

1.3 求解

使用动量梯度下降进行优化

$$\frac{\delta C}{\delta y_i} = 2 \sum_{j} (p_{j\mid i} - q_{j\mid i} + p_{i\mid j} - q_{i\mid j})(y_i - y_j)$$

带有动量项的梯度更新由下式给出：

$$y^{(t)} = y^{(t - 1)} + \eta \frac{\delta C}{\delta y} + \alpha(t) \left(y^{(t - 1)} - y^{(t - 2)}\right)$$

其中，$y^{(t)}$表示迭代t 时的解，$\eta$表示学习率，$\alpha(t)$表示迭代t 时的动量。

##### 2.t-SNE

2.1 对称SNE

将条件概率变为联合概率，对于所有的i、j，具有$p_{ij} = p_{ji}$且$q_{ij} = q_{ji}$的性质：

$$C = KL(P \vert \vert Q) = \sum_{i}\sum_{j} p_{ij} \log \frac{p_{ij}}{q_{ij}}$$

低维映射中的成对相似性$q_{ij}$：

$$q_{ij} = \frac{\exp\left(-\lVert y_i - y_j \rVert^2\right)}{\sum_{k\neq i} \exp\left(-\lVert y_k - y_i \rVert^2\right)} \tag{3}$$

定义高维空间中成对相似性$p_{ij}$：

$$p_{ij} = \frac{\exp\left(-\lVert x_i - x_j \rVert^2 / 2\sigma^2\right)}{\sum_{k\neq i} \exp\left(-\lVert x_k - x_i \rVert^2 / 2\sigma^2\right)}$$

将高维空间中的联合概率$p_{ij}$定义为对称化的条件概率，令$p_{ij} = \frac{p_{j|i} + p_{i|j}}{2n}$。确保对于所有数据点$x_i$，$\sum_{j} p_{ij} > \frac{1}{2n}$，因此每个数据点$x_i$都会对代价函数做出显著贡献。

梯度：

$$\frac{\delta C}{\delta y_i} = 4 \sum_{j} (p_{ij} - q_{ij})(y_i - y_j)$$

2.2 拥挤问题

以十维到二维为例，存在以下几个问题：

（1）等距点摆不开

在十维空间上，11个数据点，任意两个点之间的距离都是相同的，但如果降维到二维，几乎不可能让11个数据点两两距离都一样。

（2）中等距离被推远

在十维空间上，以某个数据点为中心，中圈比小圈能容纳更多数据点。但降维后，小圈可能能放下，中圈数据太多，无法容纳，只能将中等距离的数据放在离中心点更远的地方。

（3）聚类无法区分

远处数据点虽然远，但是数量过多，原本应该被分开的数据点会挤在一起。

2.3 解决拥挤问题

在低维映射中使用具有一个自由度的学生 t 分布（与柯西分布相同）作为重尾分布。使用这种分布，联合概率$q_{ij}$定义为：

$$q_{ij} = \frac{(1 + \lVert y_i - y_j \rVert^2)^{-1}}{\sum_{k\neq l} (1 + \lVert y_k - y_l \rVert^2)^{-1}}$$

求得梯度

$$\frac{\delta C}{\delta y_i} = 4 \sum_{j} (p_{ij} - q_{ij})(y_i - y_j)(1 + \lVert y_i - y_j \rVert^2)^{-1}$$

2.4 两个技巧

（1）早期压缩

添加L2范数。

（2）早期夸张

在优化的初始阶段将所有的$p_{ij}$乘以一个系数，

##### 3.优缺点

优点：

- 可以分类可视化。

缺点：

- 不清楚 t - SNE 在一般降维任务中的表现；

- t - SNE 相对局部的性质使其对数据的本征维度诅咒敏感；

- t - SNE 无法保证收敛到其代价函数的全局最优解。

```
import numpy as np
import matplotlib.pyplot as plt
from sklearn.decomposition import PCA

# 计算任意两点之前距离 ||x_i-x_j||^2
# X 维度 [N,D]
def cal_pairwise_dist(X):
    sum_X = np.sum(np.square(X), 1)
    D = np.add(np.add(-2 * np.dot(X, X.T), sum_X).T, sum_X)

    return D

# 计算P_ij 以及 log松弛度
def calc_P_and_entropy(D,beta=1.0):
    P = np.exp(-D.copy() * beta)
    sumP = np.sum(P)

    sumP = max(np.sum(sumP), 1e-10)
    
    # 计算熵
    log_entropy = np.log(sumP) + beta * np.sum(D * P) / sumP
        
    P = P/sumP
    return P,log_entropy
      
# 二值搜索寻找最优的 sigma
def binary_search(D, init_beta,logU, tol=1e-5, max_iter=50):
    
    beta_max = np.inf
    beta_min = -np.inf
    beta = init_beta
    
    P,log_entropy=calc_P_and_entropy(D,beta)
    diff_log_entropy = log_entropy - logU

    m_iter = 0
    while np.abs(diff_log_entropy)> tol and m_iter<max_iter:
        # 交叉熵比期望值大，增大beta
        if diff_log_entropy>0:
            beta_min = beta
            if beta_max == np.inf or beta_max == -np.inf:
                beta = beta*2
            else:
                beta = (beta+beta_max)/2.
        # 交叉熵比期望值小， 减少beta        
        else:
            beta_max = beta
            if beta_min == -np.inf or beta_min == -np.inf:
                beta = beta/2
            else:
                beta = (beta + beta_min)/2.
        
        # 重新计算
        P,log_entropy=calc_P_and_entropy(D,beta)
                
        diff_log_entropy = log_entropy - logU
        
        m_iter = m_iter+1
    
    # 返回最优的 beta 以及所对应的 P
    return P, beta
        
# 给定一组数据 datas ：[N,D] 
# 计算联合概率 P_ij : [N,N]
def p_joint(datas, target_perplexity):
    
    N,D = np.shape(datas)
    # 计算两两之间的距离
    distances = cal_pairwise_dist(datas)
    
    beta = np.ones([N,1])  # beta = 1/(2*sigma^2)
    logU = np.log(target_perplexity)
    p_conditional = np.zeros([N,N])
    # 对每个样本点搜索最优的sigma(beta) 并计算对应的P
    for i in range(N):
        if i %500 ==0:
            print("Compute joint P for %d points"%(i))
        # 删除 i -i 点
        Di = np.delete(distances[i,:],i)
        # 进行二值搜索，寻找 beta 
        # 使 log_entropy 最接近 logU
        P, beta[i] = binary_search(Di, beta[i],logU)
        
        # 在ii的位置插0
        p_conditional[i] = np.insert(P,i,0)

    # 计算联合概率
    P_join = p_conditional + p_conditional.T
    P_join = P_join/np.sum(P_join)
    
    print("Mean value of sigma: %f" % np.mean(np.sqrt(1 / beta)))
    return P_join
    
# Y : 低维数据 [N,d]
# 根据Y，计算低维的联合概率 q_ij
def q_tsne(Y):
    N = np.shape(Y)[0]
    sum_Y = np.sum(np.square(Y), 1)
    num = -2. * np.dot(Y, Y.T)
    num = 1. / (1. + np.add(np.add(num, sum_Y).T, sum_Y))
    num[range(N), range(N)] = 0.
    Q = num / np.sum(num)
    Q = np.maximum(Q, 1e-12)
    return Q,num
   

# datas 输入高维数据 [N,D]
# labs 高维数据的标签[N,1]
# dim 降维的维度 d  
# plot 绘图

def estimate_tsen(datas,labs,dim,target_perplexity,plot=False):
    
    N,D = np.shape(datas)
    
    # 随机初始化低维数据Y
    Y = np.random.randn(N, dim)
    
    # 计算高维数据的联合概率
    print("Compute P_joint")
    P = p_joint(datas, target_perplexity)
    
    # 开始若干轮对 P 进行放大
    P = P*4.
    P = np.maximum(P, 1e-12)
    
    # 开始进行迭代训练
    # 训练相关参数
    max_iter = 1500
    initial_momentum = 0.5
    final_momentum = 0.8
    eta = 500  # 学习率
    min_gain = 0.01
    dY = np.zeros([N, dim]) # 梯度
    iY = np.zeros([N, dim]) # Y的变化
    gains = np.ones([N, dim])
    
    for m_iter in range(max_iter):
        
        # 计算 Q 
        Q,num= q_tsne(Y)
        
        # 计算梯度
        PQ = P - Q
        for i in range(N):
            dY[i, :] = np.sum(np.tile(PQ[:, i] * num[:, i], (dim, 1)).T * (Y[i, :] - Y), 0)
    
        
        # Perform the update
        if m_iter < 20:
            momentum = initial_momentum
        else:
            momentum = final_momentum
        gains = (gains + 0.2) * ((dY > 0.) != (iY > 0.)) + \
                (gains * 0.8) * ((dY > 0.) == (iY > 0.))
        gains[gains < min_gain] = min_gain
        iY = momentum * iY - eta * (gains * dY)
        Y = Y + iY
        
        # Y 取中心化
        Y = Y - np.tile(np.mean(Y, 0), (N, 1))

        # Compute current value of cost function
        if (m_iter + 1) % 10 == 0:
            C = np.sum(P * np.log(P / Q))
            print("Iteration %d: loss is %f" % (m_iter + 1, C))

        # 停止放大P
        if m_iter == 100:
            P = P / 4.
   
        if plot and m_iter % 100 == 0:
            print("Draw Map")
            draw_pic(Y,labs)
    
    return Y
        
def draw_pic(datas,labs):
    plt.cla()
    unque_labs = np.unique(labs)
    colors = [plt.cm.Spectral(each)
      for each in np.linspace(0, 1,len(unque_labs))]
    p=[]
    legends = []
    for i in range(len(unque_labs)):
        index = np.where(labs==unque_labs[i])
        pi = plt.scatter(datas[index, 0], datas[index, 1], c =[colors[i]] )
        p.append(pi)
        legends.append(unque_labs[i])

    plt.legend(p, legends)
    plt.show()
         
mnist_datas = np.loadtxt("mnist2500_X.txt")
mnist_labs = np.loadtxt("mnist2500_labels.txt")

print("first reduce by PCA")
datas= PCA(n_components=30).fit_transform(mnist_datas)

X = datas.real

Y = estimate_tsen(X,mnist_labs,2,30, plot=True)

draw_pic(Y,mnist_labs)
```

