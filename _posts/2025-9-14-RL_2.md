---
title: 强化学习（2）Bellman Equation
tags: RL
typora-root-url: ./..
---

介绍贝尔曼方程（Bellman Equation）。

<!--more-->

##### 1.return的作用

回顾前一节的return：沿trajectory获得的discounted reward总和。

如果我们想要判断一个policy的好坏程度，return的重要性就体现出来了。

![](/images/RL/14.png)

假设起始点是$s_1$，终点是$s_4$。第一个policy和第二个policy从直觉上来看，第一个会比第二个好，因为第一个没有进入forbidden区域，但是直觉不一定对，所以需要数学来证明这件事。

对于第一个policy：

$$ \begin{aligned}
return_1 & =0+\gamma 1 + \gamma^2 1 + \dots \\
& =\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = \frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第二个policy：

$$ \begin{aligned}
return_2 & = -1+\gamma 1 + \gamma^2 1 + \dots \\
& =-1+\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = -1+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第三个policy：

$$ \begin{aligned}
return_3 & = 0.5(-1+\frac{\gamma}{1-\gamma})+0.5(\frac{\gamma}{1-\gamma}) \\
& = -0.5+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对比三个return：

$$ return_1 > return_3 > return_2 $$

上述不等式表明，第一个policy是最佳的，第二个policy是最差的，这与我们的直觉完全一致。

计算return对于评估policy非常重要。

##### 2.计算return

（1）定义法

![](/images/RL/15.png)

让$v_i$表示从$s_i$开始获得的return：

$$ \begin{aligned}
v_1 & = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots \\
v_2 & = r_2 + \gamma r_3 + \gamma^2 r_4 + \dots \\
v_3 & = r_3 + \gamma r_4 + \gamma^2 r_1 + \dots \\
v_4 & = r_4 + \gamma r_1 + \gamma^2 r_2 + \dots \\
\end{aligned}$$

（2）推导

对于$v_1$，如果从第二项到最后一项都提出一个$\gamma$：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)$$

可以发现式子中出现了$v_2$，即：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2$$

对于另外三个式子同理，于是可以得到新的公式：

$$ \begin{aligned}
v_1 & = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2 \\
v_2 & = r_2 + \gamma (r_3 + \gamma r_4 + \dots)=r_2+\gamma v_3 \\
v_3 & = r_3 + \gamma (r_4 + \gamma r_1 + \dots)=r_3+\gamma v_4 \\
v_4 & = r_4 + \gamma (r_1 + \gamma r_2 + \dots)=r_4+\gamma v_1 \\
\end{aligned}$$

这些return是相互依赖的，这四个式子很容易转为矩阵形式：

$$\underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{v}}=\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]+\left[\begin{array}{c}
\gamma v_{2} \\
\gamma v_{3} \\
\gamma v_{4} \\
\gamma v_{1}
\end{array}\right]=\underbrace{\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]}_{\mathbf{r}}+\gamma\left[\begin{array}{cccc}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0
\end{array}\right] \underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{P}}$$

简记为

$$ \mathbf{v} = \mathbf{r} + \gamma \mathbf{P} \mathbf{v} $$

这也就是一个简单的贝尔曼方程，它演示了一个核心思想：一个状态的价值依赖于其他状态的价值。

案例：考虑下面的例子每个state的value

![](/images/RL/16.png)

容易得到

$$ \begin{aligned}
v_1 & = 0 + \gamma v_3 \\
v_2 & = 1 + \gamma v_4 \\
v_3 & = 1 + \gamma v_4 \\
v_4 & = 1 + \gamma v_4 \\
\end{aligned}$$

为了解决这个式子，可以先计算$v_4$，再计算$v_1$、$v_2$、$v_3$。

##### 3.state value（状态价值）

###### 3.1 符号

考虑下面这个单步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} $$

在状态$S_t$选择动作$A_t$，得到了回报$R_{t+1}$，并且进入新的状态 $S_{t+1}$，注意$S_t$、$A_t$、$R_{t+1}$都是随机变量。

这个步骤需要考虑以下概率分布：

（1）在状态$S_t$选择动作$A_t$的policy：

$$ \pi(A_t = a \mid S_t = s) $$

（2）在状态$S_t$选择动作$A_t$后，获得的回报$R_{t+1}$：

$$ p(R_{t+1} = r \mid S_t = s, A_t = a) $$

（3）在状态$S_t$选择动作$A_t$后，进入下一个状态 $S_{t+1}$：

$$ p(S_{t+1} = s^{'} \mid S_t = s, A_t = a) $$

此时，我们假设所有概率分布已知。

将多个单步过程组合，便得到了多步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} \overset{A_{t+1}}{\rightarrow} R_{t+2},S_{t+2} \overset{A_{t+2}}{\rightarrow} R_{t+3},\dots $$

则discounted return（折扣回报）为

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots $$

其中，$\gamma \in \lbrack 0, 1)$是discounted factor（折扣因子），由于$R_t$是随机变量，所以$G_t$也是随机变量。

###### 3.2 state value（状态价值）

$G_t$的期望值被称为state-value function（状态价值函数）：

$$ v_{\pi}(s) = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack $$

- 这是一个条件期望，是关于状态s的函数，并且要求状态从s开始。

- 它基于策略$\pi$，对于不同的策略，state value可能不同。

- 它代表状态的“价值”。如果state value更大，那么策略更好，因为可以获得更大的累积奖励。

状态价值是从某个状态出发所能获得的所有可能回报的平均值。如果所有条件（$\pi(a \mid s),p(r \mid s,a),p(s' \mid s,a)$）都是确定性的，那么状态价值就等同于回报。
