---
title: 强化学习（2）Bellman Equation
tags: RL
typora-root-url: ./..
---

介绍贝尔曼方程（Bellman Equation）。

<!--more-->

##### 1.return的作用

回顾前一节的return：沿trajectory获得的discounted reward总和。

如果我们想要判断一个policy的好坏程度，return的重要性就体现出来了。

![](/images/RL/14.png)

假设起始点是$s_1$，终点是$s_4$。第一个policy和第二个policy从直觉上来看，第一个会比第二个好，因为第一个没有进入forbidden区域，但是直觉不一定对，所以需要数学来证明这件事。

对于第一个policy：

$$ \begin{aligned}
return_1 & =0+\gamma 1 + \gamma^2 1 + \dots \\
& =\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = \frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第二个policy：

$$ \begin{aligned}
return_2 & = -1+\gamma 1 + \gamma^2 1 + \dots \\
& =-1+\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = -1+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第三个policy：

$$ \begin{aligned}
return_3 & = 0.5(-1+\frac{\gamma}{1-\gamma})+0.5(\frac{\gamma}{1-\gamma}) \\
& = -0.5+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对比三个return：

$$ return_1 > return_3 > return_2 $$

上述不等式表明，第一个policy是最佳的，第二个policy是最差的，这与我们的直觉完全一致。

计算return对于评估policy非常重要。

##### 2.计算return

（1）定义法

![](/images/RL/15.png)

让$v_i$表示从$s_i$开始获得的return：

$$ \begin{aligned}
v_1 & = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots \\
v_2 & = r_2 + \gamma r_3 + \gamma^2 r_4 + \dots \\
v_3 & = r_3 + \gamma r_4 + \gamma^2 r_1 + \dots \\
v_4 & = r_4 + \gamma r_1 + \gamma^2 r_2 + \dots \\
\end{aligned}$$

（2）推导

对于$v_1$，如果从第二项到最后一项都提出一个$\gamma$：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)$$

可以发现式子中出现了$v_2$，即：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2$$

对于另外三个式子同理，于是可以得到新的公式：

$$ \begin{aligned}
v_1 & = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2 \\
v_2 & = r_2 + \gamma (r_3 + \gamma r_4 + \dots)=r_2+\gamma v_3 \\
v_3 & = r_3 + \gamma (r_4 + \gamma r_1 + \dots)=r_3+\gamma v_4 \\
v_4 & = r_4 + \gamma (r_1 + \gamma r_2 + \dots)=r_4+\gamma v_1 \\
\end{aligned}$$

这些return是相互依赖的，这四个式子很容易转为矩阵形式：

$$\underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{v}}=\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]+\left[\begin{array}{c}
\gamma v_{2} \\
\gamma v_{3} \\
\gamma v_{4} \\
\gamma v_{1}
\end{array}\right]=\underbrace{\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]}_{\mathbf{r}}+\gamma\left[\begin{array}{cccc}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0
\end{array}\right] \underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{P}}$$

简记为

$$ \mathbf{v} = \mathbf{r} + \gamma \mathbf{P} \mathbf{v} $$

这是简化的贝尔曼方程，它演示了一个核心思想：一个状态的价值依赖于其他状态的价值。

案例：考虑下面的例子每个state的value

![](/images/RL/16.png)

容易得到

$$ \begin{aligned}
v_1 & = 0 + \gamma v_3 \\
v_2 & = 1 + \gamma v_4 \\
v_3 & = 1 + \gamma v_4 \\
v_4 & = 1 + \gamma v_4 \\
\end{aligned}$$

为了解决这个式子，可以先计算$v_4$，再计算$v_1$、$v_2$、$v_3$。

##### 3.state value（状态价值）

###### 3.1 符号

考虑下面这个单步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} $$

在状态$S_t$选择动作$A_t$，得到了回报$R_{t+1}$，并且进入新的状态 $S_{t+1}$，注意$S_t$、$A_t$、$R_{t+1}$都是随机变量。

这个步骤需要考虑以下概率分布：

（1）在状态$S_t$选择动作$A_t$的policy：

$$ \pi(A_t = a \mid S_t = s) $$

（2）在状态$S_t$选择动作$A_t$后，获得的回报$R_{t+1}$：

$$ p(R_{t+1} = r \mid S_t = s, A_t = a) $$

（3）在状态$S_t$选择动作$A_t$后，进入下一个状态 $S_{t+1}$：

$$ p(S_{t+1} = s' \mid S_t = s, A_t = a) $$

此时，我们假设所有概率分布已知。

将多个单步过程组合，便得到了多步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} \overset{A_{t+1}}{\rightarrow} R_{t+2},S_{t+2} \overset{A_{t+2}}{\rightarrow} R_{t+3},\dots $$

则discounted return（折扣回报）为

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots $$

其中，$\gamma \in \lbrack 0, 1)$是discounted factor（折扣因子），由于$R_t$是随机变量，所以$G_t$也是随机变量。

###### 3.2 state value（状态价值）

$G_t$的期望值被称为state-value function（状态价值函数）：

$$ v_{\pi}(s) = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack $$

- 这是一个条件期望，是关于状态s的函数，并且要求状态从s开始。

- 它基于策略$\pi$，对于不同的策略，state value可能不同。

- 它代表状态的“价值”。如果state value更大，那么策略更好，因为可以获得更大的累积奖励。

状态价值是从某个状态出发所能获得的所有可能回报的平均值。如果所有条件（$\pi(a \mid s),p(r \mid s,a),p(s' \mid s,a)$）都是确定性的，那么state value就等同于return。

###### 3.3 案例

![](/images/RL/17.png)

在第一节的例子中，由于所有条件都是确定的，所以此时state value与return相同：

$$\begin{array}{l}
v_{\pi_{1}}\left(s_{1}\right)=0+\gamma 1+\gamma^{2} 1+\cdots=\gamma\left(1+\gamma+\gamma^{2}+\ldots\right)=\frac{\gamma}{1-\gamma} \\
v_{\pi_{2}}\left(s_{1}\right)=-1+\gamma 1+\gamma^{2} 1+\cdots=-1+\gamma\left(1+\gamma+\gamma^{2}+\ldots\right)=-1+\frac{\gamma}{1-\gamma} \\
v_{\pi_{3}}\left(s_{1}\right)=0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right)=-0.5+\frac{\gamma}{1-\gamma}
\end{array}$$

##### 4.Bellman equation（贝尔曼方程）

贝尔曼方程描述了所有state value之间的关系，可以用贝尔曼方程来计算state value。

###### 4.1 推导

对于一个随机的trajectory：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} \overset{A_{t+1}}{\rightarrow} R_{t+2},S_{t+2} \overset{A_{t+2}}{\rightarrow} R_{t+3},\dots $$

折扣回报$G_t$为：

$$ \begin{aligned}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\
& = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
& = R_{t+1} + \gamma G_{t+1}
\end{aligned}$$

根据state value的定义，可以得到：

$$ \begin{aligned}
v_{\pi}(s) & = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack \\
& = \mathbb{E} \lbrack R_{t+1} + \gamma G_{t+1} \mid S_t = s \rbrack \\
& = \mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack + \gamma \mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack
\end{aligned}$$

对于第一项，它是即时reward的平均值

$$ \begin{aligned}
\mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack & = \sum_a \pi(a \mid s) \mathbb{E} \lbrack R_{t+1} \mid S_t = s,A_t = a \rbrack \\
& = \sum_a \pi(a \mid s) \sum_r p(r \mid s,a) r
\end{aligned}$$

注：当前状态下获得回报的期望，等价于每种策略（在当前状态选择某个行动）下获得回报期望的乘积之和；当前状态选择某个行动获得的回报期望，等价于当前状态选择某个行动后的所有可能结果与相应回报的乘积之和。

对于第二项，它是未来回报的平均值

$$ \begin{aligned}
\mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack & = \sum_{s'} \mathbb{E} \lbrack G_{t+1} \mid S_t = s,S_{t+1} = s' \rbrack p(s' \mid s) \\
& = \sum_{s'} \mathbb{E} \lbrack G_{t+1} \mid S_{t+1} = s' \rbrack p(s' \mid s) \\
& = \sum_{s'} v_{\pi}(s') p(s' \mid s) \\
& = \sum_{s'} v_{\pi}(s') \sum_a p(s' \mid s,a) \pi(a \mid s)
\end{aligned}$$

第一步到第二步，是因为马尔科夫过程具有无记忆性，第二步到第三步是state value的定义，第三步到第四步是一个简单的概率计算。

综上，我们能得到一个更具体的公式

$$ \begin{aligned}
v_{\pi}(s) & = \mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack + \gamma \mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack \\
& = \sum_a \pi(a \mid s) \sum_r p(r \mid s,a) r + \gamma \sum_a \pi(a \mid s) \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \\
& = \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack, \forall s \in S
\end{aligned}$$

其中：

- $v_{\pi}(s)$和$v_{\pi}(s')$是需要被计算的state value。

- $\pi (a \mid s) $是一个给定的policy，求解方程被称为策略评估。

- $p(r \mid s,a)$和$p(s' \mid s,a)$代表动态模型。

上述方程称为贝尔曼方程，它描述了不同状态之间的状态值函数关系。其由两个部分组成：即时奖励项和未来奖励项。每个状态都有类似的方程！！！

###### 4.2 案例

![](/images/RL/16.png)

$$ v_{\pi}(s) = \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack $$

这是个简单的案例，因为策略都是确定的。

考虑在状态$s_1$的state value：

$\pi(a=a_3 \mid s_1) = 1$ 并且 $\pi(a \ne a_3 \mid s_1) = 0$

$ p(s' = s_3 \mid s_1, a_3) = 1 $ 并且 $ p(s' \ne s_3 \mid s_1, a_3) = 0 $ 

$ p(r=0 \mid s_1, a_3) = 1$ 并且 $ p(r \ne 0 \mid s_1, a_3) = 0$ 

代入贝尔曼方程，可以得到

$$v_{\pi}(s_1)  = 0 + \gamma v_{\pi}(s_3) $$

类似地，可以获得其他state的state value

$$ \begin{aligned}
v_{\pi}(s_1) &= 0 + \gamma v_{\pi}(s_3) \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4)
\end{aligned}$$

这与第二节推导的数学公式一模一样。

解决这个问题的方法是，从后向前求解

$$ \begin{aligned}
v_{\pi}(s_4) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_3) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_2) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_1) &= \frac{\gamma}{1-\gamma}
\end{aligned}$$

当$\gamma$确定后，所有state value都能求解得到。计算出state value之后，需要计算action value并改进策略。

再举个案例，作为练习：

![](/images/RL/18.png)

对于这个例子，根据贝尔曼方程计算state value，并对比上一个案例。

$$ \begin{aligned}
v_{\pi}(s_1) &= 0.5(0 + \gamma v_{\pi}(s_3)) + 0.5(-1 + \gamma v_{\pi}(s_2)) \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4)
\end{aligned}$$

从后往前计算求解

$$ \begin{aligned}
v_{\pi}(s_4) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_3) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_2) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_1) &= -0.5 + \frac{\gamma}{1-\gamma}
\end{aligned}$$

很明显，这一个policy比上一个policy的结果更坏。

###### 4.3 Bellman equation矩阵形式

对于贝尔曼方程，每个未知数依赖于另一个未知数，对于s个状态，拥有s个方程，矩阵形式能更好地表达所有方程之间的关系。

将贝尔曼方程写成简洁模式：

$$ v_{\pi}(s) = \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack = r_{\pi}(s) + \gamma \sum_{s'} p_{\pi}(s' \mid s) v_{\pi}(s') $$

其中

$$ r_{\pi}(s) \triangleq \sum_{a} \pi(a \mid s) \sum_{r} p(r \mid s, a) r, \qquad p_{\pi}(s' \mid s) \triangleq \sum_{a} \pi(a \mid s) p(s' \mid s, a) $$

假设状态为$s_i(i=1,\dots,n)$，对于每一state，贝尔曼方程为

$$ v_{\pi}(s_i) = r_{\pi}(s_i) + \gamma \sum_{s_j} p_{\pi}(s_j \mid s_i) v_{\pi}(s_j) $$

将所有状态方程组合起来，写成矩阵-向量形式：

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

如果有四个状态，则可以展开为

$$ \underbrace{\begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}}_{v_{\pi}} = \underbrace{\begin{bmatrix} r_{\pi}(s_1) \\ r_{\pi}(s_2) \\ r_{\pi}(s_3) \\ r_{\pi}(s_4) \end{bmatrix}}_{r_{\pi}} + \gamma \underbrace{\begin{bmatrix} p_{\pi}(s_1 \mid s_1) & p_{\pi}(s_2 \mid s_1) & p_{\pi}(s_3 \mid s_1) & p_{\pi}(s_4 \mid s_1) \\ p_{\pi}(s_1 \mid s_2) & p_{\pi}(s_2 \mid s_2) & p_{\pi}(s_3 \mid s_2) & p_{\pi}(s_4 \mid s_2) \\ p_{\pi}(s_1 \mid s_3) & p_{\pi}(s_2 \mid s_3) & p_{\pi}(s_3 \mid s_3) & p_{\pi}(s_4 \mid s_3) \\ p_{\pi}(s_1 \mid s_4) & p_{\pi}(s_2 \mid s_4) & p_{\pi}(s_3 \mid s_4) & p_{\pi}(s_4 \mid s_4) \end{bmatrix}}_{P_{\pi}} \underbrace{\begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}}_{v_{\pi}}$$

![](/images/RL/16.png)

对于第一个例子，可写为

$$ \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \end{bmatrix} + \gamma \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}$$

![](/images/RL/18.png)

对于第二个例子，可写为

$$ \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix} = \begin{bmatrix} 0.5(0)+0.5(-1) \\ 1 \\ 1 \\ 1 \end{bmatrix} + \gamma \begin{bmatrix} 0 & 0.5 & 0.5 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}$$

###### 4.4 求解Bellman equation

给定一个policy，找出对应的对应state value称为策略评估，这是强化学习中的一个基础问题，它是寻找更好policy的基础。

对于贝尔曼方程

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

它的闭式解为

$$ v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi} $$

当然，我们可以避免矩阵的逆运算，通过迭代的方法

$$ v_{k+1} = r_{\pi} + \gamma P_{\pi} v_{k} $$

这个算法将生成序列$\lbrace v_0,v_1,v_2,\dots \rbrace$，可以证明，当k趋近无穷时，迭代解趋近闭式解，即

$$ v_{k} \to v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi} $$

![](/images/RL/19.png)

##### 5.action value（行动价值）

###### 5.1 介绍

从state value到action value：

- state value：智能体从某个状态出发所能获得的平均回报。

- action value：智能体从某个状态出发并采取特定行动后所能获得的平均回报。

关注action value是为了判断哪种行动更优。

定义：

$$ q_{\pi}(s,a) = \mathbb{E} \lbrack G_t \mid S_t = s,A_t=a \rbrack $$

$ q_{\pi}(s,a)$是一个状态-行动对，依赖于策略$\pi$。

根据条件期望的性质

$$\underbrace{\mathbb{E}[G_t  \mid  S_t = s]}_{v_{\pi}(s)} = \sum_{a} \underbrace{\mathbb{E}[G_t  \mid  S_t = s, A_t = a]}_{q_{\pi}(s, a)} \pi(a  \mid  s)$$

因此

$$ v_{\pi}(s) = \sum_a \pi(a \mid s) q_{\pi}(s,a) $$

这个式子展示了如何从action value中获取state value。

回顾state value函数

$$v_{\pi}(s) = \sum_{a} \pi(a \mid s) \underbrace{\lbrack \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack}_{q_{\pi}(s,a)}$$

可以得到

$$ q_{\pi}(s,a) = \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') $$

这个式子展示了如何从state value中获取action value。

###### 5.2 推导

![](/images/RL/20.png)

对于这个例子，可以尝试写出它的action value

$$\begin{align}
q_{\pi}(s_1, a_1) &= -1 + \gamma v_{\pi}(s_1) \\
q_{\pi}(s_1, a_2) &= -1 + \gamma v_{\pi}(s_2) \\
q_{\pi}(s_1, a_3) &= 0 + \gamma v_{\pi}(s_3) \\
q_{\pi}(s_1, a_4) &= -1 + \gamma v_{\pi}(s_1) \\
q_{\pi}(s_1, a_5) &= 0 + \gamma v_{\pi}(s_1)
\end{align}$$

action value很重要，因为我们关心要采取哪个动作。我们可以先计算所有state value，然后计算action value。我们也可以直接计算action value，无论是否使用模型。

##### 6.总结

本文主要介绍了state value，action value以及Bellman equation。

state value：

$$ v_{\pi}(s) = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack $$

action value：

$$ q_{\pi}(s,a) = \mathbb{E} \lbrack G_t \mid S_t = s,A_t=a \rbrack $$

Bellman equation元素形式：

$$\begin{align}
v_{\pi}(s) &= \sum_{a} \pi(a \mid s) \underbrace{\lbrack \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack}_{q_{\pi}(s,a)} \\
&= \sum_a \pi(a \mid s) q_{\pi}(s,a) 
\end{align}$$

Bellman equation矩阵形式：

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

state value是状态的价值，用来判断某个policy下，状态的好坏，action value用来判断相同状态下不同行动的好坏，贝尔曼方程用来计算state value，state value和action value可以相互转换。

贝尔曼方程可以用闭式解或者迭代解求解。
