---
title: 强化学习（2）贝尔曼方程——基础第二节
tags: RL
typora-root-url: ./..
---

介绍状态价值、行动价值、贝尔曼方程（Bellman Equation）。

<!--more-->

注：本文以赵世钰课程为基础，对知识进行总结，这是第二节，一共10节。

##### 1.return的作用

回顾前一节的return：沿trajectory获得的discounted reward总和。

如果我们想要判断一个policy的好坏程度，return的重要性就体现出来了。

![](/images/RL/14.png)

假设起始点是$s_1$，终点是$s_4$。第一个policy和第二个policy从直觉上来看，第一个会比第二个好，因为第一个没有进入forbidden区域，但是直觉不一定对，所以需要数学来证明这件事。

对于第一个policy：

$$ \begin{aligned}
return_1 & =0+\gamma 1 + \gamma^2 1 + \dots \\
& =\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = \frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第二个policy：

$$ \begin{aligned}
return_2 & = -1+\gamma 1 + \gamma^2 1 + \dots \\
& =-1+\gamma (1+\gamma+\gamma^{2}+ \dots ) \\
& = -1+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对于第三个policy：

$$ \begin{aligned}
return_3 & = 0.5(-1+\frac{\gamma}{1-\gamma})+0.5(\frac{\gamma}{1-\gamma}) \\
& = -0.5+\frac{\gamma}{1-\gamma} 
\end{aligned}$$

对比三个return：

$$ return_1 > return_3 > return_2 $$

上述不等式表明，第一个policy是最佳的，第二个policy是最差的，这与我们的直觉完全一致。

计算return对于评估policy非常重要。

##### 2.计算return

（1）定义法

![](/images/RL/15.png)

让$v_i$表示从$s_i$开始获得的return：

$$ \begin{aligned}
v_1 & = r_1 + \gamma r_2 + \gamma^2 r_3 + \dots \\
v_2 & = r_2 + \gamma r_3 + \gamma^2 r_4 + \dots \\
v_3 & = r_3 + \gamma r_4 + \gamma^2 r_1 + \dots \\
v_4 & = r_4 + \gamma r_1 + \gamma^2 r_2 + \dots \\
\end{aligned}$$

（2）推导

对于$v_1$，如果从第二项到最后一项都提出一个$\gamma$：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)$$

可以发现式子中出现了$v_2$，即：

$$v_1 = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2$$

对于另外三个式子同理，于是可以得到新的公式：

$$ \begin{aligned}
v_1 & = r_1 + \gamma (r_2 + \gamma r_3 + \dots)=r_1+\gamma v_2 \\
v_2 & = r_2 + \gamma (r_3 + \gamma r_4 + \dots)=r_2+\gamma v_3 \\
v_3 & = r_3 + \gamma (r_4 + \gamma r_1 + \dots)=r_3+\gamma v_4 \\
v_4 & = r_4 + \gamma (r_1 + \gamma r_2 + \dots)=r_4+\gamma v_1 \\
\end{aligned}$$

这些return是相互依赖的，这四个式子很容易转为矩阵形式：

$$\underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{v}}=\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]+\left[\begin{array}{c}
\gamma v_{2} \\
\gamma v_{3} \\
\gamma v_{4} \\
\gamma v_{1}
\end{array}\right]=\underbrace{\left[\begin{array}{c}
r_{1} \\
r_{2} \\
r_{3} \\
r_{4}
\end{array}\right]}_{\mathbf{r}}+\gamma\left[\begin{array}{cccc}
0 & 1 & 0 & 0 \\
0 & 0 & 1 & 0 \\
0 & 0 & 0 & 1 \\
1 & 0 & 0 & 0
\end{array}\right] \underbrace{\left[\begin{array}{c}
v_{1} \\
v_{2} \\
v_{3} \\
v_{4}
\end{array}\right]}_{\mathbf{P}}$$

简记为

$$ \mathbf{v} = \mathbf{r} + \gamma \mathbf{P} \mathbf{v} $$

这是简化的贝尔曼方程，它演示了一个核心思想：一个状态的价值依赖于其他状态的价值。

案例：考虑下面的例子每个state的value

![](/images/RL/16.png)

容易得到

$$ \begin{aligned}
v_1 & = 0 + \gamma v_3 \\
v_2 & = 1 + \gamma v_4 \\
v_3 & = 1 + \gamma v_4 \\
v_4 & = 1 + \gamma v_4 \\
\end{aligned}$$

为了解决这个式子，可以先计算$v_4$，再计算$v_1$、$v_2$、$v_3$。

##### 3.state value（状态价值）

###### 3.1 符号

考虑下面这个单步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} $$

在状态$S_t$选择动作$A_t$，得到了奖励$R_{t+1}$，并且进入新的状态 $S_{t+1}$，注意$S_t$、$A_t$、$R_{t+1}$都是随机变量。

这个步骤需要考虑以下概率分布：

（1）在状态$S_t$选择动作$A_t$的policy：

$$ \pi(A_t = a \mid S_t = s) $$

（2）在状态$S_t$选择动作$A_t$后，获得的奖励$R_{t+1}$：

$$ p(R_{t+1} = r \mid S_t = s, A_t = a) $$

（3）在状态$S_t$选择动作$A_t$后，进入下一个状态 $S_{t+1}$：

$$ p(S_{t+1} = s' \mid S_t = s, A_t = a) $$

此时，我们假设所有概率分布已知。

将多个单步过程组合，便得到了多步过程：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} \overset{A_{t+1}}{\rightarrow} R_{t+2},S_{t+2} \overset{A_{t+2}}{\rightarrow} R_{t+3},\dots $$

则discounted return（折扣回报）为

$$ G_t = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots $$

其中，$\gamma \in \lbrack 0, 1)$是discounted factor（折扣因子），由于$R_t$是随机变量，所以$G_t$也是随机变量。

###### 3.2 state value（状态价值）

$G_t$的期望值被称为state-value function（状态价值函数）：

$$ v_{\pi}(s) = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack $$

- 这是一个条件期望，是关于状态s的函数，并且要求状态从s开始。

- 它基于策略$\pi$，对于不同的策略，state value可能不同。

- 它代表状态的“价值”。如果state value更大，那么策略更好，因为可以获得更大的累积奖励。

状态价值是从某个状态出发所能获得的所有可能回报的平均值。如果所有条件（$\pi(a \mid s),p(r \mid s,a),p(s' \mid s,a)$）都是确定性的，那么state value就等同于return。

###### 3.3 案例

![](/images/RL/17.png)

在第一节的例子中，由于所有条件都是确定的，所以此时state value与return相同：

$$\begin{array}{l}
v_{\pi_{1}}\left(s_{1}\right)=0+\gamma 1+\gamma^{2} 1+\cdots=\gamma\left(1+\gamma+\gamma^{2}+\ldots\right)=\frac{\gamma}{1-\gamma} \\
v_{\pi_{2}}\left(s_{1}\right)=-1+\gamma 1+\gamma^{2} 1+\cdots=-1+\gamma\left(1+\gamma+\gamma^{2}+\ldots\right)=-1+\frac{\gamma}{1-\gamma} \\
v_{\pi_{3}}\left(s_{1}\right)=0.5\left(-1+\frac{\gamma}{1-\gamma}\right)+0.5\left(\frac{\gamma}{1-\gamma}\right)=-0.5+\frac{\gamma}{1-\gamma}
\end{array}$$

##### 4.Bellman equation（贝尔曼方程）

贝尔曼方程描述了所有state value之间的关系，可以用贝尔曼方程来计算state value。

###### 4.1 推导

对于一个随机的trajectory：

$$ S_t \overset{A_t}{\rightarrow} R_{t+1},S_{t+1} \overset{A_{t+1}}{\rightarrow} R_{t+2},S_{t+2} \overset{A_{t+2}}{\rightarrow} R_{t+3},\dots $$

回报$G_t$为：

$$ \begin{aligned}
G_t & = R_{t+1} + \gamma R_{t+2} + \gamma^2 R_{t+3} + \dots \\
& = R_{t+1} + \gamma (R_{t+2} + \gamma R_{t+3} + \dots) \\
& = R_{t+1} + \gamma G_{t+1}
\end{aligned}$$

根据state value的定义，可以得到：

$$ \begin{aligned}
v_{\pi}(s) & = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack \\
& = \mathbb{E} \lbrack R_{t+1} + \gamma G_{t+1} \mid S_t = s \rbrack \\
& = \mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack + \gamma \mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack
\end{aligned}$$

对于第一项，它是即时reward的平均值

$$ \begin{aligned}
\mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack & = \sum_a \pi(a \mid s) \mathbb{E} \lbrack R_{t+1} \mid S_t = s,A_t = a \rbrack \\
& = \sum_a \pi(a \mid s) \sum_r p(r \mid s,a) r
\end{aligned}$$

注：当前状态下获得reward的期望，等价于每种策略（在当前状态选择某个行动）下与reward期望的乘积之和；当前状态选择某个行动获得的reward期望，等价于每种状态下选择某个行动的概率与相应reward的乘积之和。

对于第二项，它是未来reward的平均值

$$ \begin{aligned}
\mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack & = \sum_{s'} \mathbb{E} \lbrack G_{t+1} \mid S_t = s,S_{t+1} = s' \rbrack p(s' \mid s) \\
& = \sum_{s'} \mathbb{E} \lbrack G_{t+1} \mid S_{t+1} = s' \rbrack p(s' \mid s) \\
& = \sum_{s'} v_{\pi}(s') p(s' \mid s) \\
& = \sum_{s'} v_{\pi}(s') \sum_a p(s' \mid s,a) \pi(a \mid s)
\end{aligned}$$

第一步到第二步，是因为马尔科夫过程具有无记忆性，第二步到第三步是state value的定义，第三步到第四步是一个简单的概率计算。

综上，我们能得到一个更具体的公式

$$ \begin{aligned}
v_{\pi}(s) & = \mathbb{E} \lbrack R_{t+1} \mid S_t = s \rbrack + \gamma \mathbb{E} \lbrack G_{t+1} \mid S_t = s \rbrack \\
& = \sum_a \pi(a \mid s) \sum_r p(r \mid s,a) r + \gamma \sum_a \pi(a \mid s) \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \\
& = \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack, \forall s \in S
\end{aligned}$$

其中：

- $v_{\pi}(s)$和$v_{\pi}(s')$是需要被计算的state value。

- $\pi (a \mid s) $是一个给定的policy，求解方程被称为策略评估。

- $p(r \mid s,a)$和$p(s' \mid s,a)$代表动态模型。

上述方程称为贝尔曼方程，它描述了不同状态之间的状态价值函数关系。其由两个部分组成：即时奖励和未来奖励。每个状态都有类似的方程！！！

###### 4.2 案例

![](/images/RL/16.png)

$$ v_{\pi}(s) = \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack $$

这是个简单的案例，因为策略都是确定的。

考虑在状态$s_1$的state value：

$\pi(a=a_3 \mid s_1) = 1$ 并且 $\pi(a \ne a_3 \mid s_1) = 0$

$ p(s' = s_3 \mid s_1, a_3) = 1 $ 并且 $ p(s' \ne s_3 \mid s_1, a_3) = 0 $ 

$ p(r=0 \mid s_1, a_3) = 1$ 并且 $ p(r \ne 0 \mid s_1, a_3) = 0$ 

代入贝尔曼方程，可以得到

$$v_{\pi}(s_1)  = 0 + \gamma v_{\pi}(s_3) $$

类似地，可以获得其他state的state value

$$ \begin{aligned}
v_{\pi}(s_1) &= 0 + \gamma v_{\pi}(s_3) \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4)
\end{aligned}$$

这与第二节推导的数学公式一模一样。

解决这个问题的方法是，从后向前求解

$$ \begin{aligned}
v_{\pi}(s_4) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_3) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_2) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_1) &= \frac{\gamma}{1-\gamma}
\end{aligned}$$

当$\gamma$确定后，所有state value都能求解得到。计算出state value之后，需要计算action value并改进策略。

再举个案例，作为练习：

![](/images/RL/18.png)

对于这个例子，根据贝尔曼方程计算state value，并对比上一个案例。

$$ \begin{aligned}
v_{\pi}(s_1) &= 0.5(0 + \gamma v_{\pi}(s_3)) + 0.5(-1 + \gamma v_{\pi}(s_2)) \\
v_{\pi}(s_2) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_3) &= 1 + \gamma v_{\pi}(s_4) \\
v_{\pi}(s_4) &= 1 + \gamma v_{\pi}(s_4)
\end{aligned}$$

从后往前计算求解

$$ \begin{aligned}
v_{\pi}(s_4) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_3) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_2) &= \frac{1}{1-\gamma} \\
v_{\pi}(s_1) &= -0.5 + \frac{\gamma}{1-\gamma}
\end{aligned}$$

很明显，这一个policy比上一个policy的结果更坏。

###### 4.3 Bellman equation矩阵形式

对于贝尔曼方程，每个未知数依赖于另一个未知数，对于s个状态，拥有s个方程，矩阵形式能更好地表达所有方程之间的关系。

将贝尔曼方程写成简洁模式：

$$ \begin{aligned}
v_{\pi}(s) &= \sum_a \pi(a \mid s) \lbrack \sum_r p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack \\
&= r_{\pi}(s) + \gamma \sum_{s'} p_{\pi}(s' \mid s) v_{\pi}(s')
\end{aligned}$$

其中

$$ r_{\pi}(s) \triangleq \sum_{a} \pi(a \mid s) \sum_{r} p(r \mid s, a) r, \qquad p_{\pi}(s' \mid s) \triangleq \sum_{a} \pi(a \mid s) p(s' \mid s, a) $$

假设状态为$s_i(i=1,\dots,n)$，对于每一state，贝尔曼方程为

$$ v_{\pi}(s_i) = r_{\pi}(s_i) + \gamma \sum_{s_j} p_{\pi}(s_j \mid s_i) v_{\pi}(s_j) $$

将所有状态方程组合起来，写成矩阵-向量形式：

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

如果有四个状态，则可以展开为

$$ \underbrace{\begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}}_{v_{\pi}} = \underbrace{\begin{bmatrix} r_{\pi}(s_1) \\ r_{\pi}(s_2) \\ r_{\pi}(s_3) \\ r_{\pi}(s_4) \end{bmatrix}}_{r_{\pi}} + \gamma \underbrace{\begin{bmatrix} p_{\pi}(s_1 \mid s_1) & p_{\pi}(s_2 \mid s_1) & p_{\pi}(s_3 \mid s_1) & p_{\pi}(s_4 \mid s_1) \\ p_{\pi}(s_1 \mid s_2) & p_{\pi}(s_2 \mid s_2) & p_{\pi}(s_3 \mid s_2) & p_{\pi}(s_4 \mid s_2) \\ p_{\pi}(s_1 \mid s_3) & p_{\pi}(s_2 \mid s_3) & p_{\pi}(s_3 \mid s_3) & p_{\pi}(s_4 \mid s_3) \\ p_{\pi}(s_1 \mid s_4) & p_{\pi}(s_2 \mid s_4) & p_{\pi}(s_3 \mid s_4) & p_{\pi}(s_4 \mid s_4) \end{bmatrix}}_{P_{\pi}} \underbrace{\begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}}_{v_{\pi}}$$

![](/images/RL/16.png)

对于第一个例子，可写为

$$ \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix} = \begin{bmatrix} 0 \\ 1 \\ 1 \\ 1 \end{bmatrix} + \gamma \begin{bmatrix} 0 & 0 & 1 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}$$

![](/images/RL/18.png)

对于第二个例子，可写为

$$ \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix} = \begin{bmatrix} 0.5(0)+0.5(-1) \\ 1 \\ 1 \\ 1 \end{bmatrix} + \gamma \begin{bmatrix} 0 & 0.5 & 0.5 & 0 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \\ 0 & 0 & 0 & 1 \end{bmatrix} \begin{bmatrix} v_{\pi}(s_1) \\ v_{\pi}(s_2) \\ v_{\pi}(s_3) \\ v_{\pi}(s_4) \end{bmatrix}$$

###### 4.4 求解Bellman equation

给定一个policy，找出对应的state value称为策略评估，这是强化学习中的一个基础问题，它是寻找更好policy的基础。

对于贝尔曼方程

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

它的闭式解为

$$ v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi} $$

当然，我们可以避免矩阵的逆运算，通过迭代的方法

$$ v_{k+1} = r_{\pi} + \gamma P_{\pi} v_{k} $$

这个算法将生成序列$\lbrace v_0,v_1,v_2,\dots \rbrace$，可以证明，当k趋近无穷时，迭代解趋近闭式解，即

$$ v_{k} \to v_{\pi} = (I - \gamma P_{\pi})^{-1} r_{\pi} $$

![](/images/RL/19.png)

##### 5.action value（行动价值）

###### 5.1 介绍

从state value到action value：

- state value：智能体从某个状态出发所能获得的平均回报。

- action value：智能体从某个状态出发并采取特定行动后所能获得的平均回报。

关注action value是为了判断哪种行动更优。

定义：

$$ q_{\pi}(s,a) = \mathbb{E} \lbrack G_t \mid S_t = s,A_t=a \rbrack $$

$ q_{\pi}(s,a)$是一个状态-行动对，依赖于策略$\pi$。

根据条件期望的性质

$$\underbrace{\mathbb{E}[G_t  \mid  S_t = s]}_{v_{\pi}(s)} = \sum_{a} \underbrace{\mathbb{E}[G_t  \mid  S_t = s, A_t = a]}_{q_{\pi}(s, a)} \pi(a  \mid  s)$$

因此

$$ v_{\pi}(s) = \sum_a \pi(a \mid s) q_{\pi}(s,a) $$

这个式子展示了如何从action value中获取state value。

回顾state value函数

$$v_{\pi}(s) = \sum_{a} \pi(a \mid s) \underbrace{\lbrack \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack}_{q_{\pi}(s,a)}$$

可以得到

$$ q_{\pi}(s,a) = \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') $$

这个式子展示了如何从state value中获取action value。

###### 5.2 推导

![](/images/RL/20.png)

对于这个例子，可以尝试写出它的action value

$$\begin{align}
q_{\pi}(s_1, a_1) &= -1 + \gamma v_{\pi}(s_1) \\
q_{\pi}(s_1, a_2) &= -1 + \gamma v_{\pi}(s_2) \\
q_{\pi}(s_1, a_3) &= 0 + \gamma v_{\pi}(s_3) \\
q_{\pi}(s_1, a_4) &= -1 + \gamma v_{\pi}(s_1) \\
q_{\pi}(s_1, a_5) &= 0 + \gamma v_{\pi}(s_1)
\end{align}$$

action value很重要，因为我们关心要采取哪个动作。我们可以先计算所有state value，然后计算action value。我们也可以直接计算action value，无论是否使用模型。

##### 6.总结

本文主要介绍了state value，action value以及Bellman equation。

state value：

$$ v_{\pi}(s) = \mathbb{E} \lbrack G_t \mid S_t = s \rbrack $$

action value：

$$ q_{\pi}(s,a) = \mathbb{E} \lbrack G_t \mid S_t = s,A_t=a \rbrack $$

Bellman equation元素形式：

$$\begin{align}
v_{\pi}(s) &= \sum_{a} \pi(a \mid s) \underbrace{\lbrack \sum_{r} p(r \mid s,a) r + \gamma \sum_{s'} p(s' \mid s,a) v_{\pi}(s') \rbrack}_{q_{\pi}(s,a)} \\
&= \sum_a \pi(a \mid s) q_{\pi}(s,a) 
\end{align}$$

Bellman equation矩阵形式：

$$ v_{\pi} = r_{\pi} + \gamma P_{\pi} v_{\pi} $$

state value是状态的价值，用来判断某个policy下，状态的好坏，action value用来判断相同状态下不同行动的好坏，贝尔曼方程用来计算state value，state value和action value可以相互转换。

贝尔曼方程可以用闭式解或者迭代解求解。
