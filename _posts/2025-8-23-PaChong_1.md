---
title: 爬虫（1)静态网页信息获取
tags: WebCrawler
typora-root-url: ./..
---

爬虫入门教学，案例：豆瓣数据top250。

<!--more-->

##### 1.安装库函数

requests用来获取网页信息，bs4用来处理获得的网页信息并结构化。

~~~
pip install requests
pip install bs4
~~~

##### 2.小试牛刀

使用爬虫专用的练习网站简单尝试。

~~~
import requests
from bs4 import BeautifulSoup

# url存放想要爬取内容的网页
url = "http://books.toscrape.com/"

# 使用get获取网页信息
response = requests.get(url)

# 输出状态码，200则为成功
print(response.status_code)
~~~

状态为200的时候，再进行下一步，获取网页信息。

~~~
# ok为true，则获取成功，用text获取网页结构
if response.ok:
	content = response.text
	print(content)
	
# 网页信息结构化
soup = BeautifulSoup(content, "html.parser")
print(soup)
~~~

如果想获取更加具体的内容，比如说价格，则需要知道位于网页的结构的哪个部分，同时知道有什么特征，知道怎么捕捉。在网页页面空白处鼠标右键，选择最后一个检查，就可以看见网页结构了。

![](/images/WebCrawler/1.png)

~~~
# 使用find_all找到p标签下class=price_color的数据，这里对应价格
all_price = soup.find_all("p", attrs={"class": "price_color"})

prices = []
for i in all_price:
	# 去除html布局，提取内容
	prices.append(i.string)
	
print(prices)
~~~

![](/images/WebCrawler/2.png)

如果只想要数字，也可以继续处理，这里不做演示。

##### 3.爬取豆瓣top250数据

###### 3.1 对单页面爬取

首先，还是一样，先使用resquests看看是否能访问。

~~~
import requests
from bs4 import BeautifulSoup

# url存放想要爬取内容的网页
url = "https://movie.douban.com/top250"

# 使用get获取网页信息
response = requests.get(url)

# 输出状态码，200则为成功
print(response.status_code)
~~~

运行这个代码，会发现状态码是418，也就是说网页监测到你不是网页发送的请求，而是程序发送请求，被拒绝访问了，只要是4开头的状态码，都是拒绝访问。

这时就要去学习怎么把自己的程序伪装成网页访问，通常使用headers标头，假装你是个网页。

~~~
# 使用User-Agent，模拟浏览器或客户端身份，避免被识别为爬虫
headers = {
	"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0"
}
~~~

在网页页面空白处鼠标右键，选择最后一个检查，点击网络，之后刷新页面，右侧列表随便点击一个，最下面就有"User-Agent"信息，将信息复制过来即可。

![](/images/WebCrawler/3.png)

设置标头后，再尝试访问。

~~~
import requests
from bs4 import BeautifulSoup

# 使用User-Agent，模拟浏览器或客户端身份，避免被识别为爬虫
headers = {
	"User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0"
}

# url存放想要爬取内容的网页
url = "https://movie.douban.com/top250"

# 使用get获取网页信息
response = requests.get(url, headers = headers)

# 输出状态码，200则为成功
print(response.status_code)
~~~

访问成功后，还是一样的，获取信息，然后结构化，找到我们需要的内容，这里以获取标题为例，在span标签下class=title里面。

![](/images/WebCrawler/4.png)

~~~
content = response.text
soup = BeautifulSoup(content, "html.parser")

all_titles = soup.find_all("span", attrs={"class": "title"})

for title in all_titles:
	print(title.string)
~~~

![](/images/WebCrawler/5.png)

如果只想获取中文，可以加条件，如果都需要，则要进行优化，这里以只获取中文为例。

~~~
title_name = []
for title in all_titles:
    if "/" not in title.string:
        title_name.append(title.string)
print(title_name)
~~~

![](/images/WebCrawler/6.png)

这样，一个页面的标题就全部获取了，现在需要观察页面网页链接的关系。

![](/images/WebCrawler/7.png)

![](/images/WebCrawler/8.png)

发现这些页面的链接只有start不一样，说明他们是按顺序的网页，这里就可以直接用一个循环解决。

~~~
import requests
from bs4 import BeautifulSoup

headers = {
    "User-Agent": "Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/139.0.0.0 Safari/537.36 Edg/139.0.0.0"
}

title_name = []
for i in range(0,250,25):
    
    url = "https://movie.douban.com/top250?start=" + str(i) + "&filter="
    
    response = requests.get(url, headers = headers)
    content = response.text
    soup = BeautifulSoup(content, "html.parser")
    
    all_titles = soup.find_all("span", attrs={"class": "title"})

    for title in all_titles:
        title_string = title.string
        if "/" not in title_string:
            title_name.append(title_string)
print(title_name)
~~~

如果想要获取其他信息，也是类似的。

对于所有的静态网页页面，都可以采取上述方式获取信息。
