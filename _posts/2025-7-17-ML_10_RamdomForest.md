---
title: 机器学习（10）随机森林——Ramdom Forest
tags: ML Regression Classification
typora-root-url: ./..
---

随机森林是一种有监督学习算法，是以决策树为基学习器的集成学习算法，可以用于分类问题，也可以用于回归。

<!--more-->

##### 1.随机森林的两个随机

（1）数据集随机选取

从原始的数据集中采取有放回的抽样，构造子数据集，子数据集的数据量是和原始数据集相同的。不同子数据集的元素可以重复，同一个子数据集中的元素也可以重复。

（2）特征随机选取

随机森林中的子树的每一个分裂过程并未用到所有的待选特征，而是从所有的待选特征中随机选取一定的特征，之后再在随机选取的特征中选取最优的特征。

##### 2.步骤

（1）从原始训练集中使用Bootstraping方法随机有放回采样取出m个样本，共进行n_tree次采样。生成n_tree个训练集；

自助法（Bootstraping）：在含有 m 个样本的数据集中，每次随机挑选一个样本，将其作为训练样本，再将此样本放回到数据集中，这样有放回地抽样 m 次，生成一个与原数据集大小相同的数据集，这个新数据集就是训练集。

（2）对n_tree个训练集，分别训练n_tree个决策树模型；

（3）对于单个决策树模型，假设训练样本特征的个数为n，那么每次分裂时根据信息增益/信息增益比/基尼指数  选择最好的特征进行分裂；

（4）将生成的多颗决策树组成随机森林。对于分类问题，按照多棵树分类器投票决定最终分类结果；对于回归问题，由多颗树预测值的均值决定最终预测结果。

袋外数据OOB（out-of-bag ）：每棵决策树的生成都需要自助采样，这时就有1/3的数据未被选中，这部分数据就称为袋外数据。

![](images/randomforest/one.png)

##### 3.特征筛选

用随机森林进行特征重要性评估的思想就是看每个特征在随机森林中的每棵树上做了多大的贡献，然后取个平均值，最后比一比特征之间的贡献大小。

贡献大小通常使用基尼指数（Gini index）或者袋外数据（OOB）错误率作为评估指标来衡量。