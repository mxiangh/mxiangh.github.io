---

title: 机器学习（14）多维尺度缩放——Multidimensional Scaling，MDS
tags: ML Dimensionality_Reduction
---

#### 多维尺度缩放——Multidimensional Scaling，MDS

思想：一种无监督的基于距离度量的数据降维方法，在低维空间保持高维空间中样本的距离结构，即样本的两两距离关系，使低维空间内样本的距离尽可能接近原始高维距离。经典MDS基于度量距离，通过线性变换将高维距离矩阵映射到低维坐标，本质是一种线性降维方法。

<!--more-->

##### 1.损失函数

低维空间的距离与高维空间的距离尽可能一样，自然想到让两个距离的差越小越好，使用类似均方误差的想法，定义如下损失函数：

$$ \sum_{i \ne j} (\vert \vert z_i-z_j \vert \vert -d_{ij})^2 $$

其中，$\vert \vert z_i-z_j \vert \vert$表示低维样本两点之间欧氏距离，$ d_{ij} $表示高维空间两点距离，高维空间距离可选任意距离，但是要满足距离度量条件。

距离度量需满足：

- 非负性：$ d(x, y) \ge 0 $，且$d(x, y) = 0$等同于x = y；
- 对称性：$ d(x, y) = d(y, x) $
- 三角不等式：$ d(x, y) + d(y, z) \ge d(x, z) $

此外，为了避免分子受原始距离绝对值大小的影响，需要归一化处理，消除量纲的影响：

$$ \frac{\sum_{i \ne j} (\vert \vert z_i-z_j \vert \vert -d_{ij})^2}{\sum_{i \ne j} d_{ij}^2 } $$

最后，为了让量纲和距离一致，将函数开根，得到应力函数(stress function)：

$$ Stress(Z) = \sqrt{\frac{\sum_{i \ne j} ( \vert \vert z_i-z_j \vert \vert -d_{ij})^2}{\sum_{i \ne j} d_{ij}^2 }} $$

目标则是最小化应力函数。

这个想法看似很美好，可是这个损失函数没有唯一解，证明如下：

$$  \vert \vert z_i^{'} - z_j^{'}  \vert \vert =  \vert \vert (z_i - z_0) - (z_j - z_0)  \vert \vert  =  \vert \vert z_i - z_j  \vert \vert  $$

##### 2.距离矩阵与内积矩阵转化（需要较好的线性代数基础）

假设中心化后的数据矩阵 $ X \in \mathbb{R}^{n \times D} $，每行一个样本，其内积矩阵$ B = X X^T$，距离矩阵D满足 $D_{ij}= \vert \vert x_i-x_j \vert \vert $。

（1）距离平方展开

$$ D_{ij}^2 =  \vert \vert x_i-x_j \vert \vert ^2 =  \vert \vert x_i \vert \vert ^2 +  \vert \vert x_j \vert \vert ^2 - 2 x_i^T x_j $$

（2）矩阵形式表示：

定义：

$$ s = \lbrack  \vert \vert x_i \vert \vert ^2,  \vert \vert x_i \vert \vert ^2, \dots ,  \vert \vert x_i \vert \vert ^2 \rbrack^T ，\mathbf{1}=  \lbrack1, 1,\dots,1 \rbrack^T $$

则距离平方矩阵可表示为：

$$ D^{(2)} = s \mathbf{1}^T + \mathbf{1} s^T - 2B $$

可以得到$ D_{ij}^{(2)} = D_{ij}^2 $

（3）中心化处理：

数据中心化的定义：如果数据矩阵 $ X \in \mathbb{R}^{n \times D} $（n为样本数量，d为特征维度 ）已经中心化，意味着其每一列的均值为 0，即$\frac{1}{n}\mathbf{1}^T X = 0^T$。

对$ D^{(2)} $ 进行双中心化（行列均值归零）：

$$ H = I_n - \frac{1}{n} \mathbf{1} \mathbf{1}^T （中心化矩阵） $$

$$ B = -\frac{1}{2} H D^{(2)} H $$

（4）证明等价性（证明$ B = -\frac{1}{2} H D^{(2)} H $）：

$$\begin{aligned}
B &= -\frac{1}{2} H (s\mathbf{1}^T + \mathbf{1}s^T - 2B) H \\
&= -\frac{1}{2} H s\mathbf{1}^T H - \frac{1}{2} H \mathbf{1}s^T H + H B H \\
&= 0 + 0 + H B H  \\
&= B
\end{aligned}$$

首先，H是一个双中心化的矩阵，证明如下

$$ \mathbf{1}^T H = \mathbf{1}^T - \frac{1}{n} \mathbf{1}^T \mathbf{1} \mathbf{1}^T = \mathbf{1}^T - \mathbf{1}^T = 0 $$

$$ H \mathbf{1} = \mathbf{1} - \frac{1}{n} \mathbf{1} \mathbf{1}^T \mathbf{1} = \mathbf{1} - \mathbf{1} = 0 $$

其次，$ B = X X^T$，并且数据X已经中心化时，则容易证明$ H B = B $

$$ HB = (I_n - \frac{1}{n} \mathbf{1} \mathbf{1}^T) X X^T = X X^T - \frac{1}{n} \mathbf{1} \mathbf{1}^T X X^T $$

根据前面对中心化的定义，$ \mathbf{1}^T X = 0 $，所以$ HB = B $，同理，$ X^T \mathbf{1} = 0 $

$$ HBH = BH =  X X^T(I_n - \frac{1}{n} \mathbf{1} \mathbf{1}^T) = X X^T - \frac{1}{n} X X^T \mathbf{1} \mathbf{1}^T = B  $$

（5）元素验证：计算$B_{ij}$的第(i,j)元素：

$$\begin{aligned}
B_{ij} &= -\frac{1}{2} H D_{ij}^{(2)} D \\
&= -\frac{1}{2} ((I_n - \frac{1}{n} \mathbf{1} \mathbf{1}^T) D_{ij}^{(2)} (I_n - \frac{1}{n} \mathbf{1} \mathbf{1}^T)) \\
&= -\frac{1}{2} (D_{ij}^{(2)} - \frac{1}{n} \mathbf{1} \mathbf{1}^T D_{ij}^{(2)} - \frac{1}{n} D_{ij}^{(2)} \mathbf{1} \mathbf{1}^T + \frac{1}{n^2}\mathbf{1} \mathbf{1}^T D_{ij}^{(2)} \mathbf{1} \mathbf{1}^T) \\
&= -\frac{1}{2} (D_{ij}^{(2)} - \frac{1}{n} \sum_{k=1}^n D_{ik}^{(2)} - \frac{1}{n} \sum_{k=1}^n D_{kj}^{(2)} + \frac{1}{n^2} \sum_{k,l=1}^n D_{kl}^{(2)} ) \\
&= -\frac{1}{2} (( \vert \vert x_i \vert \vert ^2 +  \vert \vert x_j \vert \vert ^2 - 2 x_i^T x_j) - \frac{1}{n} \sum_{k=1}^n ( \vert \vert x_i \vert \vert ^2 +  \vert \vert x_k \vert \vert ^2 - 2 x_i^T x_k) - \frac{1}{n} \sum_{k=1}^n ( \vert \vert x_k \vert \vert ^2 +  \vert \vert x_j \vert \vert ^2 - 2 x_k^T x_j) + \frac{1}{n^2} \sum_{k,l=1}^n ( \vert \vert x_k \vert \vert ^2 +  \vert \vert x_l \vert \vert ^2 - 2 x_k^T x_l)) \\
&= -\frac{1}{2} (( \vert \vert x_i \vert \vert ^2 +  \vert \vert x_j \vert \vert ^2 - 2 x_i^T x_j) - ( \vert \vert x_i \vert \vert ^2 + \frac{1}{n} \sum_{k=1}^n  \vert \vert x_k \vert \vert ^2) - ( \vert \vert x_j \vert \vert ^2 + \frac{1}{n} \sum_{k=1}^n  \vert \vert x_k \vert \vert ^2) + \frac{2}{n} \sum_{k,l=1}^n  \vert \vert x_k \vert \vert ^2) \\
&= x_i^T x_j
\end{aligned}$$

结论：对于中心化数据，距离平方矩阵的双中心化结果与内积矩阵存在线性关系 $ B = -\frac{1}{2} H D^{(2)} H = X X^T $

##### 3.奇异值分解（Singular Value Decomposition，简称 SVD）

3.1 引入

假设X是中心化后的高维样本，降维后的低维样本是Z，MDS的目标是保持距离，也就是样本的高维距离和低维距离相等。

回到第一点最后的问题，既然低维距离矩阵不好求解，那么可以引入内积矩阵搭建一个高维距离和低维距离的桥梁，在前文已经证明

$$ B_{ij} = -\frac{1}{2} (D_{ij}^{2} - \frac{1}{n} \sum_{k=1}^n D_{ik}^{2} - \frac{1}{n} \sum_{k=1}^n D_{kj}^{2} + \frac{1}{n^2} \sum_{k,l=1}^n D_{kl}^{2} ) $$

也就是可以先由高维数据X求解得到B，而由于降维后的距离不变，利用距离矩阵与内积矩阵的关系$ B = Z Z^T $，可以求得低维距离矩阵Z。

求解$ B = Z Z^T $，涉及到奇异值分解，下面进行介绍。

3.2 奇异值分解

3.2.1 定义

对于任意一个实矩阵 $A_{m\times n}$（m行n列），奇异值分解是将其分解为三个矩阵的乘积形式，即：$A=U \Sigma V^T $。

其中：

- U是一个$ m\times m $的正交矩阵，$ U^T U = UU^T = I_m$（$I_m$是m阶单位矩阵），U的列向量称为左奇异向量。
- $\Sigma$是一个$m\times n$的对角矩阵，主对角线上的元素$\sigma_i$（$i = 1, 2, \cdots, \min(m, n)$）称为矩阵A的奇异值，并且通常按照从大到小的顺序排列，即$\sigma_1 \geq \sigma_2 \geq \cdots \geq \sigma_{\min(m, n)} \geq 0$ ，除了主对角线元素外，其余元素都为0。
- V是一个$n\times n$的正交矩阵，$V^TV = VV^T = I_n$（$I_n$是n阶单位矩阵），V的列向量称为右奇异向量。

3.2.2 原理

从线性变换的角度来看，矩阵A可以看作是一个从n维空间到m维空间的线性变换。奇异值分解就是将这个线性变换分解为三个连续的简单变换：

- 首先通过正交矩阵$V^T$对n维空间进行旋转。
- 然后通过对角矩阵$\Sigma$在旋转后的空间中进行缩放，缩放的比例就是奇异值。
- 最后通过正交矩阵U对m维空间进行旋转，得到变换后的结果。

3.2.3 计算方法

- 计算$A^TA$和$AA^T$：

$A^TA$是一个$n\times n$的实对称矩阵，$AA^T$是一个$m\times m$的实对称矩阵。

- 对$A^TA$进行特征值分解：

设$A^TA$的特征值为$\lambda_1 \geq \lambda_2 \geq \cdots \geq \lambda_n \geq 0$，对应的特征向量为$v_1, v_2, \cdots, v_n$，将这些特征向量组成正交矩阵$V = [v_1, v_2, \cdots, v_n]$ 。$A^TA$的特征值的平方根就是矩阵A的奇异值，即$\sigma_i = \sqrt{\lambda_i}$（$i = 1, 2, \cdots, n$）。

- 计算左奇异向量：

对于非零奇异值$\sigma_i$，左奇异向量$u_i = \frac{1}{\sigma_i}Av_i$，将所有左奇异向量组成正交矩阵U 。

3.3 问题转化

最后，问题转化为$ B = U \Sigma V^T = Z Z^T $，因为B是实对称矩阵，U=V，所以

$$ B = V \Sigma V^T = (V \Sigma^{\frac{1}{2}}) (V \Sigma^{\frac{1}{2}})^T = Z Z^T $$

可以发现，在B是实对称矩阵时，B也可以用特征值分解来求解。根据定义，V就是B的特征向量，$\Sigma$是B的特征值。

至此，MDS的损失函数构造和求解方法总算结束了。

##### 4.算法流程：

（1）中心化高维样本X，计算高维样本的距离矩阵D（一般采用欧氏距离），$D_{ij}$为样本$x_i$到样本$x_j$的距离；

（2）通过距离矩阵计算B：

$$ B_{ij} = -\frac{1}{2} (D_{ij}^{2} - D_{i·}^2 - D_{·j}^2 + D_{··}^2 ) $$

其中：

$$D_{i·}^2= \frac{1}{n} \sum_{j=1}^n D_{ij}^{2}，D_{·j}^2=\frac{1}{n} \sum_{i=1}^n D_{ij}^{2}，D_{··}^2=\frac{1}{n^2} \sum_{i=1}^n \sum_{j=1}^n D_{ij}^{2}$$

（3）对B做特征值分解：$ B =  V \Sigma V^T $

（4）取前d个最大特征值构成对角矩阵$ B = \Sigma_d $，对应特征向量矩阵$ V_d $

（5）低维坐标：$ Z = V_d \Sigma_d^{\frac{1}{2}} $

##### 5.简单说一下度量MDS与非度量MDS：

- 度量MDS：保持距离数值关系，适用于定量数据

$$ \underset{Z}{min} \sum_{i < j} ( \vert \vert z_i-z_j \vert \vert -d_{ij})^2 $$

- 非度量MDS：仅保持距离排序关系，适用于定性数据

$$ \underset{Z}{min} \sum_{i < j} (f(D_{ij}) -  \vert \vert z_i-z_j \vert \vert )^2 $$

##### 6.MDS优缺点

优点：

- 保留数据的全局结构。
- MDS 将高维数据映射到 2D 或 3D 空间，生成的散点图可直接可视化样本间的 “相似性”：距离越近的样本在原始数据中越相似。
- 支持度量、非度量模式，适配定量、定性距离数据，非常灵活。

缺点：

- 计算复杂度高。
- 高度依赖原始距离的有效性，原始距离的微小误差会被MDS放大。
- 对从高维到低维的映射必然伴随信息损失，MDS 无法完美保留所有距离关系，可能出现距离失真。
- 高维欧氏距离无法反映流形上的真实距离，使得降维效果差。

~~~
# 手写实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn import datasets
from sklearn.datasets import load_iris

#返回任意两个点之间距离的平方
def cal_pairwise_dist(x):
    '''
    计算欧式距离, x是样本矩阵
    (a-b)^2 = a^2 + b^2 - 2*a*b
    '''
    sum_x = np.sum(np.square(x), 1) # 求L2范数
    dist = np.add(np.add(-2 * np.dot(x, x.T), sum_x).T, sum_x)
    
    return dist

# # 手动实现经典MDS算法
def mds(X, n_components=2):
    """
    实现经典MDS降维
    
    参数:
    X: 原始数据矩阵，形状为(n_samples, n_features)
    n_components: 降维后的维度
    
    返回:
    Z: 降维后的矩阵，形状为(n_samples, n_components)
    """
    # 步骤1: 计算距离平方矩阵
    n = X.shape[0]
    
    D_sq = cal_pairwise_dist(X)
    
    # 步骤2: 构造中心化矩阵H
    H = np.eye(n) - np.ones((n, n)) / n
    
    # 步骤3: 计算双中心化后的内积矩阵B
    B = -0.5 * H @ D_sq @ H
    
    # 步骤4: 对B进行奇异值分解(SVD)
    U, Sigma, Vt = np.linalg.svd(B)
    
    # 步骤5: 选取前n_components个奇异值和对应的特征向量
    Sigma_k = np.diag(np.sqrt(Sigma[:n_components]))
    U_k = U[:, :n_components]
    
    # 步骤6: 计算低维坐标
    Z = U_k @ Sigma_k
    
    return Z
    
# datasets数据集
X, color = datasets.make_swiss_roll(n_samples=500, noise=0.01, random_state=42)

data = mds(X)

fig = plt.figure(figsize=(14,7))

ax1 = fig.add_subplot(1, 2, 1, projection='3d')  # 1行2列的第1个
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax1.set_title("make_swiss_roll")
ax1.view_init(4, -75)  # 调整视角

plt.subplot(122)
plt.title("MDS")
plt.scatter(data[:, 0], data[:, 1], c = color, cmap=plt.cm.Spectral)
plt.show()
~~~

~~~
# sklearn实现
import numpy as np
import matplotlib.pyplot as plt
from sklearn.manifold import MDS
from sklearn import datasets

# datasets数据集
X, color = datasets.make_swiss_roll(n_samples=500, noise=0.01, random_state=42)

data = MDS(n_components=2, random_state=42, normalized_stress='auto').fit_transform(X)

fig = plt.figure(figsize=(14,7))

ax1 = fig.add_subplot(1, 2, 1, projection='3d')
ax1.scatter(X[:, 0], X[:, 1], X[:, 2], c=color, cmap=plt.cm.Spectral)
ax1.set_title("make_swiss_roll")
ax1.view_init(4, -75)  # 调整视角

plt.subplot(122)
plt.title("MDS")
plt.scatter(data[:, 0], data[:, 1], c = color, cmap=plt.cm.Spectral)
plt.show()
~~~

