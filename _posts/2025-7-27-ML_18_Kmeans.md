---
title: 机器学习（18）K均值聚类——K-Means
tags: ML Clustering
typora-root-url: ./..
---

思想：一种无监督聚类算法，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

<!--more-->

##### 1.优化目标推导

K均值算法通过将数据划分成不同的簇对样本进行分类。簇指的是类别，将数据划分成多个簇指的是将数据分为多个类别，想分成几类，就需要找几个簇。既然是找簇，那每个簇肯定有一个质心（最中间的点），质心也是簇内样本均值构成的点。如果每个簇内的样本离质心都很近，就意味着这个簇内样本相关度高，它们是同一个类别的概率非常大。

假设簇划分为$\lbrace c_1, c_2,\cdots,c_k \rbrace$，可以用质心和样本之间的均值误差来衡量模型的好坏：

$$ min \quad \sum_{i=1}^k \sum_{x \in \mu_i} \vert \vert x - \mu_i \vert \vert_2^2$$

其中，$\mu_i$是簇$c_i$的均值向量，也就是质心

$$ \mu_i = \frac{1}{\vert C_i \vert} \sum_{x \in C_i} x$$

##### 2.求解

直接求解上述问题是一个NP难问题，故采用启发式算法进行求解。

算法步骤：

（1）从数据集D中随机选择 k 个样本作为初始的 k 个质心向量，构成簇；

（2）计算每个样本与所有质心的距离，选择距离最小的质心作为样本的簇；使用均值更新每个簇的质心，如果质心不变，则结束，否则重复步骤2知道质心不变；

（3）输出簇划分。

![](/assets/images/Kmeans/one.png)

图中以k=2为例，(b)随机选择两个初始簇心，(c)根据样本和所有簇距离更新样本所属簇，(d)计算新的质心，(e)计算新的样本距离和所属簇，(f)更新质心，稳定。

##### 3.K-Means优缺点

优点：
- 原理比较简单，实现也是很容易，收敛速度快
- 聚类效果较优（依赖K的选择）
- 算法的可解释度比较强
- 主要需要调参的参数仅仅是簇数 k

缺点：
- K值的选取不好把握
- 对于不是凸的数据集比较难收敛
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳
- 采用迭代方法，得到的结果只能保证局部最优，不一定是全局最优（与K的个数及初值选取有关）
- 对噪音和异常点比较的敏感（中心点易偏移）

##### 4.k-means++

初始位置选择对聚类结果和运行时间都有很大的影响，如果仅仅是完全随机的选择，有可能导致算法收敛很慢。故k-means++对算法步骤（1）进行改进，剩下步骤不变。

质心选择策略：

（1）从输入的数据点集合中随机选择一个点作为第一个质心$\mu_i$；

（2）首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离)，用$D(x)$表示

$$ D(x) = arg\ min \vert \vert x_i-\mu_r \vert \vert_2^2 $$

接着计算每个样本被选为下一个聚类中心的概率

$$ \frac{D(x)^2}{\sum_{x \in X}D(x)^2} $$

最后，按照轮盘法选择出下一个聚类中心；

（3）重复步骤2，直到选择K个质心。

##### 4.k均值改进——k-means++

初始位置选择对聚类结果和运行时间都有很大的影响，如果仅仅是完全随机的选择，有可能导致算法收敛很慢。故k-means++对算法步骤（1）进行改进，剩下步骤不变。

质心选择策略：

（1）从输入的数据点集合中随机选择一个点作为第一个质心$\mu_i$；

（2）首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离)，用$D(x)$表示

$$ D(x) = arg\ min \vert \vert x_i-\mu_r \vert \vert_2^2 $$

接着计算每个样本被选为下一个聚类中心的概率

$$ \frac{D(x)^2}{\sum_{x \in X}D(x)^2} $$

最后，按照轮盘法选择出下一个聚类中心；

（3）重复步骤2，直到选择K个质心。

##### 5.k均值改进——K值选择

（1）肘部法（Elbow Method）；

（2）轮廓系数法（Silhouette Score）；

（3）Calinski-Harabasz 指数

~~~
# 手写实现，无改进
import numpy as np
import matplotlib.pyplot as plt

# 西瓜数据集4.0
data = np.array([
    [0.697, 0.460], [0.774, 0.376], [0.634, 0.264], [0.608, 0.318], [0.556, 0.215],
    [0.403, 0.237], [0.481, 0.149], [0.437, 0.211], [0.666, 0.091], [0.243, 0.267],
    [0.245, 0.057], [0.343, 0.099], [0.639, 0.161], [0.657, 0.198], [0.360, 0.370],
    [0.593, 0.042], [0.719, 0.103], [0.245, 0.057], [0.343, 0.099], [0.639, 0.161],
    [0.657, 0.198], [0.360, 0.370], [0.593, 0.042], [0.748, 0.232], [0.714, 0.346],
    [0.483, 0.312], [0.478, 0.437], [0.525, 0.369], [0.751, 0.489], [0.532, 0.472],
    [0.473, 0.376], [0.725, 0.445], [0.446, 0.459]
])

# K均值聚类算法
def kmeans(data, k, max_iter=100):
    # 随机选择K个初始中心点
    np.random.seed(42)  # 设置随机种子，确保结果可复现
    centers = data[np.random.choice(data.shape[0], k, replace=False)]
    
    for _ in range(max_iter):
        # 分配每个点到最近的中心点
        clusters = [[] for _ in range(k)]
        for point in data:
            distances = [np.linalg.norm(point - center) for center in centers]
            cluster_index = np.argmin(distances)
            clusters[cluster_index].append(point)
        
        # 更新中心点
        new_centers = [np.mean(cluster, axis=0) for cluster in clusters]
        
        # 检查中心点是否收敛
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    
    return centers, clusters

# 聚类结果
k = 3  # 假设分为3个簇
centers, clusters = kmeans(data, k)

# 可视化结果
plt.figure(figsize=(10, 6))
colors = ['r', 'g', 'b']
for i, cluster in enumerate(clusters):
    cluster = np.array(cluster)
    plt.scatter(cluster[:, 0], cluster[:, 1], c=colors[i], label=f'Cluster {i+1}')
plt.scatter(np.array(centers)[:, 0], np.array(centers)[:, 1], c='k', marker='x', s=100, label='Centers')
plt.xlabel('Density')
plt.ylabel('Sugar Rate')
plt.title('K-Means Clustering of Watermelon Dataset 4.0')
plt.legend()
plt.show()
~~~

