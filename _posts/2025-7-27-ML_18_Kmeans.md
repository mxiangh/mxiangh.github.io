---
title: 机器学习（18）K均值聚类——K-Means
tags: ML Clustering
typora-root-url: ./..
---

思想：一种无监督聚类算法，对于给定的样本集，按照样本之间的距离大小，将样本集划分为K个簇。让簇内的点尽量紧密的连在一起，而让簇间的距离尽量的大。

<!--more-->

##### 1.优化目标推导

K均值算法通过将数据划分成不同的簇对样本进行分类。簇指的是类别，将数据划分成多个簇指的是将数据分为多个类别，想分成几类，就需要找几个簇。既然是找簇，那每个簇肯定有一个质心（最中间的点），质心也是簇内样本均值构成的点。如果每个簇内的样本离质心都很近，就意味着这个簇内样本相关度高，它们是同一个类别的概率非常大。

假设簇划分为$\lbrace c_1, c_2,\cdots,c_k \rbrace$，可以用质心和样本之间的均值误差来衡量模型的好坏：

$$ min \quad \sum_{i=1}^k \sum_{x \in \mu_i} \vert \vert x - \mu_i \vert \vert_2^2$$

其中，$\mu_i$是簇$c_i$的均值向量，也就是质心

$$ \mu_i = \frac{1}{\vert C_i \vert} \sum_{x \in C_i} x$$

##### 2.求解

直接求解上述问题是一个NP难问题，故采用启发式算法进行求解。

算法步骤：

（1）从数据集D中随机选择 k 个样本作为初始的 k 个质心向量，构成簇；

（2）计算每个样本与所有质心的距离，选择距离最小的质心作为样本的簇；使用均值更新每个簇的质心，如果质心不变，则结束，否则重复步骤2知道质心不变；

（3）输出簇划分。

![](/assets/images/Kmeans/one.png)

图中以k=2为例，(b)随机选择两个初始簇心，(c)根据样本和所有簇距离更新样本所属簇，(d)计算新的质心，(e)计算新的样本距离和所属簇，(f)更新质心，稳定。

##### 3.K-Means优缺点

优点：
- 原理比较简单，实现也是很容易，收敛速度快
- 聚类效果较优（依赖K的选择）
- 算法的可解释度比较强
- 主要需要调参的参数仅仅是簇数 k

缺点：
- K值的选取不好把握
- 对于不是凸的数据集比较难收敛
- 如果各隐含类别的数据不平衡，比如各隐含类别的数据量严重失衡，或者各隐含类别的方差不同，则聚类效果不佳
- 采用迭代方法，得到的结果只能保证局部最优，不一定是全局最优（与K的个数及初值选取有关）
- 对噪音和异常点比较的敏感（中心点易偏移）

##### 4.k-means++

初始位置选择对聚类结果和运行时间都有很大的影响，如果仅仅是完全随机的选择，有可能导致算法收敛很慢。故k-means++对算法步骤（1）进行改进，剩下步骤不变。

质心选择策略：

（1）从输入的数据点集合中随机选择一个点作为第一个质心$\mu_i$；

（2）首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离)，用$D(x)$表示

$$ D(x) = arg\ min \vert \vert x_i-\mu_r \vert \vert_2^2 $$

接着计算每个样本被选为下一个聚类中心的概率

$$ \frac{D(x)^2}{\sum_{x \in X}D(x)^2} $$

最后，按照轮盘法选择出下一个聚类中心；

（3）重复步骤2，直到选择K个质心。

##### 4.k均值改进——k-means++

初始位置选择对聚类结果和运行时间都有很大的影响，如果仅仅是完全随机的选择，有可能导致算法收敛很慢。故k-means++对算法步骤（1）进行改进，剩下步骤不变。

质心选择策略：

（1）从输入的数据点集合中随机选择一个点作为第一个质心$\mu_i$；

（2）首先计算每个样本与当前已有聚类中心之间的最短距离（即与最近的一个聚类中心的距离)，用$D(x)$表示

$$ D(x) = arg\ min \vert \vert x_i-\mu_r \vert \vert_2^2 $$

接着计算每个样本被选为下一个聚类中心的概率

$$ \frac{D(x)^2}{\sum_{x \in X}D(x)^2} $$

最后，按照轮盘法选择出下一个聚类中心；

（3）重复步骤2，直到选择K个质心。

##### 5.k均值改进——K值选择

（1）肘部法（Elbow Method）；

（2）轮廓系数法（Silhouette Score）；

（3）Calinski-Harabasz 指数

~~~
# 手写实现，无改进
import numpy as np
import matplotlib.pyplot as plt

# 提供的西瓜数据集4.0
data = np.array([
    [1, 0.697, 0.460, 1],
    [2, 0.774, 0.376, 1],
    [3, 0.634, 0.264, 1],
    [4, 0.608, 0.318, 1],
    [5, 0.556, 0.215, 1],
    [6, 0.403, 0.237, 0],
    [7, 0.481, 0.149, 0],
    [8, 0.437, 0.211, 0],
    [9, 0.666, 0.091, 0],
    [10, 0.243, 0.267, 0],
    [11, 0.245, 0.057, 0],
    [12, 0.343, 0.099, 0],
    [13, 0.639, 0.161, 1],
    [14, 0.657, 0.198, 1],
    [15, 0.360, 0.370, 0],
    [16, 0.593, 0.042, 0],
    [17, 0.719, 0.103, 1],
    [18, 0.359, 0.188, 0],
    [19, 0.339, 0.241, 0],
    [20, 0.282, 0.257, 0],
    [21, 0.748, 0.232, 1],
    [22, 0.714, 0.346, 1],
    [23, 0.483, 0.312, 0],
    [24, 0.478, 0.437, 0],
    [25, 0.525, 0.369, 1],
    [26, 0.751, 0.489, 1],
    [27, 0.532, 0.472, 1],
    [28, 0.473, 0.376, 0],
    [29, 0.725, 0.445, 1],
    [30, 0.446, 0.459, 0]
])

# 提取密度和含糖率作为特征
X = data[:, 1:3]

# K均值聚类算法
def kmeans(X, k, max_iter=100):
    # 随机选择K个初始中心点
    np.random.seed(42)  # 设置随机种子，确保结果可复现
    centers = X[np.random.choice(X.shape[0], k, replace=False)]
    
    for _ in range(max_iter):
        # 分配每个点到最近的中心点
        clusters = [[] for _ in range(k)]
        for point in X:
            distances = [np.linalg.norm(point - center) for center in centers]
            cluster_index = np.argmin(distances)
            clusters[cluster_index].append(point)
        
        # 更新中心点
        new_centers = [np.mean(cluster, axis=0) for cluster in clusters]
        
        # 检查中心点是否收敛
        if np.allclose(centers, new_centers):
            break
        centers = new_centers
    
    return centers, clusters

# 聚类结果
k = 3  # 假设分为3个簇
centers, clusters = kmeans(X, k)

# 可视化结果
plt.figure(figsize=(10, 6))

# 自动生成颜色
colors = plt.cm.get_cmap('tab10', k).colors

# 可视化结果
plt.figure(figsize=(10, 6))
for i in range(k):
    cluster_data = X[labels == i]
    plt.scatter(cluster_data[:, 0], cluster_data[:, 1], color=[colors[i]], s=100 , label=f'Cluster {i+1}')

# 绘制聚类中心
plt.scatter(np.array(centers)[:, 0], np.array(centers)[:, 1], c='k', marker='x', s=100, label='Centers')

plt.xlabel('Density')
plt.ylabel('Sugar Content')
plt.title('K-Means Clustering of Watermelon Dataset 4.0')
plt.legend()
plt.show()
~~~

