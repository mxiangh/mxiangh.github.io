---
title: 机器学习（22）密度峰值聚类——Density Peak Clustering. DPC
tags: ML Clustering
typora-root-url: ./..
---

思想：聚类中心点的密度高于其邻近点，且与更高密度的点之间保持相对较远的距离。

<!--more-->

论文：Clustering by fast search and find of density peaks

##### 1.假设

DPC首先有两个假设：

假设1：聚类中心被低局部密度邻居包围，也就是聚类中心点的密度比它的邻居密度要大。

假设2：且与更高密度点保持较大距离，也就是不同聚类中心点尽可能远。

##### 2.定义

（1）每个数据点的局部密度$ \rho_i$，是该数据点与其他数据点的距离$d_{ij}$小于设定的截断距离$d_c$下的数据点总数

$$ \rho_i = \sum_j \chi(d_{ij} - d_c) $$

$$ \chi(x)=\left\{\begin{array}{1}
1,x<0\\
0,otherwise
\end{array}\right. $$

（2）与密度更高的数据点之间距离$ \delta_i $为当前数据点到 “更高密度区域” 的最近距离

$$ \delta_i = \underset{j:\rho_j>\rho_i}{min} d_{ij} $$

注意到密度最大的数据点没有比它密度更高的数据点，也就无法获取$ \delta_i $，故定义密度最高的点的密度$ \delta_i = max_j d_{ij} $。

注：把$ \delta_i $理解为非中心点划分簇的距离依据：距离越小，越可能属于某个簇；距离越大，越不可能属于某个簇。当距离最大时，这个点更可能作为一个新的聚类中心。

##### 3.簇划分

基于第一节的两个假设，局部密度高且离更高密度点距离最大的点，也就是高$\delta$和高$\rho$的点可以作为中心点。而高$\delta$低$\rho$点则为孤立点（异常值）。

![](/images/DPC/one.png)

确定中心点后，每个剩余点被分配到比其密度更高且距离最近的邻居所属簇。为量化分配可靠性，通过定义簇边界区域（与其它簇距离小于$d_c$的点集），确定边界内最高密度$\rho_b$，将密度高于$\rho_b$的点归为中心点（强关联），其余归为光晕点（可视为噪声）。

##### 4.优缺点

优点

- 对数据分布要求不高，尤其对于非球形簇。

- 原理简单，功能强大。

缺点

- 二次时间复杂度，效率低，大数据集不友好。

- 不适合高维。

- 截断距离是一个超参数，需要选择。

~~~
# 手写实现
import numpy as np
import matplotlib.pyplot as plt

# 计算数据点两两之间的距离
def getDistanceMatrix(datas):
    N,D = np.shape(datas)
    dists = np.zeros([N,N])
    
    for i in range(N):
        for j in range(N):
            vi = datas[i,:]
            vj = datas[j,:]
            dists[i,j]= np.sqrt(np.dot((vi-vj),(vi-vj)))
    return dists
    
# 找到密度计算的阈值dc
# 要求平均每个点周围距离小于dc的点的数目占总点数的1%-2%
def select_dc(dists):    
    N = np.shape(dists)[0]
    tt = np.reshape(dists,N*N)
    percent = 2.0
    position = int(N * (N - 1) * percent / 100)
    dc = np.sort(tt)[position  + N]

    return dc
    
# 计算每个点的局部密度    
def get_density(dists,dc,method=None):
    N = np.shape(dists)[0]
    rho = np.zeros(N)
    
    for i in range(N):
        if method == None:
            rho[i]  = np.where(dists[i,:]<dc)[0].shape[0]-1
        else:
            rho[i] = np.sum(np.exp(-(dists[i,:]/dc)**2))-1
    return rho
    
# 计算每个数据点的密度距离
# 即对每个点，找到密度比它大的所有点
# 再在这些点中找到距离其最近的点的距离
def get_deltas(dists,rho):
    N = np.shape(dists)[0]
    deltas = np.zeros(N)
    nearest_neiber = np.zeros(N)
    # 将密度从大到小排序
    index_rho = np.argsort(-rho)
    for i,index in enumerate(index_rho):
        # 对于密度最大的点
        if i==0:
            continue
            
        # 对于其他的点
        # 找到密度比其大的点的序号    
        index_higher_rho = index_rho[:i]
        # 获取这些点距离当前点的距离,并找最小值
        deltas[index] = np.min(dists[index,index_higher_rho])
        
        #保存最近邻点的编号
        index_nn = np.argmin(dists[index,index_higher_rho])
        nearest_neiber[index] = index_higher_rho[index_nn].astype(int)
    
    deltas[index_rho[0]] = np.max(deltas)   
    return deltas,nearest_neiber
        
# 通过阈值选取 rho与delta都大的点
# 作为聚类中心    
def find_centers_auto(rho,deltas):
    rho_threshold = (np.min(rho) + np.max(rho))/ 2
    delta_threshold  = (np.min(deltas) + np.max(deltas))/ 2
    N = np.shape(rho)[0]
    
    centers = []
    for i in range(N):
        if rho[i]>=rho_threshold and deltas[i]>delta_threshold:
            centers.append(i)
    return np.array(centers)

# 选取 rho与delta乘积较大的点作为
# 作为聚类中心   
def find_centers_K(rho,deltas,K):
    rho_delta = rho*deltas
    centers = np.argsort(-rho_delta)
    return centers[:K]

def DPC(rho,centers,nearest_neiber):
    K = np.shape(centers)[0]
    if K == 0:
        print("can not find centers")
        return
    
    N = np.shape(rho)[0]
    labs = -1*np.ones(N).astype(int)
    
    # 首先对几个聚类中进行标号
    for i, center in enumerate(centers):
        labs[center] = i
   
    # 将密度从大到小排序
    index_rho = np.argsort(-rho)
    for i, index in enumerate(index_rho):
        # 从密度大的点进行标号
        if labs[index] == -1:
            # 如果没有被标记过
            # 那么聚类标号与距离其最近且密度比其大
            # 的点的标号相同
            labs[index] = labs[int(nearest_neiber[index])]
    return labs

# 绘制密度/距离分布图
def draw_decision(data, rho,deltas):       
    plt.cla()
    for i in range(np.shape(data)[0]):
        plt.scatter(rho[i], deltas[i], s=16., facecolors='none', edgecolors=(0,0,0))
        plt.annotate(str(i), xy = (rho[i], deltas[i]),xytext = (rho[i], deltas[i]))
        plt.xlabel("rho")
        plt.ylabel("deltas")
    plt.show()

# 绘制聚类结果
def draw_cluster(data,labs,centers, dic_colors):     
    plt.cla()
    K = np.shape(centers)[0]
    
    for k in range(K):
        sub_index = np.where(labs == k)
        sub_data = data[sub_index]
        # 画数据点
        plt.scatter(sub_data[:,0],sub_data[:,1],s=20.,color=dic_colors[k])
        # 画聚类中心
        plt.scatter(data[centers[k],0],data[centers[k],1],color="k",marker="+",s = 200)
    plt.show()

dic_colors = {0:(.8,0,0),1:(0,.8,0),
              2:(0,0,.8),3:(.8,.8,0),
              4:(.8,0,.8),5:(0,.8,.8),
              6:(0,0,0)}

# 提供的西瓜数据集4.0
data = np.array([
    [1, 0.697, 0.460, 1],[2, 0.774, 0.376, 1],[3, 0.634, 0.264, 1],[4, 0.608, 0.318, 1],[5, 0.556, 0.215, 1],
    [6, 0.403, 0.237, 0],[7, 0.481, 0.149, 0],[8, 0.437, 0.211, 0],[9, 0.666, 0.091, 0],[10, 0.243, 0.267, 0],
    [11, 0.245, 0.057, 0],[12, 0.343, 0.099, 0],[13, 0.639, 0.161, 1],[14, 0.657, 0.198, 1],[15, 0.360, 0.370, 0],
    [16, 0.593, 0.042, 0],[17, 0.719, 0.103, 1],[18, 0.359, 0.188, 0],[19, 0.339, 0.241, 0],[20, 0.282, 0.257, 0],
    [21, 0.748, 0.232, 1],[22, 0.714, 0.346, 1],[23, 0.483, 0.312, 0],[24, 0.478, 0.437, 0],[25, 0.525, 0.369, 1],
    [26, 0.751, 0.489, 1],[27, 0.532, 0.472, 1],[28, 0.473, 0.376, 0],[29, 0.725, 0.445, 1],[30, 0.446, 0.459, 0]
])

# 提取密度和含糖率作为特征
X = data[:, 1:3]

# 计算距离矩阵
dists = getDistanceMatrix(data)

# 计算dc
dc = select_dc(dists)
print("dc",dc)

# 计算局部密度 
rho = get_density(dists,dc,method="Gaussion")
# 计算密度距离
deltas, nearest_neiber= get_deltas(dists,rho)

# 绘制密度/距离分布图
draw_decision(data, rho,deltas)

# 获取聚类中心点，二选一
centers = find_centers_K(rho,deltas,3)
# centers = find_centers_auto(rho,deltas)
print("centers",centers)

labs = DPC(rho,centers,nearest_neiber)
draw_cluster(data,labs,centers, dic_colors) 
    
~~~



~~~
~~~

