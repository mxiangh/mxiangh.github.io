---

title: 深度学习（6）初识循环神经网络RNN
tags: DL RNN
typora-root-url: ./..
---

认识循环神经网络（Recurrent Neural Network，RNN）的由来、特点。

<!--more-->

##### 1.序列

先了解一下序列数据的特点：

- 有序性：序列数据中的元素是按照特定的顺序排列的，这个顺序通常是时间顺序、空间顺序或其他逻辑顺序。

- 依赖性：序列数据中的每个元素通常依赖于前面的元素。这种依赖性可以通过时间依赖（如时间序列中的趋势和季节性）或语义依赖（如文本中的上下文）来体现。

- 动态性：序列数据通常具有动态变化的特性。

常见的序列数据有时间、文本、音频、视频、生物序列。这些序列长短不一，也无法拆分成独立的一个一个样本（依赖性）。在此前介绍的DNN和CNN中，样本都是独立的，输入的维度是固定的，所以对于序列数据没有较好的处理能力。

此外，DNN和CNN也不能记住之前输入的数据，对于动态性的数据，处理效果也不好。

##### 2.RNN原理详解

为了解决上述问题，RNN引入隐状态h（hidden state）的概念，隐状态h可以对序列形的数据提取特征，接着再转换为输出。这里先给出RNN的经典结构，再慢慢拆分详解。![](/images/RNN/1.png)

###### 2.1 符号解释

- 时间步（Time Step）：序列数据处理中的一个基本概念，用于表示序列中数据点的顺序或位置。在处理序列数据时，如时间序列分析、自然语言处理、语音识别等任务，数据通常由一系列按时间或逻辑顺序排列的元素组成。时间步就是这些元素的索引或序号，它反映了序列中元素的顺序关系。

- $x_1,x_2,x_3,x_4$：这些通常代表输入序列的数据点，每个$x_i$是序列中的一个时间步的输入。例如，在自然语言处理中，$x_i$可以是一个单词或字符的向量表示；在时间序列分析中，$x_i$可以是一个时间点的观测值。

- $h_0,h_1,h_2,h_3,h_4$：这些代表隐藏状态（hidden states），隐藏状态是RNN内部的表示，用于捕捉序列中到目前为止的信息。隐藏状态在每个时间步都会更新，并且会传递到下一个时间步。$h_0$通常是初始隐藏状态，可能被初始化为零向量或通过某种方式学习得到。

- $y_1,y_2,y_3,y_4$：这些代表输出序列的数据点，每个$y_i$是序列中对应时间步的输出。在某些任务中，如语言模型或文本生成，$y_i$可以是预测的下一个单词或字符；在其他任务中，如序列标注，$y_i$可以是每个输入的标签或分类。

- 从$x_i$到$h_i$的箭头表示输入数据流向当前时间步的隐藏状态。

- 从$h_i$到$y_i$的箭头表示输入数据流向当前时间步的隐藏状态。

- 从$h_i$到$h_{i+1}$的箭头表示输入数据流向当前时间步的隐藏状态。

简述：

如果是文本，时间步表示按逻辑顺序的位置信息（信息也是一种数据），$x_i$是在按顺序的第i个位置的信息，将信息传入对应位置的隐状态$h_i$后，再输出预测值$y_i$。

如果是时间序列，时间步表示按时间顺序的数据信息，$x_i$是在按顺序的第i个位置的数据，将数据传入对应位置的隐状态$h_i$后，再输出预测值$y_i$。

期间，隐状态获得的信息会按照对应的顺序关系，往下传递给下一个隐状态，目的是让当前信息和之前输入的信息相关联。

###### 2.2 隐状态的计算

![](/images/RNN/2.png)

![](/images/RNN/3.png)

其中：

- $U$表示将前一个隐藏状态$h_{i-1}$映射到当前的隐藏状态空间的权重矩阵；
- $W$表示将当前时间步的输入$x_i$映射到当前隐藏状态空间的权重矩阵；
- $b$是隐藏状态的偏置项；
- $f$是激活函数。

注意，在隐藏状态中，参数$U$、$W$和$b$是共享的，这与CNN卷积核类似。

循环神经网络中的循环，就是指隐藏状态在网络中从一个时间步传递到下一个时间步的过程。这个过程使得网络能够保持对先前信息的记忆，并利用这些信息来处理序列中的后续元素。如果将隐状态去除，模型将退化成一个从x到y的映射，与DNN结构一样。

###### 2.3 输出层的计算

![](/images/RNN/4.png)

其中：

- $V$表示当前时间步的隐藏状态空间到输出的权重矩阵；
- $c$是隐藏状态的偏置项；
- $Softmax$是激活函数，此处也可以依据特定任务使用其他激活函数。

这个过程相对而言比较简单，与之前的DNN结构一样，$g$（权重×输入+偏置）=输出。

###### 2.4 RNN的简洁表示

![](/images/RNN/5.png)

左边是最常见的RNN结构，右边是对这个结构的拆分，与上文拆分讲述的结构是一样的。图中的A是隐状态空间的计算过程：

$$ f(W_{xh}X_t+W_{hh}H_{t-1}+b)$$
为了突出时间步的特点，下标从$i$变为$t$，$W_{xh}$为输入到隐状态空间的权重，$W_{hh}$为上一隐状到当前隐状态空间的权重，$f$是激活函数。

至此，RNN的所有细节都讲完了，结构并不复杂，公式甚至比CNN还简单。

##### 
