---
title: 自然语言处理（4）编码器、解码器、Seq2Seq
tags: DL RNN NLP
typora-root-url: ./..
---

学习编码器Encoder、解码器Decoder以及Seq2Seq模型。

<!--more-->

论文：2014，Sequence to Sequence Learning with Neural Networks，Seq2Seq模型

##### 1.编码器Encoder

###### 1.1 作用

编码器的主要作用是将输入序列（例如源语言句子）转化为一个**固定长度**的向量表示，这个向量也被称为上下文向量（Context Vector）。它会对输入序列中的信息进行编码，捕捉序列中的语义和结构信息。

白话：把外界的各种信息转成计算机能看懂的数据。

![](/images/NLP/8.png)

###### 1.2 以RNN举例

- 对于一个输入序列$ (x = \lbrack x_1, x_2, ..., x_T \rbrack )$，其中 $x_t$ 表示输入序列在时刻$ t $的元素，$T $是序列的长度。编码器按顺序逐个处理输入序列中的元素。
- 在每个时间步$ t$，RNN 单元接收当前输入 $x_t$和上一个时间步的隐藏状态 $h_{t - 1}$，通过内部的计算更新隐藏状态$h_t$。例如 $h_t = f( x_t, h_{t - 1})$，其中$f$是 RNN 单元的计算函数。
- 当处理完整个输入序列后，最后一个时间步的隐藏状态 $h_T$ 就作为编码器输出的上下文向量，它包含了整个输入序列的编码信息。

##### 2.解码器Decoder

###### 2.1 作用

解码器的任务是根据编码器输出的上下文向量，生成目标序列（例如目标语言句子）。它通过对上下文向量进行解码，逐步生成目标序列中的每个元素。

白话：把数据转成人能看懂的信息。

![](/images/NLP/9.png)

###### 2.2 以RNN举例

- 解码器通常以一个特殊的起始符号（如 \<sos\> 表示句子开始）作为初始输入。
- 在每个时间步 $t$，解码器接收上一个时间步的隐藏状态$h_{t - 1}和$上一个时间步的输出 $y_{t - 1}$（在第一个时间步，$y_0$是起始符号），以及编码器输出的上下文向量 c。
- 通过计算 $h_t = f(y_{t - 1}, h_{t - 1}, c)$更新隐藏状态，然后通过一个输出层（如全连接层加上 softmax 函数）计算出当前时间步生成每个可能单词的概率，从而预测出当前时间步的输出 $y_t$。
- 重复这个过程，直到生成结束符号（如 \<eos\> 表示句子结束）或者达到预设的最大输出长度。

##### 3.上下文向量（Context Vector）

编码器将输入序列转换成的**固定长度**向量，作为解码器的输入。

当编码器处理完输入序列（比如一句话的所有单词）之后，它会生成一个代表整个输入序列的内部状态，这个状态就是上下文向量。这个向量包含了输入序列中的所有关键信息，能够帮助解码器理解输入内容的总体意思。

![](/images/NLP/11.png)

##### 4.Seq2Seq模型

在此前的DNN网络中，通常要求输入和输出具有固定的维度，但是序列的维度不一定是固定，例如在机器翻译中，中文和英文序列长度大概率是不一样长的。

Sequence-to-Sequence模型是一种用于将一个序列映射到另一个序列的深度学习架构。核心思想是将输入序列（如一种语言的句子）编码为一个固定大小的向量，然后将这个向量解码为输出序列（如另一种语言的句子）。在机器翻译中经常使用。

在论文Sequence to Sequence Learning with Neural Networks中，作者使用了两个LSTM训练模型，第一个LSTM用于获得固定维度的向量表示，第二个LSTM用于将向量表示输出为序列，这两个LSTM构成了一个“编码器—解码器”的架构。

此外，论文还发现颠倒源句中单词的顺序，还可以让模型在翻译长句子的任务中表现良好，比如原本输入a、b、c，翻译$\alpha 、\beta 、\gamma$，将输入颠倒为c、b、a，但是输出不变。

作者认为，颠倒顺序是引入了更多的短期依赖关系，在之前RNN中，长期依赖问题严重，如果能更多地使用短期依赖，模型训练难度变低，且效果可能更好。

结论：找到一种具有最多短期依赖关系的问题编码方式非常重要，因为这会让学习问题变得简单得多。特别是，虽然我们无法在非颠倒的翻译问题上训练标准的循环神经网络（RNN），但作者认为，当源语句被颠倒时，标准的 RNN 应该很容易训练（尽管作者没有通过实验验证这一点）。
